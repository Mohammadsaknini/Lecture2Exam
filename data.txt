[Lecture Start]

------------Introduction_into_nlp------------
What is NLP?


What is NLP?
Natural language processing (NLP) is an interdisciplinary subfield of 
computer science and information retrieval. It is primarily concerned with 
giving computers the ability to support and manipulate human language. It 
involves processing natural language datasets, such as text corpora or 
speech corpora, using either rule-based or probabilistic (i.e. statistical and, 
most recently, neural network-based) machine learning approaches. The 
goal is a computer capable of "understanding" the contents of documents, 
including the contextual nuances of the language within them. To this end, 
natural language processing often borrows ideas from theoretical 
linguistics. The technology can then accurately extract information and 
insights contained in the documents as well as categorize and organize the 
documents themselves.
‚Äú
‚Äù
Source: https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1215529997
focus in this lecture


What are common NLP tasks?
‚Ä¢ Text Classification / Sentiment Analysis / Moderation Systems
‚Ä¢ Summarization
‚Ä¢ Text Generation / Autocomplete / Recommendation
‚Ä¢ Assistant systems
‚Ä¢ Translation
‚Ä¢ Search / Retrieval / QA
‚Ä¢ Speech to Text
‚Ä¢ Entity Recognition (linking to Knowledge Bases)


What are common challenges in NLP?
‚Ä¢ Ambiguities / Homonyms
‚Ä¢ Computation
‚Ä¢ Speech 2 Text losses: Informal Speech / Filler Words, Utterances
‚Ä¢ Vectorization / Representation (flexible input lengths, vocab sizes‚Ä¶)
‚Ä¢ Typos
‚Ä¢ Dataset sizes
‚Ä¢ Languages, Character Sets, Writing styles, Accents
‚Ä¢ Hallucinations
‚Ä¢ Explainability
‚Ä¢ Biases in datasets / need for diverse datasets


Application Areas, Tasks & 
Examples


Text classification
‚Ä¢ Spam classification
‚Ä¢ Text Sentiment Analysis
Spam Classifier
Spam
Ham
Classifier
Negative
Positive
Often also as
scoring /
regression variant

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents the fundamental concepts of text classification and sentiment analysis. Text classification refers to the process of categorizing text data into predefined categories, such as spam or not spam. In this case, the diagram illustrates how a spam classifier is used to determine whether an email is spam or ham (not spam). The presence of an often also as scoring/regression variant suggests that these classification methods can be extended to include numerical scores or regression analysis.

On the other hand, sentiment analysis focuses on determining the emotional tone or attitude conveyed in a piece of text. The diagram shows a text sentiment analysis with a classifier that categorizes text into positive, negative, or neutral sentiments. This process is essential for understanding public opinion, customer feedback, and overall sentiment towards a product or service. As we delve into NLP, these concepts form the building blocks for more advanced techniques in understanding and interacting with human language. [IDE]


Text classification: Sentiment Analysis
Source: https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment

[IDS] In this NLP lecture, we are presented with a demonstration of text classification using sentiment analysis. The slide showcases the hosted inference API from Hugging Face, a popular platform for natural language processing. The interface allows users to input text, and it provides a classification result based on the sentiment, which in this case is labeled as "Examples". The code snippet provided is written in JSON format, indicating the structured data used for training the model. The output displays the computation time and the sentiment score, which can be used to determine the polarity of the input text. This tool is useful for understanding public opinion or user feedback in various applications, such as social media monitoring or customer service. [IDE]


Machine Translation (MT)
‚Ä¢ Google Translate (translate.google.com)
‚Ä¢ DeepL (www.deepl.com) 

[IDS] We are in a lecture about NLP, which stands for Natural Language Processing. The image shows a presentation slide with examples of machine translation from English to German using different services like Google Translate and DeepL. This demonstrates how NLP technology is used to translate text between languages, which is a fundamental aspect of understanding human language and communication. [IDE]


Keyword Extraction
‚Ä¢ Extract the most important 
phrases (keywords, key 
phrases) from a document
‚Ä¢ Token classification 
Source: https://huggingface.co/jasminejwebb/KeywordIdentifier 

[IDS] In the image, we are presented with a slide from an NLP lecture titled "Keyword Extraction". The slide emphasizes the importance of extracting key phrases (keywords) from a document, which is a fundamental process in Natural Language Processing (NLP). It explains that these keywords are identified through artificial intelligence and can be used to understand the content of a large amount of natural language data. Additionally, the slide introduces the concept of token classification, which involves computing the time it takes for various NLP tasks on a CPU, using examples such as natural language processing, logistics, computer science, and others. This slide serves as an introductory overview of keyword extraction and token classification within the field of NLP. [IDE]


Text Summarization
‚Ä¢ Produce a shorter version
‚Ä¢ Preserve important info
Source: https://huggingface.co/tasks/summarization

[IDS] The image you're seeing is a slide from an NLP (Natural Language Processing) lecture, specifically titled "Introduction into nlp". The slide focuses on the concept of text summarization, which is a key topic in NLP. It emphasizes the importance of producing shorter versions of text while preserving important information. The slide uses a simple diagram to visually represent the process of text summarization, where inputs are transformed into outputs. This process involves reducing the length of text while retaining its essence, which is a fundamental challenge and area of research in NLP. [IDE]


Image Captioning
‚Ä¢ Describe contents of an image
‚Ä¢ Generate a title for an image
Source: https://huggingface.co/tasks/image-to-text

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents the process of image captioning, which is a subfield of NLP. The image showcases how an input image, such as the giraffe and zebra, is processed to generate a detailed description and a title for the image. This involves using NLP techniques to analyze the visual content and produce textual descriptions that convey the essence of the image. The slide serves as an example of how NLP can bridge the gap between visual and textual data, enabling machines to understand and describe images in a human-like manner. [IDE]


Text Generation
‚Ä¢ Text continuations
‚Ä¢ Can also be conditioned
‚Ä¢ Context
‚Ä¢ Topic
‚Ä¢ Contents
‚Ä¢ Questions
‚Ä¢ Language
‚Ä¢ ‚Ä¶
Source: https://huggingface.co/tasks/text-generation

[IDS] In this NLP lecture, we're discussing the fundamentals of text generation, a key aspect of Natural Language Processing. The slide outlines the core components involved in generating text, such as inputs, context, topic, contents, questions, and language. It also emphasizes the importance of being able to condition these inputs and mentions the Text Generation Model as a significant part of our exploration. This model draws inspiration from historical figures like Alexander the Great and Chaucer, showcasing how NLP can bring ancient explorers and poets to life through modern technology. The source for this information is provided as https://huggingface.co/tasks/text-generation, indicating that this is a resource from Hugging Face, a well-known entity in the NLP community. [IDE]


Image Generation from Text (T2I)
‚Ä¢ Text to Image
‚Ä¢ Also interactive
Source: https://huggingface.co/tasks/text-to-image

[IDS] In this NLP lecture, we are exploring the concept of "Image Generation from Text" or T2I. This process involves using natural language processing to generate images from textual inputs. The slide presents a visual representation of the T2I model, where inputs like text descriptions and styles are processed to produce an output image. The example given is a cityscape described in a Victorian style, which is then transformed into a colorful and fantastical image. The source for this information is provided as a link to a GitHub repository containing tasks related to text-to-image generation. As we delve deeper into the topic, we learn about the intricacies of how language is translated into visual representations, highlighting the intersection of NLP and computer vision. [IDE]


Question Answering
‚Ä¢ Ask questions about text
‚Ä¢ Get answers
‚Ä¢ Variants:
‚Ä¢ Relevant passage given
‚Ä¢ Corpus based
‚Ä¢ General purpose model
Source: https://huggingface.co/tasks/question-answering

[IDS] The image depicts a slide from an NLP lecture, specifically addressing the concept of "Question Answering" within the field. It highlights various aspects such as asking questions about text, getting answers, and using different models like variants (relevant passage given, corpus-based, general purpose), context (Amazon rainforest, also known in English as Amazonia or the Amazon Jungle), and inputs (question and question answering model). The source is mentioned as a website with tasks and questions related to this topic. [IDE]


‚ÄúChat Bots‚Äù
‚Ä¢ IRC / Discord
‚Ä¢ Bot Users / Chat integration
‚Ä¢ Often keyword / rule based
‚Ä¢ Intent Recognition Systems:
‚Ä¢ Customer support (the annoying things on websites / phone hotlines)
‚Ä¢ (Air Canada Incident! https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know )
‚Ä¢ (To some degree after Speech 2 Text) Alexa, Cortana, Google Assistant, Siri 
‚Ä¢ Chat assistant / conversational AI systems
‚Ä¢ ChatGPT, Gemini (Bard), Copilot, Claude, ‚Ä¶
‚Ä¢ General Task Interfaces
[Lecture End]

[Lecture Start]

------------Text_processing------------
Regular expressions are used everywhere
‚ó¶Part of every text processing task
‚ó¶Not a general NLP solution (for that we use large NLP 
systems we will see in later lectures)
‚ó¶But very useful as part of those systems (e.g., for pre-
processing or text formatting)
‚ó¶Necessary for data analysis of text data
‚ó¶A widely used tool in industry and academics
2


Regular expressions
A formal language for specifying text strings
How can we search for mentions of these cute animals in text?
‚ó¶woodchuck
‚ó¶woodchucks
‚ó¶Woodchuck
‚ó¶Woodchucks
‚ó¶Groundhog
‚ó¶groundhogs

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a visual aid used to explain how to search for specific patterns or expressions within text data. The beaver, as a symbol of hard work and diligence, may metaphorically represent the meticulous process of searching through text strings using regular expressions. This technique is crucial in NLP for tasks such as data preprocessing, pattern matching, and text analysis. [IDE]


Regular Expressions: Disjunctions
Letters inside square brackets []
Ranges using the dash [A-Z]
 
 
Pattern
Matches
[wW]oodchuck
Woodchuck, woodchuck
[1234567890] 
Any one digit
Pattern
Matches
[A-Z]
An upper case letter
Drenched Blossoms
[a-z]
A lower case letter
my beans were impatient
[0-9]
A single digit
Chapter 1: Down the Rabbit Hole
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[wW]oodchuck</td>
      <td>Woodchuck, woodchuck</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[1234567890]</td>
      <td>Any one digit</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[A-Z]</td>
      <td>An upper case letter</td>
      <td>Drenched Blossoms</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[a-z]</td>
      <td>A lower case letter</td>
      <td>my beans were impatient</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[0-9]</td>
      <td>A single digit</td>
      <td>Chapter 1: Down the Rabbit Hole</td>
    </tr>
  </tbody>
</table>

Regular Expressions: Negation in Disjunction
Carat as first character in [] negates the list
‚ó¶Note: Carat means negation only when it's first in []
‚ó¶Special characters (., *, +, ?) lose their special meaning inside []
Pattern
Matches
Examples
[^A-Z]
Not an upper case letter
Oyfn pripetchik
[^Ss]
Neither ‚ÄòS‚Äô nor ‚Äòs‚Äô
I have no exquisite reason‚Äù
[^.]
Not a period
Our resident Djinn
[e^]
Either e or ^
Look up ^ now
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[^A-Z]</td>
      <td>Not an upper case letter</td>
      <td>Oyfn pripetchik</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[^Ss]</td>
      <td>Neither ‚ÄòS‚Äô nor ‚Äòs‚Äô</td>
      <td>I have no exquisite reason‚Äù</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[^.]</td>
      <td>Not a period</td>
      <td>Our resident Djinn</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[e^]</td>
      <td>Either e or ^</td>
      <td>Look up ^ now</td>
    </tr>
  </tbody>
</table>

Regular Expressions: Convenient aliases
Pattern
Expansion
Matches
Examples
\d
[0-9]
Any digit
Fahreneit 451
\D 
[^0-9]
Any non-digit
Blue Moon
\w
[a-ZA-Z0-9_]
Any alphanumeric or _
Daiyu
\W
[^\w]
Not alphanumeric or _
Look!
\s
[ \r\t\n\f]
Whitespace (space, tab)
Look‚ê£up
\S
[^\s]
Not whitespace
Look up
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Expansion</th>
      <th>Matches</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\d</td>
      <td>[0-9]</td>
      <td>Any digit</td>
      <td>Fahreneit 451</td>
    </tr>
    <tr>
      <th>1</th>
      <td>\D</td>
      <td>[^0-9]</td>
      <td>Any non-digit</td>
      <td>Blue Moon</td>
    </tr>
    <tr>
      <th>2</th>
      <td>\w</td>
      <td>[a-ZA-Z0-9_]</td>
      <td>Any alphanumeric or _</td>
      <td>Daiyu</td>
    </tr>
    <tr>
      <th>3</th>
      <td>\W</td>
      <td>[^\w]</td>
      <td>Not alphanumeric or _</td>
      <td>Look!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>\s</td>
      <td>[ \r\t\n\f]</td>
      <td>Whitespace (space, tab)</td>
      <td>Look‚ê£up</td>
    </tr>
    <tr>
      <th>5</th>
      <td>\S</td>
      <td>[^\s]</td>
      <td>Not whitespace</td>
      <td>Look up</td>
    </tr>
  </tbody>
</table>

Regular Expressions: More Disjunction
Groundhog is another name for woodchuck!
The pipe symbol | for disjunction
Pattern
Matches
groundhog|woodchuck
woodchuck
yours|mine
yours
a|b|c
= [abc]
[gG]roundhog|[Ww]oodchuck
Woodchuck
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>groundhog|woodchuck</td>
      <td>woodchuck</td>
    </tr>
    <tr>
      <th>1</th>
      <td>yours|mine</td>
      <td>yours</td>
    </tr>
    <tr>
      <th>2</th>
      <td>a|b|c</td>
      <td>= [abc]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[gG]roundhog|[Ww]oodchuck</td>
      <td>Woodchuck</td>
    </tr>
  </tbody>
</table>
[IDS] The image is a visual aid for a lecture on Natural Language Processing (NLP), specifically focusing on text processing. It illustrates the concept of "Regular Expressions" as a tool for disjunction, which is a way to match patterns in text data. The example given is the word "Groundhog" and its alternative name "Woodchuck." The slide provides a pattern and matches section, showing how regular expressions can be used to find occurrences of these names in text. This is relevant to NLP as it demonstrates how computational methods can be applied to process and analyze natural language texts. [IDE]


Wildcards, optionality, repetition: . ? * +
Stephen C Kleene
Pattern
Matches
Examples
beg.n
Any char
begin
begun 
beg3n  beg n
woodchucks?
Optional s
woodchuck 
woodchucks
to*
0 or more of 
previous char
t to too tooo
to+
1 or more of 
previous char
to too tooo 
toooo
Kleene *,   Kleene +  
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>beg.n</td>
      <td>Any char</td>
      <td>begin begun\nbeg3n beg n</td>
    </tr>
    <tr>
      <th>1</th>
      <td>woodchucks?</td>
      <td>Optional s</td>
      <td>woodchuck\nwoodchucks</td>
    </tr>
    <tr>
      <th>2</th>
      <td>to*</td>
      <td>0 or more of\nprevious char</td>
      <td>t to too tooo</td>
    </tr>
    <tr>
      <th>3</th>
      <td>to+</td>
      <td>1 or more of\nprevious char</td>
      <td>to too tooo\ntoooo</td>
    </tr>
  </tbody>
</table>
[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on text processing techniques. It introduces the concept of wildcards and optionality in regular expressions, which are crucial for pattern matching in text data. The slide outlines how to use these concepts to match different patterns, such as any character, optional sequences, zero or more occurrences of a previous character, and one or more occurrences of a previous character. These patterns are essential in NLP for tasks like tokenization, parsing, and search algorithms. The examples provided demonstrate how to apply these patterns in regular expressions. The presence of the name "Stephen C Kleene" suggests that the lecture might be discussing the contributions of Stephen Kleene, a mathematician known for his work on regular expressions and formal languages. [IDE]


Regular Expressions: Anchors  ^   $
Pattern
Matches
^[A-Z] 
Palo Alto
^[^A-Za-z] 
1    ‚ÄúHello‚Äù
\.$ 
The end.
.$ 
The end?  The end!
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pattern</th>
      <th>Matches</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>^[A-Z]</td>
      <td>Palo Alto</td>
    </tr>
    <tr>
      <th>1</th>
      <td>^[^A-Za-z]</td>
      <td>1 ‚ÄúHello‚Äù</td>
    </tr>
    <tr>
      <th>2</th>
      <td>\.$</td>
      <td>The end.</td>
    </tr>
    <tr>
      <th>3</th>
      <td>.$</td>
      <td>The end? The end!</td>
    </tr>
  </tbody>
</table>

A note about Python regular expressions
‚ó¶Regex and Python both use backslash "\" for 
special characters. You must type extra backslashes!
‚ó¶"\\d+"  to search for 1 or more digits
‚ó¶"\n" in Python means the "newline" character, not a 
"slash" followed by an "n". Need "\\n" for two characters.
‚ó¶Instead: use Python's raw string notation for regex:
‚ó¶r"[tT]he"
‚ó¶r"\d+" matches one or more digits
‚ó¶instead of "\\d+"
10


The iterative process of writing regex's
Find me all instances of the word ‚Äúthe‚Äù in a text.
the
Misses capitalized examples
[tT]he
Incorrectly returns other or Theology
\W[tT]he\W


False positives and false negatives
The process we just went through was based on 
fixing two kinds of errors:
1. Not matching things that we should have matched 
(The)
False negatives
2. Matching strings that we should not have matched 
(there, then, other)
False positives


Characterizing work on NLP
In NLP we are always dealing with these kinds of errors.
Reducing the error rate for an application often 
involves two antagonistic efforts: 
‚ó¶Increasing coverage (or recall) (minimizing false negatives).
‚ó¶Increasing accuracy (or precision) (minimizing false positives)


Regular expressions play a surprisingly large role
Widely used in both academics and industry
1. Part of most text processing tasks, even for big 
neural language model pipelines
‚ó¶including text formatting and pre-processing
2. Very useful for data analysis of any text data
14


Basic Text 
Processing
Regular Expressions


Basic Text 
Processing
More Regular Expressions: 
Substitutions and ELIZA


Substitutions
Substitution in Python and UNIX commands:
s/regexp1/pattern/ 
e.g.:
s/colour/color/ 


Capture Groups
‚Ä¢ Say we want to put angles around all numbers:
the 35 boxes √† the <35> boxes 
‚Ä¢ Use parens () to "capture" a pattern into a 
numbered register (1, 2, 3‚Ä¶)
‚Ä¢ Use \1  to refer to the contents of the register
s/([0-9]+)/<\1>/ 
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>the</th>
      <th>&lt;</th>
      <th>35</th>
      <th>&gt;</th>
      <th>boxes</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

Capture groups: multiple registers
/the (.*)er they (.*), the \1er we \2/ 
Matches
      the faster they ran, the faster we ran 
But not
      the faster they ran, the faster we ate 


But suppose we don't want to capture?
Parentheses have a double function: grouping terms, and 
capturing
Non-capturing groups: add a ?: after paren:
/(?:some|a few) (people|cats) like some \1/ 
matches 
‚ó¶some cats like some cats 
but not 
‚ó¶some cats like some some


Lookahead assertions
(?= pattern) is true if pattern matches, but is 
zero-width; doesn't advance character pointer
(?! pattern) true if a pattern does not match 
How to match, at the beginning of a line, any single 
word that doesn‚Äôt start with ‚ÄúVolcano‚Äù: 
/ÀÜ(?!Volcano)[A-Za-z]+/ 


Simple Application: ELIZA
Early NLP system that imitated a Rogerian 
psychotherapist 
‚ó¶Joseph Weizenbaum, 1966. 
Uses pattern matching to match, e.g.,:
‚ó¶‚ÄúI need X‚Äù 
and translates them into, e.g.
‚ó¶‚ÄúWhat would it mean to you if you got X? 


Simple Application: ELIZA
Men are all alike.
IN WHAT WAY
They're always bugging us about something or other. 
CAN YOU THINK OF A SPECIFIC EXAMPLE 
Well, my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE 
He says I'm depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED 


How ELIZA works
s/.* I‚ÄôM (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \1/ 
s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAT WAY?/ 
s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE?/ 


Basic Text 
Processing
More Regular Expressions: 
Substitutions and ELIZA


Basic Text 
Processing
Words and Corpora


How many words in a sentence?
"I do uh main- mainly business data processing"
‚ó¶Fragments, filled pauses
"Seuss‚Äôs cat in the hat is different from other cats!" 
‚ó¶Lemma: same stem, part of speech, rough word sense
‚ó¶cat and cats = same lemma
‚ó¶Wordform: the full inflected surface form
‚ó¶cat and cats = different wordforms


How many words in a sentence?
they lay back on the San Francisco grass and looked at the stars 
and their
Type: an element of the vocabulary.
Token: an instance of that type in running text.
How many?
‚ó¶15 tokens (or 14)
‚ó¶13 types (or 12) (or 11?)


How many words in a corpus?
N = number of tokens
V = vocabulary = set of types, |V| is size of vocabulary
Heaps Law = Herdan's Law =                                 where often .67 < Œ≤ < .75
i.e., vocabulary size grows with > square root of the number of word tokens
Tokens = N
Types = |V|
Switchboard phone conversations
2.4 million
20 thousand
Shakespeare
884,000
31 thousand
COCA
440 million
2 million
Google N-grams
1 trillion
13+ million
and in fact this relationship between the number of types |V| and nu
 N is called Herdan‚Äôs Law (Herdan, 1960) or Heaps‚Äô Law (Heaps, 1
iscoverers (in linguistics and information retrieval respectively). It is sh
1, where k and b are positive constants, and 0 < b < 1.
|V| = kNb
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>Tokens = N</th>
      <th>Types = |V|</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Switchboard phone conversations</td>
      <td>2.4 million</td>
      <td>20 thousand</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Shakespeare</td>
      <td>884,000</td>
      <td>31 thousand</td>
    </tr>
    <tr>
      <th>2</th>
      <td>COCA</td>
      <td>440 million</td>
      <td>2 million</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Google N-grams</td>
      <td>1 trillion</td>
      <td>13+ million</td>
    </tr>
  </tbody>
</table>

Corpora
Words don't appear out of nowhere! 
A text is produced by 
‚Ä¢ a specific writer(s), 
‚Ä¢ at a specific time, 
‚Ä¢ in a specific variety,
‚Ä¢ of a specific language, 
‚Ä¢ for a specific function.


Corpora vary along dimension like
‚ó¶Language: 7097 languages in the world
‚ó¶Variety, like African American Language varieties.
‚ó¶AAE Twitter posts might include forms like "iont" (I don't)
‚ó¶Code switching, e.g., Spanish/English, Hindi/English:
 
S/E: Por primera vez veo a @username actually being hateful! It was beautiful:) 
 
   [For the first time I get to see @username actually being hateful! it was beautiful:) ] 
 
H/E: dost tha or ra- hega ... dont wory ... but dherya rakhe 
 
   [‚Äúhe was and will remain a friend ... don‚Äôt worry ... but have faith‚Äù] 
‚ó¶Genre: newswire, fiction, scientific articles, Wikipedia
‚ó¶Author Demographics: writer's age, gender, ethnicity, SES 


Corpus datasheets
Motivation: 
‚Ä¢ Why was the corpus collected?
‚Ä¢ By whom? 
‚Ä¢ Who funded it? 
Situation: In what situation was the text written?
Collection process: If it is a subsample how was it sampled? Was 
there consent? Pre-processing?
   +Annotation process, language variety, demographics, etc.
Gebru et al (2020), Bender and Friedman (2018)


Basic Text 
Processing
Words and Corpora


Basic Text 
Processing
Word tokenization


Text Normalization
Every NLP task requires text normalization: 
1. Tokenizing (segmenting) words
2. Normalizing word formats
3. Segmenting sentences


Space-based tokenization
A very simple way to tokenize
‚ó¶For languages that use space characters between words
‚ó¶Arabic, Cyrillic, Greek, Latin, etc., based writing systems
‚ó¶Segment off a token between instances of spaces
Unix tools for space-based tokenization
‚ó¶The "tr" command
‚ó¶Inspired by Ken Church's UNIX for Poets
‚ó¶Given a text file, output the word tokens and their frequencies


Simple Tokenization in UNIX
(Inspired by Ken Church‚Äôs UNIX for Poets.)
Given a text file, output the word tokens and their frequencies
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < shakes.txt
| sort 
     | uniq ‚Äìc 
1945 A
  72 AARON
  19 ABBESS
   5 ABBOT
 ... ...
25 Aaron
6 Abate
1 Abates
5 Abbess
6 Abbey
3 Abbot
....   ‚Ä¶
Change all non-alpha to newlines
Sort in alphabetical order
Merge and count each type


The first step: tokenizing
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < shakes.txt | head
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
We
...


The second step: sorting
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < shakes.txt | sort | head
A
A
A
A
A
A
A
A
A
...   


More counting
Merging upper and lower case
tr ‚ÄòA-Z‚Äô ‚Äòa-z‚Äô < shakes.txt | tr ‚Äìsc ‚ÄòA-Za-z‚Äô ‚Äò\n‚Äô | sort | uniq ‚Äìc 
Sorting the counts
tr ‚ÄòA-Z‚Äô ‚Äòa-z‚Äô < shakes.txt | tr ‚Äìsc ‚ÄòA-Za-z‚Äô ‚Äò\n‚Äô | sort | uniq ‚Äìc | sort ‚Äìn ‚Äìr
23243 the
22225 i
18618 and
16339 to
15687 of
12780 a
12163 you
10839 my
10005 in
8954  d
What happened here?


Issues in Tokenization
Can't just blindly remove punctuation:
‚ó¶m.p.h., Ph.D., AT&T, cap‚Äôn
‚ó¶prices ($45.55)
‚ó¶dates (01/02/06)
‚ó¶URLs (http://www.stanford.edu)
‚ó¶hashtags (#nlproc)
‚ó¶email addresses (someone@cs.colorado.edu)
Clitic: a word that doesn't stand on its own
‚ó¶"are" in we're, French "je" in j'ai, "le" in l'honneur
When should multiword expressions (MWE) be words?
‚ó¶New York, rock ‚Äôn‚Äô roll 


Tokenization in NLTK
ficient finite state automata. For example, Fig. 2.12 shows an example of a basic
regular expression that can be used to tokenize with the nltk.regexp tokenize
function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009;
http://www.nltk.org).
>>> text = ‚ÄôThat U.S.A. poster-print costs $12.40...‚Äô
>>> pattern = r‚Äô‚Äô‚Äô(?x)
# set flag to allow verbose regexps
...
([A-Z]\.)+
# abbreviations, e.g. U.S.A.
...
| \w+(-\w+)*
# words with optional internal hyphens
...
| \$?\d+(\.\d+)?%?
# currency and percentages, e.g. $12.40, 82%
...
| \.\.\.
# ellipsis
...
| [][.,;"‚Äô?():-_‚Äò]
# these are separate tokens; includes ], [
... ‚Äô‚Äô‚Äô
>>> nltk.regexp_tokenize(text, pattern)
[‚ÄôThat‚Äô, ‚ÄôU.S.A.‚Äô, ‚Äôposter-print‚Äô, ‚Äôcosts‚Äô, ‚Äô$12.40‚Äô, ‚Äô...‚Äô]
Figure 2.12
A Python trace of regular expression tokenization in the NLTK Python-based
natural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)
verbose flag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird
et al. (2009).
Carefully designed deterministic algorithms can deal with the ambiguities that
arise, such as the fact that the apostrophe needs to be tokenized differently when used
as a genitive marker (as in the book‚Äôs cover), a quotative as in ‚ÄòThe other class‚Äô, she
Bird, Loper and Klein (2009), Natural Language Processing with Python. O‚ÄôReilly


Tokenization in languages without spaces 
Many languages (like Chinese, Japanese, Thai) don't 
use spaces to separate words!
How do we decide where the token boundaries 
should be?


Word tokenization in Chinese
Chinese words are composed of characters called 
"hanzi" (or sometimes just "zi")
Each one represents a meaning unit called a morpheme.
Each word has on average 2.4 of them.
But deciding what counts as a word is complex and not 
agreed upon.


How to do word tokenization in Chinese?
ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ  ‚ÄúYao Ming reaches the finals‚Äù
3 words?
ÂßöÊòé        ËøõÂÖ•
ÊÄªÂÜ≥Ëµõ
YaoMing  reaches  finals 
5 words?
Âßö      Êòé
ËøõÂÖ•
ÊÄª
ÂÜ≥Ëµõ
Yao    Ming    reaches    overall    finals 
7 characters? (don't use words at all):
ÂßöÊòé
Ëøõ
ÂÖ•
ÊÄª
ÂÜ≥
Ëµõ
Yao Ming enter enter overall decision game
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ ‚ÄúYao Ming reaches the finals‚Äù</th>
      <th>Col1</th>
      <th>Col2</th>
      <th>Col3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3 words?\nÂßöÊòé ËøõÂÖ• ÊÄªÂÜ≥Ëµõ\nYaoMing reaches finals</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5 words?\nÂßö Êòé ËøõÂÖ• ÊÄª ÂÜ≥Ëµõ\nYao Ming reaches overall finals\n7 characters? (don't use words at all):\nÂßö Êòé Ëøõ ÂÖ• ÊÄª ÂÜ≥ Ëµõ\nYao Ming enter enter overall decision game</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>7 characters? (don't use words at all):\nÂßö Êòé Ëøõ ÂÖ• ÊÄª ÂÜ≥ Ëµõ\nYao Ming enter enter overall decision game</td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>

How to do word tokenization in Chinese?
ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ  ‚ÄúYao Ming reaches the finals‚Äù
3 words?
ÂßöÊòé        ËøõÂÖ•
ÊÄªÂÜ≥Ëµõ
YaoMing  reaches  finals 
5 words?
Âßö      Êòé
ËøõÂÖ•
ÊÄª
ÂÜ≥Ëµõ
Yao    Ming    reaches    overall    finals 
7 characters? (don't use words at all):
ÂßöÊòé
Ëøõ
ÂÖ•
ÊÄª
ÂÜ≥
Ëµõ
Yao Ming enter enter overall decision game
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>3 words? ÂßöÊòé    ËøõÂÖ• ÊÄªÂÜ≥Ëµõ YaoMing reaches finals</th>
      <th>Col1</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5 words?\nÂßö Êòé ËøõÂÖ• ÊÄª ÂÜ≥Ëµõ\nYao Ming reaches overall finals\n7 characters? (don't use words at all):\nÂßö Êòé Ëøõ ÂÖ• ÊÄª ÂÜ≥ Ëµõ\nYao Ming enter enter overall decision game</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>7 characters? (don't use words at all):\nÂßö Êòé Ëøõ ÂÖ• ÊÄª ÂÜ≥ Ëµõ\nYao Ming enter enter overall decision game</td>
      <td></td>
    </tr>
  </tbody>
</table>

How to do word tokenization in Chinese?
ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ  ‚ÄúYao Ming reaches the finals‚Äù
3 words?
ÂßöÊòé        ËøõÂÖ•
ÊÄªÂÜ≥Ëµõ
YaoMing  reaches  finals 
5 words?
Âßö      Êòé
ËøõÂÖ•
ÊÄª
ÂÜ≥Ëµõ
Yao    Ming    reaches    overall    finals 
7 characters? (don't use words at all):
ÂßöÊòé
Ëøõ
ÂÖ•
ÊÄª
ÂÜ≥
Ëµõ
Yao Ming enter enter overall decision game


How to do word tokenization in Chinese?
ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ  ‚ÄúYao Ming reaches the finals‚Äù
3 words?
ÂßöÊòé        ËøõÂÖ•
ÊÄªÂÜ≥Ëµõ
YaoMing  reaches  finals 
5 words?
Âßö      Êòé
ËøõÂÖ•
ÊÄª
ÂÜ≥Ëµõ
Yao    Ming    reaches    overall    finals 
7 characters? (don't use words at all):
ÂßöÊòé
Ëøõ
ÂÖ•
ÊÄª
ÂÜ≥
Ëµõ
Yao Ming enter enter overall decision game


Word tokenization / segmentation
So in Chinese it's common to just treat each character 
(zi) as a token.
‚Ä¢ So the segmentation step is very simple
In other languages (like Thai and Japanese), more 
complex word segmentation is required.
‚Ä¢ The standard algorithms are neural sequence models 
trained by supervised machine learning.


Basic Text 
Processing
Word tokenization


Basic Text 
Processing
Word Normalization and 
other issues


Word Normalization
Putting words/tokens in a standard format
‚ó¶U.S.A. or USA
‚ó¶uhhuh or uh-huh
‚ó¶Fed or fed
‚ó¶am, is, be, are 


Case folding
Applications like IR: reduce all letters to lower case
‚ó¶Since users tend to use lower case
‚ó¶Possible exception: upper case in mid-sentence?
‚ó¶e.g., General Motors
‚ó¶Fed vs. fed
‚ó¶SAIL vs. sail
For sentiment analysis, MT, Information extraction
‚ó¶Case is helpful (US versus us is important)


Lemmatization
Represent all words as their lemma, their shared root 
 
= dictionary headword form:
‚ó¶am, are, is ¬Æ be
‚ó¶car, cars, car's, cars' ¬Æ car
‚ó¶Spanish quiero (‚ÄòI want‚Äô), quieres (‚Äòyou want‚Äô) 
¬Æ querer ‚Äòwant'
‚ó¶He is reading detective stories 
¬Æ He be read detective story 


Lemmatization is done by Morphological Parsing
Morphemes:
‚ó¶The small meaningful units that make up words
‚ó¶Stems: The core meaning-bearing units
‚ó¶Affixes: Parts that adhere to stems, often with grammatical 
functions
Morphological Parsers:
‚ó¶Parse  cats into two morphemes cat and s
‚ó¶Parse Spanish amaren (‚Äòif in the future they would love‚Äô) into 
morpheme amar ‚Äòto love‚Äô, and the morphological features 
3PL and future subjunctive. 


Stemming
Reduce terms to stems, chopping off affixes crudely
This was not the map we 
found in Billy Bones‚Äôs 
chest, but an accurate 
copy, complete in all 
things-names and heights 
and soundings-with the 
single exception of the 
red crosses and the 
written notes. 
Thi wa not the map we 
found in Billi Bone s chest 
but an accur copi complet 
in all thing name and 
height and sound with the 
singl except of the red 
cross and the written note 
. 


Porter Stemmer
Based on a series of rewrite rules run in series
‚ó¶A cascade, in which output of each pass fed to next pass
Some sample rules:
and soundings-with the single exception of the red crosses
and the written notes.
produces the following stemmed output:
Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
The algorithm is based on series of rewrite rules run in series, as a cascade, in
ascade
which the output of each pass is fed as input to the next pass; here is a sampling of
the rules:
ATIONAL ! ATE (e.g., relational ! relate)
ING ! ‚úè
if stem contains vowel (e.g., motoring ! motor)
SSES ! SS
(e.g., grasses ! grass)
Detailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)
can be found on Martin Porter‚Äôs homepage; see also the original paper (Porter, 1980).
Simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. Nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (Krovetz, 1993):
Errors of Commission
Errors of Omission
organization organ
European Europe
doing
doe
analysis
analyzes


Dealing with complex morphology is necessary 
for many languages
‚ó¶e.g., the Turkish word:
‚ó¶Uygarlastiramadiklarimizdanmissinizcasina
‚ó¶`(behaving) as if you are among those whom we could not civilize‚Äô
‚ó¶Uygar `civilized‚Äô + las `become‚Äô 
+ tir `cause‚Äô + ama `not able‚Äô 
+ dik `past‚Äô + lar ‚Äòplural‚Äô
+ imiz ‚Äòp1pl‚Äô + dan ‚Äòabl‚Äô 
+ mis ‚Äòpast‚Äô + siniz ‚Äò2pl‚Äô + casina ‚Äòas if‚Äô 


Sentence Segmentation
!, ? mostly unambiguous but period ‚Äú.‚Äù is very ambiguous
‚ó¶Sentence boundary
‚ó¶Abbreviations like Inc. or Dr.
‚ó¶Numbers like .02% or 4.3
Common algorithm: Tokenize first: use rules or ML to 
classify a period as either (a) part of the word or (b) a 
sentence-boundary. 
‚ó¶An abbreviation dictionary can help
Sentence segmentation can then often be done by rules 
based on this tokenization.


Basic Text 
Processing
Word Normalization and 
other issues


Basic Text 
Processing
Byte Pair Encoding


Another option for text tokenization
Instead of 
‚Ä¢ white-space segmentation
‚Ä¢ single-character segmentation 
Use the data to tell us how to tokenize.
Subword tokenization (because tokens can be parts 
of words as well as whole words)


Subword tokenization
Three common algorithms:
‚ó¶Byte-Pair Encoding (BPE) (Sennrich et al., 2016)
‚ó¶Unigram language modeling tokenization (Kudo, 2018)
‚ó¶WordPiece (Schuster and Nakajima, 2012)
All have 2 parts:
‚ó¶A token learner that takes a raw training corpus and induces 
a vocabulary (a set of tokens). 
‚ó¶A token segmenter that takes a raw test sentence and 
tokenizes it according to that vocabulary


Byte Pair Encoding (BPE) token learner
Let vocabulary be the set of all individual characters 
 
= {A, B, C, D,‚Ä¶, a, b, c, d‚Ä¶.}
Repeat:
‚ó¶Choose the two symbols that are most frequently 
adjacent in the training corpus (say 'A', 'B') 
‚ó¶Add a new merged symbol 'AB' to the vocabulary
‚ó¶Replace every adjacent 'A' 'B' in the corpus with 'AB'. 
Until k merges have been done.


BPE token learner algorithm
2.4
‚Ä¢
TEXT NORMALIZATION
19
function BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V
V all unique characters in C
# initial set of tokens is characters
for i = 1 to k do
# merge tokens til k times
tL, tR  Most frequent pair of adjacent tokens in C
tNEW  tL + tR
# make new token by concatenating
V V + tNEW
# update the vocabulary
Replace each occurrence of tL, tR in C with tNEW
# and update the corpus
return V
Figure 2.13
The token learner part of the BPE algorithm for taking a corpus broken up
into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.
Figure adapted from Bostrom and Durrett (2020).
from the training data, greedily, in the order we learned them. (Thus the frequencies
in the test data don‚Äôt play a role, just the frequencies in the training data). So first
we segment each test sentence word into characters. Then we apply the first rule:


Byte Pair Encoding (BPE) Addendum
Most subword algorithms are run inside space-
separated tokens. 
So we commonly first add a special end-of-word 
symbol '__' before space in training corpus
Next, separate into letters.


BPE token learner
ER 2
‚Ä¢
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
The algorithm is usually run inside words (not merging across word boundaries),
so the input corpus is first white-space-separated to give a set of strings, each corre-
sponding to the characters of a word, plus a special end-of-word symbol
, and its
counts. Let‚Äôs see its operation on the following tiny input corpus of 18 word tokens
with counts for each word (the word low appears 5 times, the word newer 6 times,
and so on), which would have a starting vocabulary of 11 letters:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w
2
l o w e s t
6
n e w e r
3
w i d e r
2
n e w
The BPE algorithm first count all pairs of adjacent symbols: the most frequent
is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences1. We then merge these symbols, treating er as one
symbol, and count again:
Original (very fascinatingüôÑ) corpus:
low low low low low lowest lowest newer newer newer    
newer newer newer wider wider wider new new
Add end-of-word tokens, resulting in this vocabulary:
representation

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide discussing the BPE (Byte Pair Encoding) token learner algorithm. The original, very fascinating corpus mentioned refers to the input text that the algorithm processes. The slide explains how BPE works by iteratively replacing the lowest frequency word pair with a new token until no more pairs can be replaced. The resulting vocabulary, listed as an example in the slide, is a set of tokens derived from the corpus through this process. This technique is essential in NLP for creating better word representations and improving models' performance on tasks like language translation and text summarization. [IDE]


BPE token learner
sponding to the characters of a word, plus a special end-of-word symbol
, and its
counts. Let‚Äôs see its operation on the following tiny input corpus of 18 word tokens
with counts for each word (the word low appears 5 times, the word newer 6 times,
and so on), which would have a starting vocabulary of 11 letters:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w
2
l o w e s t
6
n e w e r
3
w i d e r
2
n e w
The BPE algorithm first count all pairs of adjacent symbols: the most frequent
is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences1. We then merge these symbols, treating er as one
symbol, and count again:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
Merge e r to er
and so on), which would have a starting vocabulary of 11 letters:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w
2
l o w e s t
6
n e w e r
3
w i d e r
2
n e w
The BPE algorithm first count all pairs of adjacent symbols: the most frequent
is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences1. We then merge these symbols, treating er as one
symbol, and count again:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er
2
l o w e s t


BPE
Merge er  _ to er_
The BPE algorithm first count all pairs of adjacent symbols: the most frequent
is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences1. We then merge these symbols, treating er as one
symbol, and count again:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Next n e (total count of 8) get merged to ne:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er , ne
2
l o w e s t
6
ne w er
is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences1. We then merge these symbols, treating er as one
symbol, and count again:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Next n e (total count of 8) get merged to ne:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er , ne
2
l o w e s t
6
ne w er


BPE
Merge n  e  to ne
3
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Next n e (total count of 8) get merged to ne:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er , ne
2
l o w e s t
6
ne w er
3
w i d er
2
ne w
If we continue, the next merges are:
Merge
Current Vocabulary
(ne, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new
(l, o)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo
(lo, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low
(new, er )
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer
3
w i d er
2
n e w
Now the most frequent pair is er
, which we merge; our system has learned
that there should be a token for word-final er, represented as er :
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Next n e (total count of 8) get merged to ne:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er , ne
2
l o w e s t
6
ne w er
3
w i d er
2
ne w
If we continue, the next merges are:
Merge
Current Vocabulary
(ne, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new
(l, o)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo
(lo, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low
(new, er )
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer


BPE
The next merges are:
corpus
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, er, er , ne
2
l o w e s t
6
ne w er
3
w i d er
2
ne w
If we continue, the next merges are:
Merge
Current Vocabulary
(ne, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new
(l, o)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo
(lo, w)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low
(new, er )
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer
(low,
)
, d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer , low
Once we‚Äôve learned our vocabulary, the token parser is used to tokenize a te
sentence. The token parser just runs on the test data the merges we have learne
1
Note that there can be ties; we could have instead chosen to merge r
first, since that also has
frequency of 9.


BPE token segmenter algorithm
On the test data, run each merge learned from the 
training data:
‚ó¶Greedily
‚ó¶In the order we learned them
‚ó¶(test frequencies don't play a role)
So: merge every e r to er, then merge er _ to er_, etc.
Result: 
‚ó¶Test set "n e w e r _" would be tokenized as a full word 
‚ó¶Test set "l o w e r _" would be two tokens: "low er_"


Properties of BPE tokens
Usually include frequent words
And frequent subwords
‚Ä¢ Which are often morphemes like -est or ‚Äìer
A morpheme is the smallest meaning-bearing unit of a 
language
‚Ä¢ unlikeliest has 3 morphemes un-, likely, and -est 
[Lecture End]

[Lecture Start]

------------Transformer_decoder_and_Large_Langauge_Models------------

Large Language Models
Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)

[IDS] The image you're seeing is a visual representation of the evolution and structure of Large Language Models (LLMs) in Natural Language Processing (NLP). It's likely part of a lecture or presentation on NLP, specifically focusing on the Transformer decoder and Large Language Models. The figure shows how different models have been developed over time, starting from GPT-1 in 2015 to more recent models like GPT-7 and GShard. Each model is represented by a node in the tree, with lines indicating the direction of development and connections showing the relationships between them. This kind of visualization helps students and professionals understand the progression and interconnections within the field of LLMs. [IDE]



Large Language Models
Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)
Encoder Models (previous 
lecture on Transformers)

[IDS] The image is a visual representation of the evolution of transformer models and large language models, which are key topics in natural language processing (NLP). The flowchart starts with the basic architecture of transformers and moves towards more advanced and recent models. It's likely that this chart was used in a lecture to help students understand how these models have developed over time and their interconnections. As we are in a NLP lecture, this image serves as a guide to understanding the complex landscape of transformer models and their role in NLP tasks such as language generation and translation. [IDE]



Large Language Models
Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)
Decoder Models 
(this lecture)

[IDS] In this NLP lecture, we are discussing the evolution of language models and their impact on natural language processing. The slide presents a visual representation of the progression from early language models to more advanced ones like GPT-3 and beyond. It highlights the role of large language models in tasks such as translation and summarization, emphasizing their importance in the field. Additionally, there's a reference to a decoder model, which is likely a specific type of neural network used for generating text or making predictions in sequence-to-sequence tasks. The lecture aims to provide an overview of the current state-of-the-art in language modeling and its potential applications. [IDE]



Transformer Architecture
Fig. 2: Transformer Encoder Decoder 
(source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image you see is a schematic representation of the Transformer architecture, which is a fundamental component in large language models. The Transformer model, proposed by Vaswani et al. in 2017, has become a cornerstone in natural language processing tasks due to its ability to process sequential data effectively. The diagram illustrates the encoder and decoder components of the Transformer model, with attention mechanisms that allow the model to focus on different parts of the input sequence during translation. Understanding this architecture is crucial for NLP lectures as it forms the basis for many state-of-the-art models used in tasks such as machine translation, text generation, and question answering. [IDE]



Transformer Architecture
Fig. 2: Transformer Encoder Decoder 
(source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In the context of a NLP lecture, specifically on the topic "Transformer Decoder and Large Language Models," this image serves as a visual aid to explain the architecture of Transformer models. It highlights the components and flow of information within a Transformer Encoder Decoder structure. Understanding this architecture is crucial for grasping how these models process input data and generate outputs in tasks such as language translation or text generation. The image's focus on the encoder-decoder aspect suggests that attention mechanisms are key to the model's ability to understand and respond to input, which is a central theme in large language models. [IDE]



Decoder
Fig. 3: Transformer Decoder (source: 
Attention is all you need. Vaswani et 
al. 2017)
‚óèWe do not have an encoder so we throw 
away the Multi-Head Attention block

[IDS] The image represents a simplified explanation of how the Transformer Decoder works in the context of Large Language Models within Natural Language Processing (NLP). The lecture seems to focus on understanding the architecture and components of these models. [IDE]



Decoder
‚óèWe do not have an encoder so we throw 
away the Multi-Head Attention block
‚óèThis is the basic GPT architecture 
(generative pretrained transformer)
‚óèThis decoder can generate text by 
predicting the next token
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In this NLP lecture, we are discussing the Transformer Decoder and its role in Large Language Models. The slide illustrates the architecture of a Transformer Decoder, which is a key component in GPT (Generative Pre-trained Transformer) models. It emphasizes that unlike traditional encoders, these decoders do not throw away Multi-Head Attention blocks, which are essential for generating text predictions. The diagram shows the flow of data through the decoder, including positional encoding and masked multi-head attention, which allows the model to generate text from the next token. The slide also references Figure 4, which details the Transformer Decoder architecture and mentions Vaswani et al. (2017), indicating that this is a foundational work in the field. [IDE]



Decoder - Inputs
‚óèWe start with a partial sequence. 
Example: <s> Hello
‚óèWe tokenize this: [‚Äò<s>‚Äô, ‚ÄòHello‚Äô]
‚óèWe lookup the token ids: [0, 245]
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In this NLP lecture, we are discussing the Transformer Decoder, a key component of large language models. The diagram illustrates the architecture of the Transformer Decoder, which consists of multiple layers of attention mechanisms and feed-forward networks. These layers process the input tokens sequentially to generate the output tokens. The decoder starts with a partial sequence of examples and tokenizes it using a special token like '<s>'. It then looks up the token IDs in a vocabulary of size 245. This process involves positional encoding to maintain the order of tokens and outputs a shifted right sequence of outputs. The lecture emphasizes the importance of attention in the Transformer Decoder, as it allows the model to focus on different parts of the input sequence while generating the output. [IDE]



Decoder - Inputs
‚óèWe start with a partial sequence. 
Example: <s> Hello
‚óèWe tokenize this: [‚Äò<s>‚Äô, ‚ÄòHello‚Äô]
‚óèWe lookup the token ids: [0, 245]
‚óèWe start with initial embeddings: 
[E<s>,EHello]
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual representation of a Transformer Decoder, a component used in large language models within the field of Natural Language Processing (NLP). It illustrates the process of decoding and generating text from encoded input. The decoder consists of multiple layers, each with self-attention mechanisms that allow it to consider different parts of the input sequence simultaneously. This architecture enables the model to generate coherent and contextually relevant outputs. The figure serves as an educational tool to help students and researchers understand how the Transformer Decoder works and its role in NLP tasks such as machine translation and language generation. [IDE]



Decoder - Inputs
‚óèWe start with a partial sequence. 
Example: <s> Hello
‚óèWe tokenize this: [‚Äò<s>‚Äô, ‚ÄòHello‚Äô]
‚óèWe lookup the token ids: [0, 245]
‚óèWe start with initial embeddings: 
[y<s>,yHello]
‚óèWe create the positional embeddings:
[P0, P1]
‚óèWe add these together:
[y<s>+P0, yHello+P1] = [x<s>, xHello]
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image represents a section of a lecture on Natural Language Processing (NLP), specifically focusing on Transformer Decoders and Large Language Models. It visually outlines the process of how a decoder, part of a Transformer architecture, works to generate text from a partial sequence. The diagram breaks down the steps taken by the decoder, including tokenization, initial embeddings, positional encodings, and finally outputs the next tokens in the sequence. This is a fundamental concept in NLP, particularly in the field of machine translation and language generation tasks, where such models are used to produce coherent and contextually relevant text. [IDE]



Decoder - Attention
‚óèOur embeddings: [x<s>, xHello]
‚óèPerform Multi-Head Attention:
Attention weight matrix = QKT
These tell us how much information about 
the other tokens we should take into 
account for each token.
‚óèWe mask the attention weights, such that 
a token can not attend to any subsequent 
token. In this example the embedding for 
<s> should not get any information about 
what comes after it.
This is called a causal model.
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual aid from a lecture on NLP, specifically focusing on the Transformer Decoder and Large Language Models. It outlines the process of how embeddings are used to inform attention weights in a multi-head attention mechanism within a Transformer Decoder architecture. The flowchart details the sequence of operations starting from input embeddings, through positional encoding, to the final outputs. This process is crucial for models like GPT-3 or BERT that rely on attention mechanisms to understand and generate text. The lecture aims to provide insight into how these models work internally, emphasizing the importance of understanding what information tokens should take into account for each token in the sequence. [IDE]



Decoder - Outputs
‚óèAfter feeding the embeddings through N 
transformer layers, we get our output 
embeddings: [o<s>, oHello]. This is a matrix 
of sequence length times embedding size
‚óèNow we feed this into a linear layer to 
map this to a vector of vocabulary size.
‚óèFinally we perform a softmax.
‚óèWe now have a probability distribitution 
over all tokens in the vocabulary
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In this NLP lecture, we are discussing the Transformer Decoder and its role in large language models. The diagram illustrates the process of output probabilities and how they are derived from embedding sequences. The Transformer Decoder is a key component in these models, enabling the generation of output tokens by processing input sequences through a series of attention mechanisms and feed forward layers. The lecture emphasizes the importance of positional encoding to maintain the order of the sequence and references the work of Vaswani et al. (2017) for further understanding. [IDE]



Decoder ‚Äì Next Token
‚óèWe now have a probability vector that 
tells us how likely the next word is token 
0, 1, 2, 3, ‚Ä¶
‚óèWe can either take the argmax or sample 
from this distribution (roulette wheel 
sampling)
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)
Next Token 
ID
Next Token
Probability
75
World
0.75
5
there
0.12
109
,
0.08
20343
everyone
0.03
3
folks
0.02

[IDS] The image is a slide from an NLP lecture, specifically discussing the Transformer Decoder and its role in large language models. It outlines the process of decoding next tokens using a probability vector, which is determined by the softmax function applied to linear transformations of the encoder's outputs. The decoder can either take argmax or sample from the distribution provided by the roulette wheel sampling method. This is illustrated with a flowchart showing the components involved, such as masked multi-head attention, positional encoding, and output embedding. Additionally, there's a mention of Figure 4, which explains how the Transformer Decoder works (adapted from Vaswani et al., 2017). The slide also references the Hochschule Bonn-Rhein-Sieg and Tim Metzler, possibly indicating the institution and the lecturer or author of the presentation. [IDE]



Decoder ‚Äì Next Token
‚óèWe now have a probability vector that 
tells us how likely the next word is token 
0, 1, 2, 3, ‚Ä¶
‚óèWe can either take the argmax or sample 
from this distribution (roulette wheel 
sampling)
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)
Next Token 
ID
Next Token
Probability
75
World
0.75
5
there
0.12
109
,
0.08
20343
everyone
0.03
3
folks
0.02

[IDS] In the context of a NLP lecture on Transformer Decoder and Large Language Models, the image represents a crucial aspect of how these models process and generate text. It illustrates the mechanism behind predicting the next word in a sequence, which is a fundamental component of language generation tasks. The diagram provides a visual explanation of the softmax function and the probability vectors used to determine the most likely next word based on the context provided by the model's previous outputs. This understanding is essential for students studying NLP as it helps them grasp how these advanced models can be trained to produce coherent and contextually relevant text. [IDE]



Decoder ‚Äì Next Iteration
‚óèAssume we picked the token ‚Äúthere‚Äù with 
the id 5 as the next token.
‚óèWe append this to the previous input 
(shifted right)
‚óèWe repeat this process until we hit a 
maximum sequence length or produce the 
end of sentence token </s>
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)
there
<s> Hello there

[IDS] In this NLP lecture, we are discussing the concept of a Transformer Decoder as part of large language models. The slide explains the process of picking a token from the previous input (shifted right) and then repeating the process until a maximum sequence length is reached or the end of the sentence is encountered. This is illustrated with a flowchart that includes components like attention, positional encoding, and output embedding. Additionally, there's a mention of a paper by Vaswani et al. from 2017, which is likely the foundational research for this topic. [IDE]



Decoder ‚Äì Training
‚óèTake a training sequence:
‚Äù<s> I like to eat pizza </s>‚Äù
‚óèPredict the next token from ‚Äú<s>‚Äù
BCE Loss between the output 
probabilities and the correct token ‚ÄúI‚Äù.
‚óèFeed the sequence ‚Äú<s> I‚Äù into the model.
‚óèPredict the next token
BCE Loss
‚óèFeed the correct next token to the input:
‚Äù<s> I like‚Äù
‚óèRepeat for all training sequences
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual representation of the Transformer Decoder and its role in large language models, specifically focusing on how it's used during training. It illustrates the process of taking a training sequence, predicting the next token from the sequence, and feeding that prediction back into the model for further training. The diagram shows the structure of the Transformer Decoder with attention mechanisms, indicating how it processes and learns from input sequences to generate coherent outputs. This is a fundamental concept in natural language processing (NLP), where such models are trained to understand and generate human-like text. [IDE]



Generative Pre-trained Transformer
‚óèGPT-1 (06/2018): 117M parameters
Trained on BookCorpus (7000 unpublished 
books)
‚óèGPT-2 (02/2019, 11/2019): 1.5B parameters
Trained on Web, Reddit, BookCorpus
‚óèGPT-3 (05/2020): 175B parameters
Trained on filtered Common Crawl, Web, 
Books, Wikipedia
Fig. 4: Transformer Decoder (adapted 
from: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image represents a diagram of the architecture of a Generative Pre-trained Transformer (GPT) model, which is a type of large language model commonly used in natural language processing (NLP). In the context of an NLP lecture, this diagram would be used to explain how the GPT model processes input text and generates output. It illustrates the flow of information through various layers of the model, including the embedding layer, the transformer decoder, and the attention mechanism. This is crucial for understanding how these models are trained on large datasets like BookCorpus and filtered Common Crawl data, and how they can be fine-tuned for specific tasks such as language translation or summarization. The reference to "Transformer Decoder" adapted from Vaswani et al. 2017 further emphasizes the foundational research in the field that has led to the development of these advanced language models. [IDE]



Generative Pre-trained Transformer
What can these models do?
‚óèPredict the next token
‚óèTranslate
‚óèSummarize
‚óèCode generation
‚óè...

[IDS] The image is a slide from an NLP lecture discussing the capabilities of Generative Pre-trained Transformer models. It poses the question of what these models can do, listing tasks such as predicting the next token, translating, summarizing, and code generation. The slide is likely part of a presentation given by Tim Metzler at Hochschule Bonn-Rhein-Sieg, focusing on the decoder aspect of Transformer models and their applications in large language models. [IDE]



Generative Pre-trained Transformer
What can these models do?
‚óèPredict the next token
‚óèTranslate
‚óèSummarize
‚óèCode generation
‚óè...
Why can these models do that?

[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP). It discusses Generative Pre-trained Transformer models, which are a type of deep learning model used in NLP tasks. The slide is asking questions about the capabilities of these models, such as predicting the next token, translating, summarizing, and code generation. It also prompts the audience to consider why these models can perform these tasks. The slide includes logos of the Hochschule Bonn-Rhein-Sieg and Tim Metzler, suggesting that this is an educational context, possibly a course or seminar at the mentioned institution. [IDE]



‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù
Radford et al (OpenAI), 2019
From the abstract:
‚ÄùNatural language processing tasks, such as 
question answering, machine translation, reading 
comprehension, and summarization, are typically
approached with supervised learning on task-
specific datasets. We demonstrate that language
models begin to learn these tasks without any ex-
plicit supervision when trained on a new dataset
of millions of webpages called WebText. ‚Äú

[IDS] The image seems to be a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the Transformer Decoder and Large Language Models. It references an abstract from a paper by Radford et al published in OpenAI in 2019, which discusses the use of unsupervised multitask learning for tasks such as question answering, machine translation, reading comprehension, and summarization. The slide also includes logos of Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler's name, indicating the possible affiliation or authorship related to the content of the lecture. [IDE]



‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù
Radford et al (OpenAI), 2019
Closer look into the training data:

[IDS] The image depicts a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of Transformer decoder and Large Language Models. The slide, titled "Language Models are Unsupervised Multitask Learners," is authored by Radford et al (OpenAI), dated 2019. It provides insights into how language models can learn to perform various tasks without explicit supervision. The slide contains text in French and English, highlighting the versatility of these models. Additionally, there's a reference to a study by Hochschule Bonn-Rhein-Sieg, suggesting a practical application or research related to the topic. [IDE]



‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù
Radford et al (OpenAI), 2019
Closer look into the training data:
Using this knowledge to translate sentences 
with GPT2 
(https://transformer.huggingface.co/doc/gpt2-
large)

[IDS] The image is a snapshot from an NLP lecture focused on "Transformer_decoder_and_Large_Langauge_Models". It showcases the capabilities of language models like GPT2 in understanding and generating human-like responses. The lecture includes a demonstration of how a German friend's statement is translated to English, emphasizing the model's ability to handle multilingual conversations and context-aware responses. The slide also provides examples from a textbook to illustrate the natural demonstrations of English to French and French to English translation, highlighting the WebText training set used for this purpose. The content suggests that the lecture aims to educate about the workings and applications of transformer-based large language models in natural language processing tasks. [IDE]



‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù
Radford et al (OpenAI), 2019
Exercise:
Go to this website and try to get the model to 
perform one of the following tasks:
- Translation
- Sentiment Analysis
- Summarization
- Question Answering 
(https://transformer.huggingface.co/doc/gpt2-
large)

[IDS] In the image, we see a slide from an NLP lecture discussing "Language Models are Unsupervised Multitask Learners" by Radford et al (OpenAI), 2019. The slide is numbered 2 and contains a set of tasks for students to practice their skills in website navigation and model performance evaluation. These tasks include Translation, Sentiment Analysis, Summarization, Question Answering, and more. There's also a QR code linking to a repository on GitHub for further exploration. The footer of the slide mentions Hochschule Bonn-Rhein-Sieg and the names Tim Metzler and Fachbereich Informatik. [IDE]



Instruct GPT
GPT has the ability to perform a variety of 
tasks but it is hard to engineer the correct 
prompt.
‚Üí We need to train the model to respond 
correctly to our prompts:
‚óèCurrent Prompt:
My German friend says ‚ÄúPizza ist mein 
Lieblingsessen‚Äù or in short ‚Äú
‚óèDesired Prompt:
Translate ‚ÄúPizza ist mein Lieblingsessen‚Äù to 
German. 

[IDS] The image appears to be a slide from an NLP lecture focused on transformer decoders and large language models. The main content is about the capabilities of GPT (Generative Pre-trained Transformer) in performing tasks and its need for training to respond correctly to prompts. The slide mentions a German friend's statement, "Pizza ist mein Lieblingsessen," which translates to "Pizza is my favorite food." This example is used to demonstrate the model's ability to understand and respond in German. The lecture is likely discussing how transformer decoders, like GPT, can be fine-tuned to perform specific tasks or respond to certain prompts effectively. [IDE]



Instruct GPT

[IDS] The image is a screenshot of a lecture slide on Natural Language Processing (NLP), specifically focusing on the topic "Transformer_decoder_and_Large_Langauge_Models." It contains a prompt and a code snippet related to GPT-3, a large language model developed by OpenAI. The code demonstrates how to calculate the binomial coefficient using the GPT-3 model and its instruction set. The slide also includes the names of two contributors, Hochschule Bonn-Rhein-Sieg and Tim Metzler, suggesting their involvement in the content or presentation of the lecture. [IDE]



Instruct GPT
How to get from 
here to there?

[IDS] In this NLP lecture, we are exploring the intricacies of Transformer decoders and their role in Large Language Models. The slide provides a detailed explanation of the binomial coefficient function, which is used within the context of GPT models to determine the values stored in a list C. This understanding is crucial for grasping how these models operate and the mathematical principles that underpin them. As we delve deeper into the topic, we will learn about the components and mechanisms that enable these models to process and generate human-like language, a testament to the power of computational linguistics. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 
‚ÄúThese models are not aligned with their user.‚Äù
‚Üí The model might produce content that is 
not helpful for users, toxic or untruthful. So 
they need to be aligned to produce the desired 
output.

[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It discusses the importance of training language models to follow instructions with human feedback. The slide highlights that the models should not be aligned with their users and emphasizes the need for alignment between the model's content and the desired output. It also mentions that toxic or untruthful content is not helpful for users, suggesting that the model should be designed to produce content that is aligned with what is considered desirable by society. The slide includes logos of the Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler, who might be the presenter or author of the lecture material. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 

[IDS] In this NLP lecture, you're learning about the role of transformer decoders and large language models in natural language processing. Specifically, you're being introduced to the concept of fine-tuning these models with human feedback, as demonstrated by the OpenAI paper "Ouyang et al (OpenAI), 2021". This involves collecting demonstration data and training a supervised policy, which is then used to guide the model towards desired outputs. The lecture emphasizes the importance of understanding how these models work internally, as shown by the illustration of the three steps of the fine-tuning process using the SFT (Supervised Fine-tuning) method. This method, which includes a supervised fine-tuning step for one year, is highlighted as a crucial part of the process. The lecture also touches on the broader context of language models and their applications, such as generating text or answering questions, and how they can be improved through fine-tuning and human interaction. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 
OpenAI hired 40 contractors to 
label data. They were selected 
based on a screening test.

[IDS] The image is a visual representation of the process involved in training language models to follow human feedback. In the context of an NLP lecture on transformer decoder and large language models, this diagram illustrates the steps taken by OpenAI in their study using fine-tuning (SFT) and supervised fine-tuning (SFT) with 40 contractors to collect and label data. The labeled data was then used to train the AI model. This process is a common practice in NLP to improve the performance of language models by incorporating human feedback and expertise. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 
OpenAI hired 40 contractors to 
label data. They were selected 
based on a screening test.
Problems:
This is an expensive manual 
process. 
There are no good large 
prompt datasets.

[IDS] The image depicts a flowchart illustrating the process of training language models to follow instructions with human feedback, which is likely a topic in a NLP lecture. The flowchart outlines the steps taken by OpenAI in their 2021 study using OpenAI hired contractors to label data based on a screening test. This labeled data was then used for fine-tuning GPT-3 with supervised learning. The diagram emphasizes the complexity and the need for an expensive manual process to achieve good large prompt datasets. The lecture might discuss various aspects of transformer decoder and large language models, such as how they are trained, how they handle human feedback, and the challenges associated with creating high-quality datasets for these models. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 
Training a reward model to 
predict the ranking allows for 
large scale training.

[IDS] The image illustrates the process of training language models to follow instructions with human feedback, specifically using the OpenAI model from 2021. It's a part of a lecture on NLP (Natural Language Processing) focusing on Transformer Decoder and Large Language Models. The steps depicted are a simplified fine-tuning method (SFT), which is a technique used to improve the performance of pre-trained language models like GPT-3 by training them on specific tasks or datasets with human guidance. This approach allows for large-scale training that can enhance the model's ability to understand and generate human-like text. [IDE]



‚ÄúTraining language models to follow instructions with human feedback‚Äù
Ouyang et al (OpenAI), 2021 

[IDS] In the context of a NLP lecture on Transformer decoder and large language models, this image represents the process of training a model to generate human-like responses. The figure illustrates the three-step method used for fine-tuning supervised learning models, which involves collecting demonstration data, comparing and training a reward model, and optimizing the policy against the reward using reinforcement learning. This approach is essential in teaching AI systems to understand and mimic human communication patterns effectively. [IDE]



‚ÄúLearning to summarize from human feedback‚Äù
Stiennon et al (OpenAI), 2022 
Train several policies using the Reddit TL;DR 
(too long, didn‚Äôt read) dataset.
Task: Produce a summary from a text

[IDS] In this NLP lecture, we are learning about the various components and techniques used in natural language processing. Specifically, we are focusing on transformer decoders and large language models. These are advanced tools in NLP that help computers understand and generate human-like text. By understanding these concepts, we can improve communication between humans and machines, enabling more efficient and accurate information retrieval and generation. [IDE]



‚ÄúLearning to summarize from human feedback‚Äù
Stiennon et al (OpenAI), 2022 

[IDS] In this NLP lecture, we are discussing the process of summarizing information from human feedback using a large language model. This is illustrated in a diagram that outlines the steps taken to collect and analyze human feedback. The feedback is gathered from a Reddit post and used to inform the training of a model, which in turn is evaluated through summaries provided by human judges. The goal is to improve the model's ability to generate better summaries, as indicated by the feedback from both experts and non-experts. [IDE]



‚ÄúLearning to summarize from human feedback‚Äù
Stiennon et al (OpenAI), 2022 

[IDS] The image depicts a flowchart titled "Learning to summarize from human feedback" by Stiennon et al (OpenAI), 2022. It illustrates the process of collecting human feedback, training a reward model based on that feedback, and using it to improve the performance of a language model. The flowchart is divided into three main sections: Collect human feedback, Train reward model, and Use reward model. Each section contains detailed steps such as posting summaries to Reddit for evaluation, calculating rewards based on human judges' feedback, and updating the reward model. This process is part of a larger effort in NLP to develop more effective language models that can generate summaries or responses tailored to human preferences, as indicated by the lecture topic "Transformer_decoder_and_Large_Langauge_Models." [IDE]



‚ÄúLearning to summarize from human feedback‚Äù
Stiennon et al (OpenAI), 2022 

[IDS] The image is a visual representation of the process involved in summarizing human feedback using a reward model. This is relevant to NLP (Natural Language Processing) as it demonstrates how models can be trained and fine-tuned based on human input, which is a key aspect of developing large language models. In the context of this lecture, it would highlight the importance of incorporating human feedback into machine learning algorithms to improve their performance in tasks such as text summarization. [IDE]



‚ÄúLearning to summarize from human feedback‚Äù
Stiennon et al (OpenAI), 2022 

[IDS] In the context of an NLP lecture on Transformer Decoder and Large Language Models, the image represents a discussion about summarizing human feedback from an AI model. The example provided in the image illustrates how to extract relevant information from a text conversation between two individuals to understand their preferences and plans. This process involves identifying key phrases and summarizing the content to capture the essence of the dialogue. In an NLP setting, such techniques are crucial for interpreting user feedback and improving the performance of language models by incorporating real-world interactions and understanding user needs. [IDE]
[Lecture End]

[Lecture Start]

------------FineTuning_of_LLMs------------

Transformer Decoder
Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)
Prompt
Tokens
Input
Embeddings

[IDS] The image represents a Transformer Decoder, which is a component of transformer-based language models. In the context of NLP (Natural Language Processing), this decoder is used to generate outputs or predictions based on the inputs it receives. The architecture consists of multiple layers that process tokens, with each layer containing self-attention mechanisms and feed-forward networks. These layers are stacked together to form the decoder, which is responsible for producing the final output sequence. The diagram emphasizes the complexity and depth of the model, highlighting the importance of understanding how these components work together to generate coherent and contextually relevant responses. [IDE]



Transformer Decoder
Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)
Prompt
Tokens
Input
Embeddings
1. Adapt the prompt

[IDS] The image illustrates the concept of a Transformer Decoder, a crucial component in natural language processing (NLP). As we're in a lecture on FineTuning_of_LLMs, which refers to fine-tuning large language models, this diagram serves as an educational tool to explain how such models process and generate text. The Transformer Decoder is a key part of these models, responsible for generating the output sequence based on the input embeddings and tokens. The process involves adding and normalizing attention masks, feeding forward information, and repeating these steps multiple times to produce the final output. The lecture would likely cover the importance of fine-tuning these models to improve their performance on specific tasks or domains. [IDE]



Transformer Decoder
Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)
Prompt
Tokens
Input
Embeddings
1. Adapt the prompt
2. Adapt the input embeddings 
(prefix tuning)

[IDS] The image illustrates the concept of fine-tuning pre-trained language models, which is a crucial topic in Natural Language Processing (NLP). Fine-tuning involves taking a large pre-trained model, such as BERT or GPT, and adapting it for a specific task by training it on a smaller dataset relevant to that task. This process is depicted through the Transformer Decoder architecture, where the input embeddings are adapted using prefix tuning, and attention is prompted with a special token. The image serves as a visual aid to help NLP students understand how these models can be effectively fine-tuned for better performance on specific tasks. [IDE]



Transformer Decoder
Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)
Prompt
Tokens
Input
Embeddings
1. Adapt the prompt
2. Adapt the input embeddings 
(prefix tuning)
3. Adapt the model weights

[IDS] The image is a visual representation of the Transformer Decoder architecture, which is a key component in the field of Natural Language Processing (NLP). In the context of our lecture on FineTuning_of_LLMs, this diagram illustrates how the Transformer Decoder works by processing input embeddings through positional encoding and adapting both the model weights and the input embeddings using prefix tuning. This process is essential for fine-tuning Large Language Models (LLMs) to better understand and generate human-like text. The figure is adapted from the research paper "Attention is All You Need" by Vaswani et al., published in 2017, and it provides a simplified overview of the model's structure and its fine-tuning process. [IDE]



Transformer Decoder
Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)
Prompt
Tokens
Input
Embeddings
1. Adapt the prompt
2. Adapt the input embeddings 
(prefix tuning)
3. Adapt the model weights
4. Adapt the output layer

[IDS] The image is a visual representation of the Transformer Decoder, a component commonly used in NLP tasks such as language translation. In the context of a lecture on FineTuning_of_LLMs (Large Language Models), this diagram serves as an educational tool to illustrate how the model processes input embeddings and adapts them for generating outputs. The lecture might cover techniques like prompt adaptation and adjusting model weights to improve the performance of LLMs in specific tasks. The reference to "Hochschule Bonn-Rhein-Sieg Fachbereich Informatik Tim Metzler" suggests that this is part of a course curriculum at the mentioned institution, emphasizing the importance of understanding the inner workings of these models for fine-tuning purposes. [IDE]



Adapt the prompts
Types of prompting:
1) Zero Shot Prompting
2) Zero Shot Train of Thought Prompting
3) Few Shot Prompting
4) Few Shot Train of Though Prompting
Prompt
Tokens
Input
Embeddings
1. Adapt the prompt

[IDS] The image depicts a diagram illustrating the process of fine-tuning large language models (LLMs) in the context of Natural Language Processing (NLP). It emphasizes the importance of adapting prompts to suit the specific task at hand, such as generating text or answering questions. The diagram includes a flowchart with input embeddings, positional encoding, and attention mechanisms, highlighting how these components work together during the fine-tuning process. The lecture is likely focused on teaching students how to effectively fine-tune LLMs for different NLP tasks by selecting appropriate types of prompting. [IDE]



Zero Shot Prompting
Definition:
Model is provided with prompt that is not part of the 
training data.
The model performs the task without being explicitly 
trained on it.
The model is given no examples (zero shot).
Problem:
Does not work for more complex tasks.

[IDS] The image you're seeing is likely from a lecture on fine-tuning large language models (LLMs). The slide focuses on the concept of "Zero Shot Prompting," which is a technique in NLP where a model performs a task without explicit training on it. This is an important topic in the field, as it demonstrates the model's ability to generalize and apply knowledge from its pre-trained state to new tasks. The slide also includes a definition and examples of zero-shot prompts, along with some information about the institutions and people involved in the development of this technology. [IDE]



Zero Shot Prompting
Definition:
Model is provided with prompt that is not part of the 
training data.
The model performs the task without being explicitly 
trained on it.
The model is given no examples (zero shot).
Problem:
Does not work for more complex tasks.
Example Prompt:
Prompt:
Classify the following text into the labels [funny, not 
funny, slightly funny]. Only respond with the label:
Text: Tom went to the pub.
Response (ChatGPT 3.5):
not funny

[IDS] The image is a slide from a lecture on fine-tuning large language models (LLMs), specifically focusing on the concept of "Zero Shot Prompting." The slide explains that this technique involves providing a model with a prompt that isn't part of its training data, and it then performs a task without being explicitly trained on it. An example of this is given in the form of a text response task where the model is asked to respond to a label without having been previously trained on it. This aligns with the topic of fine-tuning LLMs, as it showcases the model's ability to adapt and generate relevant responses without direct training. [IDE]



Zero Shot Prompting
Complex Prompt:
Prompt:
A juggler can juggle 16 balls, half of which are golf 
balls, half of them are green. How many green balls 
does she have?
Response (Llama 2, 70B):
The juggler has 16 balls in total, and half of them are 
green. Therefore, the number of blue balls is 16/2 = 8.
Since half of the balls are golf balls, the number of golf 
balls is also 8.
So, the juggler has 8 green balls and 8 golf balls.

[IDS] The image displays a slide from a lecture on "FineTuning_of_LLMs", which stands for fine-tuning of large language models. The slide's content is centered around the Zero-Shot Prompting technique, a method used in natural language processing to enable a language model to perform tasks without explicit training on the specific task. The example provided illustrates how a juggling prompt can be used to elicit responses from a language model about various aspects of juggling, such as the number of balls, the number of green balls, and the total number of balls. This is indicative of how Zero-Shot Prompting allows for flexible and creative generation of text based on given prompts, without the need for extensive training on each possible input. [IDE]



Zero Shot Train of Thought Prompting
Tell the LLM to explain the process step by step1.
Prompt:
A juggler can juggle 16 balls, half of which are 
golf balls, half of them are green. How many 
green balls does she have?
Lets think step by step and answer how many 
golf balls there are. Then answer how many of 
the golf balls are green.
1 Wei, J., Wang, X., Schuurmans, D., et al. (2023). 
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 
Google Research, Brain team.

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents the concept of fine-tuning pre-trained language models like LLMs (Large Language Models). The slide titled "Zero Shot Train of Thought Prompting" suggests an approach where a juggling analogy is used to explain the process of fine-tuning. This process involves starting with a basic number of balls (green for half and uncolored for half), then adding green balls to reach a desired number (16 balls in this case). The prompt encourages thinking step by step and answering how many golf balls there are after the process. This metaphor likely relates to the incremental updates and adjustments made to LLMs during fine-tuning to improve their performance on specific tasks or domains. [IDE]



Zero Shot Train of Thought Prompting
Tell the LLM to explain the process step by step1.
Prompt:
A juggler can juggle 16 balls, half of which are 
golf balls, half of them are green. How many 
green balls does she have?
Lets think step by step and answer how many 
golf balls there are. Then answer how many of 
the golf balls are green.
Response (Llama 2, 70B):
Sure, let's break this down step by step!
1) First, we know that the juggler can juggle 16 balls.
2) We also know that half of the balls are golf balls. 
Therefore, the number of golf balls is 16/2 = 8.
3) Now, we know that half of the golf balls are green. 
Therefore, the number of green golf balls is 8/2 = 4.
4) Finally, we add the number of green golf balls to the 
number of green balls that are not golf balls to find 
the total number of green balls. Since we don't know 
how many green balls that are not golf balls, we'll 
just add 4 to the number of green balls that are not 
golf balls.
So, the answer to the question "How many green balls 
does she have?" is 4 + x, where x is the number of 
green balls that are not golf balls.
1 Wei, J., Wang, X., Schuurmans, D., et al. (2023). 
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 
Google Research, Brain team.

[IDS] In the context of a NLP lecture focused on FineTuning_of_LLMs, the image likely represents a slide or visual aid that explains the process of fine-tuning large language models. The slide may include diagrams, flowcharts, or other graphical representations to illustrate the steps involved in fine-tuning these models, such as data preparation, model selection, training, and evaluation. The overall goal would be to provide a clear and concise understanding of how to improve the performance of pre-trained language models for specific tasks or domains. [IDE]



Few Shot Prompting
Definition:
Model is provided with prompt that is not part of the 
training data.
The model performs the task without being explicitly 
trained on it.
The model is given a few examples (few shot).

[IDS] In the context of a NLP lecture, the image represents a slide titled "Few Shot Prompting" which is likely a part of the topic "FineTuning_of_LLMs". The slide explains that a model is provided with a prompt that is not part of the training data and the model performs the task without being explicitly trained on it. It also mentions that the model is given a few examples (few shot) to perform the task. The logos at the bottom suggest that this information might be from Hochschule Bonn-Rhein-Sieg and could be related to their Fachbereich Informatik (Department of Computer Science), presented by Tim Metzler. [IDE]



Few Shot Prompting
Definition:
Model is provided with prompt that is not part of the 
training data.
The model performs the task without being explicitly 
trained on it.
The model is given a few examples (few shot).
Advantage:
We can even give completely new tasks and learn 
by example.
Problem:
Does not work for more complex tasks.
Example Prompt:
Prompt:
Complete this with a single answer (red, green):
These are the examples:
This is great ‚Üí red
This is good ‚Üí red
This is awful ‚Üí green
Complete this:
This is horrible ‚Üí
This is nice ‚Üí 
Response (ChatGPT 3.5):
This is horrible ‚Üí green
This is nice ‚Üí red

[IDS] In the context of a NLP lecture focused on FineTuning_of_LLMs, this image represents a slide discussing the concept of "Few Shot Prompting." It emphasizes that a model provided with a prompt that is not part of the training data performs the task without being explicitly trained on it. The slide also explains the advantages and disadvantages of this approach, such as the ability to learn new tasks quickly but also the risk of incompatibility with more complex tasks. Examples are given for both successful and unsuccessful prompts, showcasing how few-shot prompting can be applied in practice. The slide is likely part of a larger discussion on the versatility and limitations of fine-tuning large language models (LLMs) using minimal training data. [IDE]



Few Shot Prompting
Complex example.
Prompt:
Q: A juggler can juggle 16 balls, half of which are 
golf balls, half of them are green. How many 
green balls does she have?
A: 4+x, where x is the number of green balls that 
are not golf balls.
Q: Susan buys bottles and cans of soda. She 
has 12 bottles and cans. Half of them are cans. 
Half of them contain lemonade. How many 
lemonades does she have?
A:
Response (ChatGPT 3.5):
Let's break down the information given:
1. Susan has 12 bottles and cans of soda.
2. Half of them are cans.
So, the number of cans is 12 / 2 = 6.
3. Half of the bottles and cans contain lemonade.
Now, half of the total number of bottles and cans is 12 / 
2 = 6. Since this is also the number of cans, we can 
conclude that all the cans contain lemonade.
Therefore, Susan has 6 lemonades.

[IDS] The image is a visual aid from a lecture on Fine-Tuning of Large Language Models (LLMs), likely discussing the process of adapting these models for specific tasks. The slide titled "Few Shot Prompting" suggests a focus on using few-shot learning, where models are trained with a small number of examples to perform well on a new task. The complex example provided involves counting green balls and cans, which is a simple task that could be used to illustrate how LLMs can be fine-tuned for various tasks through prompt engineering. The presence of the Hochschule Bonn-Rhein-Sieg logo indicates the institution hosting or affiliated with the lecture, and the name Tim Metzler might refer to the lecturer or presenter of the material. [IDE]



Few Shot Train of Thought Prompting
Complex example with train of thought.
Prompt:
Q: A juggler can juggle 16 balls, half of which are 
golf balls, half of them are green. How many 
green balls does she have?
A: Let‚Äôs think step by step: There are 8 golf balls. 
4 golf balls are green. She has 4 green balls plus 
how many other balls are green
Q: Susan buys bottles and cans of soda. She 
has 12 bottles and cans. Half of them are cans. 
Half of them contain lemonade. How many 
lemonades does she have?
A:
Response (ChatGPT 3.5):
Let's break it down step by step:
1. Half of the bottles and cans are cans.
   So, 12 / 2 = 6 of them are cans.
2. Half of the cans contain lemonade.
   So, 6 / 2 = 3 cans contain lemonade.
Therefore, Susan has 3 lemonades.

[IDS] The image you're seeing is a slide from a lecture on "FineTuning_of_LLMs", which likely stands for Fine-Tuning of Large Language Models. The slide seems to be focused on the concept of thought prompting, specifically using the ChatGPT 3.5 model as an example. It includes a complex example with a train of thought, showing how to use the model to generate a response to a given prompt. This is a common practice in NLP, where models are fine-tuned to perform specific tasks or generate coherent responses to prompts. The presence of the names Tim Metzler and Hochschule Bonn-Rhein-Sieg suggests that this might be part of a course or seminar at that institution, possibly taught by Tim Metzler. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Prompt
Tokens
Input
Embeddings
2. Adapt the input embeddings 
(prefix tuning)

[IDS] The image is a visual representation of the process of fine-tuning language models, specifically focusing on the use of prefix tuning. This technique involves optimizing continuous prompts for generating outputs in NLP tasks. The diagram shows how input embeddings are processed through positional encoding and then passed through layers such as linear, add & norm, feed forward, and masked multi-head attention. The purpose of this lecture, titled "FineTuning_of_LLMs," is likely to educate students about how to effectively adapt and fine-tune pre-trained language models like BONN-RHEIN-SIEG for specific tasks by adjusting the input embeddings. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Prompt
Tokens
Input
Embeddings
Recap:
In a transformer model we:
‚óè
tokenize the text into token ids,
‚óè
look up the initial embeddings for these tokens
‚óè
add position information
‚óè
feed this trough attention layers
Input text: 
‚ÄùSummarize: NLP is cool. ‚Ä¶‚Äù
Token ids:
[15, 8, 14, 3, 200, ‚Ä¶]
Input Embeddings:
[y15, y8, y14, y3, y200, ...]

[IDS] The image you're seeing is a visual representation of the concept of "Prefix-Tuning: Optimizing Continuous Prompts for Generation" in the context of NLP, or Natural Language Processing. In a NLP lecture focused on FineTuning_of_LLMs (Large Language Models), this diagram would likely be used to explain how adding and normalizing features, such as tokens, attention, and position embeddings, can improve the performance of language models. The diagram illustrates the process of input embedding and encoding, which are crucial steps in fine-tuning these models. It's a tool to help students and professionals understand the complex interactions within NLP models and how they can be optimized for better generation of text. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Prompt
Tokens
Input
Embeddings
Recap:
In a transformer model we:
‚óè
tokenize the text into token ids,
‚óè
look up the initial embeddings for these tokens
‚óè
add position information
‚óè
feed this trough attention layers
Input text: 
‚ÄùSummarize: NLP is cool. ‚Ä¶‚Äù
Token ids:
[15, 8, 14, 3, 200, ‚Ä¶]
Input Embeddings:
[y15, y8, y14, y3, y200, ...]
Prompt engineering optimizes this text

[IDS] In the context of a NLP lecture, specifically focusing on FineTuning_of_LLMs, we are likely discussing the process of adapting pre-trained language models to perform specific tasks. The image represents a transformer model architecture, which is commonly used in NLP tasks such as language generation or translation. The model consists of an input layer that processes embeddings from tokens, followed by multiple layers of transformer blocks for processing these inputs. The output layer then generates probabilities for different outputs.

The lecture might cover topics like how to fine-tune pre-trained models using labeled data for a specific task, adjusting the model's parameters to improve its performance on the target task, and evaluating the effectiveness of the fine-tuned model. The goal is to leverage the knowledge gained from pre-training on a large dataset to achieve better results on a specific application domain. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Prompt
Tokens
Input
Embeddings
Recap:
In a transformer model we:
‚óè
tokenize the text into token ids,
‚óè
look up the initial embeddings for these tokens
‚óè
add position information
‚óè
feed this trough attention layers
Input text: 
‚ÄùSummarize: NLP is cool. ‚Ä¶‚Äù
Token ids:
[15, 8, 14, 3, 200, ‚Ä¶]
Input Embeddings:
[y15, y8, y14, y3, y200, ...]
Prompt engineering optimizes this text
Can we optimize this instead?

[IDS] In this NLP lecture, we are discussing the concept of fine-tuning large language models (LLMs). The image presents a diagram illustrating the process of fine-tuning an LLM. It shows how input embeddings are processed through positional encoding and then fed into a transformer model, which consists of layers for adding and normalizing attention masks, feeding forward, and summing up the attention values. The output probabilities from the transformer model are then used to generate a response. The text in the image emphasizes the importance of optimizing continuous prompts for generating text. Additionally, there's a prompt asking if we can optimize the process further by engineering the text. This lecture seems to be focused on teaching the techniques and considerations involved in fine-tuning LLMs for natural language processing tasks. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Input Embeddings:
[y15, y8, y14, y3, y200, ‚Ä¶]
Add a prefix to the embeddings:
[p0, y15, y8, y14, y3, y200, ‚Ä¶]

[IDS] The image depicts a slide from a lecture focused on "FineTuning_of_LLMs," which likely stands for fine-tuning of large language models. These are advanced tools in Natural Language Processing (NLP) that have been pre-trained on vast amounts of data and can be further refined to perform specific tasks such as sentiment analysis, translation, or question answering. The presence of logos suggests collaboration between the Hochschule Bonn-Rhein-Sieg and Tim Metzler, possibly indicating that this lecture is part of an academic course or research project at the mentioned institution. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Input Embeddings:
[y15, y8, y14, y3, y200, ‚Ä¶]
Add a prefix to the embeddings:
[p0, y15, y8, y14, y3, y200, ‚Ä¶]
The prefix p0 is now a vector of embedding size (e.g. 
768).
Fine-tune this vector on a supervised dataset.
In general: Have one or more task dependent prefix 
vectors

[IDS] The image is a snapshot of a presentation slide from an NLP (Natural Language Processing) lecture, specifically focusing on the concept of fine-tuning large language models (LLMs). The title "FineTuning_of_LLMs" suggests that the content is about optimizing these models for specific tasks or domains. The slide includes a reference to a paper titled "Prefix-Tuning: Optimizing Continuous Prompts for Generation" by Xiang Lisa Li and Percy Liang from Stanford University in 2021. This indicates that the lecture might be discussing recent research in the field, particularly how to improve the performance of LLMs through prefix tuning, which is a technique for adjusting the model's embeddings to enhance its task-specific capabilities. [IDE]



‚ÄúPrefix-Tuning: Optimizing Continuous Prompts for Generation‚Äù
Xiang Lisa Li and Percy Liang, Stanford 2021
Input Embeddings:
[y15, y8, y14, y3, y200, ‚Ä¶]
Add a prefix to the embeddings:
[p0, y15, y8, y14, y3, y200, ‚Ä¶]
The prefix p0 is now a vector of embedding size (e.g. 
768).
Fine-tune this vector on a supervised dataset.
In general: Have one or more task dependent prefix 
vectors.
Advantage:
Instead of manually optimizing text prompts we can 
automatically optimize the parameters of the prefix vector.
Instead of fine-tuning the whole model we only fine-tune the 
input.
Works with relatively small amount of examples (50-500 
examples).
Disadvantage:
We only do instruction tuning. We can not learn completely 
new tasks.

[IDS] The image represents a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Fine-Tuning of Large Language Models" or LLMs. The slide discusses the concept of "Prefix-Tuning," which is a method to optimize continuous prompts for generating text. It explains how to add a prefix to embeddings and mentions that the prefix is now a vector of embedding size, for example, 768. The slide also introduces the idea of fine-tuning the whole model instead of just fine-tuning the input. It compares this method with instruction tuning and emphasizes the advantages such as manually optimizing text prompts and working with relatively small amounts of examples, like 50-500 examples. The disadvantages mentioned include the need for more data and computational resources. The slide also includes logos indicating the affiliation of the lecture with Hochschule Bonn-Rhein-Sieg and Tim Metzler's involvement in the lecture. [IDE]



Fine-Tuning the Output Layer
Prompt
Tokens
Input
Embeddings
4. Adapt the output layer

[IDS] The image is a visual representation of the process for fine-tuning the output layer in language models. In the context of NLP (Natural Language Processing), this diagram illustrates how an existing language model can be adapted to perform a specific task by modifying its output layer. The steps shown are part of the training process where additional layers are added to enhance the model's ability to generate more accurate outputs. This is a common technique used in machine learning to improve the performance of models on specific tasks. [IDE]



Feature-Based Approaches and Fine-Tuning Approaches
Prompt
Tokens
Input
Embeddings
4. Adapt the last layers or train 
classifier on top of output
Fig. 2: Fine-Tuning Approaches (adapted from 
https://magazine.sebastianraschka.com/p/finetuning-large-language-models, accessed 
18.01.2024)

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image illustrates the process of fine-tuning large language models. These models are pre-trained transformer networks that have been trained on vast amounts of data to understand the nuances of human language. The fine-tuning process involves adapting these models for specific tasks or domains. In the diagram, we see the architecture of a model with multiple layers, where the input embeddings are processed through several transformer blocks, resulting in an output. The arrows indicate the flow of information through the network. This is a crucial concept in NLP as it allows for the development of domain-specific language models that can perform better on tasks like sentiment analysis or text classification within their respective fields. [IDE]



Feature-Based Approaches and Fine-Tuning Approaches
Feature-Based Approach:
Take output embeddings of model
Train classifier on top (e.g. sentiment classification)
Advantages:
Only inference is done.
Few parameters in classifier to train. (fast and efficient)
Embeddings can be stored beforehand.
Disadvantages:
Low accuracy
Fig. 2: Fine-Tuning Approaches (adapted from 
https://magazine.sebastianraschka.com/p/finetuning-large-language-models, 
accessed 18.01.2024)

[IDS] In the image, we see a slide from a lecture on "Fine-Tuning of LLMs," which stands for Large Language Models. The slide is titled "Feature-Based Approaches and Fine-Tuning Approaches" and focuses specifically on approach number 1: Feature-Based Approach. This approach involves taking output embeddings of a model, such as a transformer, and training a classifier on top of it to perform tasks like sentiment classification. The advantages listed include the fact that inference is done only once, parameters are kept frozen, and embeddings can be stored before handling. However, the disadvantages mentioned are low accuracy and the need for a classifier to train, which can be fast and efficient but also time-consuming. The slide is part of a series, as indicated by the references to other slides and individuals involved in the presentation. [IDE]



Feature-Based Approaches and Fine-Tuning Approaches
Feature-Based Approach:
Take output embeddings of model
Train classifier on top (e.g. sentiment classification)
Advantages:
Only inference is done.
Few parameters in classifier to train. (fast and efficient)
Embeddings can be stored beforehand.
Disadvantages:
Low accuracy
Fine-Tuning I:
Train the output layers of the model on your data.
Advantages:
Slightly higher accuracy.
Disadvantages:
Slightly longer training time.
For inference we need to run the full model.
Fig. 2: Fine-Tuning Approaches (adapted from 
https://magazine.sebastianraschka.com/p/finetuning-large-language-models, 
accessed 18.01.2024)

[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP), specifically focused on the topic of "Fine-Tuning of Large-Language Models" or LLMs. It compares two approaches to fine-tuning LLMs: a feature-based approach and a fine-tuning approach. The feature-based approach uses a pre-trained transformer model that's kept frozen, only updating the classifier. This method has advantages such as being fast and efficient with embeddings stored before-hand, but also has disadvantages like low accuracy and the need for a large training set. On the other hand, the fine-tuning approach trains the output layers of the model on your data, which offers slightly higher accuracy but requires longer training time and a full model for inference. [IDE]



Fine-Tuning the Full Model
Fig. 2: Fine-Tuning Approaches (adapted from 
https://magazine.sebastianraschka.com/p/finetuning-large-language-models, 
accessed 18.01.2024)

[IDS] The image is a visual representation of the process involved in fine-tuning large language models (LLMs) as discussed in an NLP lecture. It illustrates the steps taken to adapt pre-trained transformer models to a specific domain or task, which is a common practice in natural language processing to improve the performance of language models on particular tasks. The diagram provides a structured overview of the feature-based approach and the fine-tuning process, including the use of labeled training sets and updates to the model's parameters. This process is crucial for NLP applications where domain-specific knowledge can significantly enhance the accuracy and relevance of language models. [IDE]



Fine-Tuning the Full Model
Fine-Tuning the Full Model:
Train the full model on new data.
Advantages:
Potentially higher accuracy
Disadvantages:
Overfitting
Needs a lot of examples
Catastrophic forgetting
Fig. 2: Fine-Tuning Approaches (adapted from 
https://magazine.sebastianraschka.com/p/finetuning-large-language-models, 
accessed 18.01.2024)

[IDS] The image is a visual representation of the process and benefits of fine-tuning large language models (LLMs) in the context of Natural Language Processing (NLP). It's part of a lecture that explains how to adapt pre-trained transformer models, such as those from Hugging Face or GPT, for specific NLP tasks by fine-tuning them on new data. The figure illustrates this process with two main approaches: Feature-based approach and Fine-tuning I & II methods. The advantages of fine-tuning include potentially higher accuracy, while the disadvantages involve overfitting and the need for a lot of examples. The lecture also emphasizes the importance of catastrophic forgetting and the use of techniques like update or keep frozen to mitigate it. This information is crucial for NLP practitioners who want to leverage the power of pre-trained models for their specific applications without losing the generalization capabilities they have developed during training on diverse datasets. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Problem: 
We only want to adapt the parameters of the model a little. 
Typically the model already has knowledge which is 
beneficial for the task.
How to determine which parameters to update?

[IDS] In the context of an NLP lecture, the image represents a discussion on the process of FineTuning of Large Language Models (LLMs). The slide titled "LoRa: Low-Rank Adaptation of Large Language Models" by Hu et al., published in Microsoft 2021, focuses on parameter-efficient fine-tuning. This technique is crucial for NLP as it allows models to adapt to specific tasks while keeping the number of parameters manageable. The slide suggests that the problem lies in adapting the model's parameters with limited knowledge of which parameters are beneficial for the task. It also introduces a method to determine which parameters to update, emphasizing the importance of efficient adaptation in NLP. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Problem: 
We only want to adapt the parameters of the model a little. 
Typically the model already has knowledge which is 
beneficial for the task.
How to determine which parameters to update?
Solution:
Decompose the weight matrices W into an original part W 
and an update part ŒîW.

[IDS] The image is a slide from a lecture on "FineTuning_of_LLMs," which stands for Fine-Tuning of Large Language Models. It discusses the challenge of adapting these models, which typically have a lot of knowledge beneficial for tasks, to new parameters without compromising their existing capabilities. The slide outlines the problem and solution approaches, such as decomposing weight matrices into original parts and updating part W. This is likely part of a larger discussion on how to effectively fine-tune large language models for specific tasks while maintaining their overall performance and understanding. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Problem: 
We only want to adapt the parameters of the model a little. 
Typically the model already has knowledge which is 
beneficial for the task.
How to determine which parameters to update?
Solution:
Decompose the weight matrices W into an original part W 
and an update part ŒîW.
Fig. 3: Frozen original weights and trainable update (from 
https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-
finetuning-large-models-936bce1a07c6, accessed 18.01.2024)

[IDS] In the image, we see a slide from a lecture on "FineTuning of LLMs," which stands for Large Language Models. The slide is titled "LoRa: Low-Rank Adaptation of Large Language Models" and is authored by Hu et al., published in Microsoft 2021. It presents a parameter-efficient fine-tuning method for adapting the parameters of a model, especially those that already have knowledge beneficial for the task at hand. The slide outlines a solution to determine which parameters to update using a pre-trained model W and an original part W. It also mentions an update part ŒîW and includes a diagram showing the flow of operations in the fine-tuning process. Additionally, there are references to frozen original weights and a trainable update from a specific URL. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Low-Rank Assumption:
The update weight matrix ŒîW does not contain a lot of 
new information.
This means the rank is lower than the original dimension.
‚Üí We can represent the weight update using two smaller 
matrices:
ŒîW = AB
A and B have a lower dimensionality than ŒîW
Fig. 3: Frozen original weights and trainable update (from 
https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-
finetuning-large-models-936bce1a07c6, accessed 18.01.2024)

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept known as "Low-Rank Adaptation of Large Language Models" or LoRa for short. It's a technique that allows for efficient fine-tuning of large pre-trained language models like BERT or GPT. The slide explains how by assuming low-rank and utilizing the fact that the update weight matrix ŒîW is not very large, we can significantly reduce the computational resources needed during the fine-tuning process. This is done by representing ŒîW using two smaller matrices: A and B, which are lower in dimensionality than ŒîW itself. This method, as shown in the flowchart, can be applied to pre-trained weights to achieve faster and more efficient fine-tuning, which is essential in NLP tasks where fine-tuning large models on specific tasks is often necessary but computationally expensive. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Low-Rank Assumption:
The update weight matrix ŒîW does not contain a lot of 
new information.
This means the rank is lower than the original dimension.
‚Üí We can represent the weight update using two smaller 
matrices:
ŒîW = AB
A and B have a lower dimensionality than ŒîW
Fig. 4: Frozen original weights and trainable update A and B (from 
https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-
finetuning-large-models-936bce1a07c6, accessed 18.01.2024)

[IDS] The image you see is a visual representation of the concept "FineTuning_of_LLMs," which stands for fine-tuning of large language models. It illustrates the process of adapting these models to specific tasks or domains. The diagram shows how a low-rank assumption can be used to represent the weight matrix with two smaller matrices, reducing the dimensionality and potentially improving the efficiency of the model. This is a common technique in machine learning, particularly in natural language processing (NLP), where large language models are fine-tuned for various applications such as sentiment analysis, text classification, and language translation. [IDE]



‚ÄúLoRa: Low-Rank Adaptation of Large Language Models‚Äù
Hu et al, Microsoft 2021
Parameter-efficient Fine-Tuning
Advantages
‚óè
Instead of training large weight matrices ŒîW we only 
train smaller matrices A and B
‚óè
We can publish the new weights ŒîW
Anyone who wants to use our model downloads the 
original weights and our update and applies it.
(Llama ‚Üí Alpaca ‚Üí Vicuna etc)
‚óè
We avoid licensing issues (we only publish the new 
part)
‚óè
We can have a large base model and switch different 
update matrices to get a model family (e.g. one for 
summarization, one for sentiment, etc)
Disadvantages:
‚óè
How to choose dimensionality r?
‚óè
Which weight matrices to fine-tune?
‚óè
Low rank may cause low performance
Fig. 4: Frozen original weights and trainable update A and B (from 
https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-
finetuning-large-models-936bce1a07c6, accessed 18.01.2024)

[IDS] The image is a visual aid from a lecture on the topic of "FineTuning_of_LLMs", which stands for Fine-Tuning of Large Language Models. It illustrates the concept of using pre-trained weights and applying them to a specific task, like updating a model family for different tasks such as sentiment analysis or fine-tuning. The diagram shows how the original large language model (LLM) can be modified by adding new layers on top and adjusting the existing layers with new weights to suit a particular task. This process is beneficial as it allows for more efficient training by leveraging the knowledge already present in the pre-trained model, rather than starting from scratch. The slide also mentions challenges associated with this approach, such as choosing the right weight matrices and dealing with low rank issues that could cause poor performance. The slide is likely part of a larger presentation aimed at educating attendees about the techniques and considerations involved in fine-tuning large language models. [IDE]
[Lecture End]

[Lecture Start]

------------Retrival_augemented_generation------------
Hi, I'm Daniel
I studied here at H-BRS (Embedded - and Autonomous Systems.)
Did a PhD at the Univerity of Bonn in the field of ML.
Worked at Fraunhofer IAIS.
For the last few years I worked at Telekom Techn1k in a department helps the
organization with digitalization and automatization.
Digitalization has a lot of room for ML/AI.
1


Before we dive into RAG systems,
bare with me to understand why it is needed.
2


ChatGPT hit the industry like a truck!
OpenAI has demonstrated with the introduction of ChatGPT
how well language models work and
they created an extremely usable interface.
This is why everybody in the industry loves language models 
3


Think about this
A large part of knowledge, processes, documentation, communication, FAQs,
regulations, etc. are in written form.
In addition to programming assistance, there exist thousands of use cases:
Ask your documentation directly
Consult customers without having to call an employee.
Use a chatbot for website navigation; very modern!
Simplify onboarding
4


Language models are sequence models
5


Modelling sequences, classic edition: Element by element
Language models are seq2seq  models.
6

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Retrival_augemented_generation". It illustrates the concept of 'Modelling sequences, classic edition: Element by element', which is likely a method or technique used in NLP for processing and analyzing sequential data. The slide also mentions that language models are models, suggesting a discussion on the nature of language models within this context. The presence of the WXYZ sequence, along with an encoder-decoder architecture diagram, indicates a possible exploration of sequence-to-sequence models in the lecture. [IDE]


Modelling sequences, element by element
Language models are seq2seq  models.
So far this was done element by element, using
Regression on time windowed input, Markov chains,
recurrence e.g. RNNs, or
memory e.g. GRUs. (LSTM 1997)
But this approach cannot easily be parallelized!
6

[IDS] The image is a visual representation of sequence-to-sequence models in NLP, which are used for tasks such as machine translation, text summarization, and chatbots. These models consist of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence. The lecture highlights that these models can be difficult to parallelize due to their sequential nature, making them less efficient on parallel hardware like GPUs. [IDE]


Transformers solved this problem...
... by consuming the entire sequence at once as input.
Think about it, there are many sequences that are finished.
We humans just consume it as sequence.
Books, emails, videos, images, webpages, ...
7

[IDS] In this NLP lecture, the focus is on transformer models and their ability to process data in sequence. The slide emphasizes that transformers consume an entire sequence at once as input, which is a characteristic feature of these models. It also highlights that humans tend to think about sequences that are finished, such as books or emails, rather than ongoing processes like videos or webpages. The content suggests a discussion around the nature of sequence processing in natural language understanding and generation tasks. [IDE]


Embedding the tokens
The tokens are each embedded into a numerical domain.
Have a look at the word2vec  paper:
Efficient Estimation of Word Representations in Vector Space
So the input to the transformer are not vectors, but vectors of vectors.
8

[IDS] In this NLP lecture, we are discussing the concept of embedding tokens into a numerical domain for efficient estimation of word representations in vector space. This process involves converting words into vectors to facilitate better understanding and analysis by machines. The presentation highlights that all transformers are cool, suggesting the importance of transformer models in NLP tasks. It also mentions that the input to the transformer is not vectors but vectors of vectors, indicating the complexity involved in these representations. [IDE]


Calculate the matrix of
pairwise attentions
In a sequence, the elements are
related to each other. (not iid)
For each possible pair of
embedded tokens the
corresponding attention-score is
calculated.
This attention matrix is the heart
of the transformer block .
9

[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP), specifically discussing the concept of "Retrival_augemented_generation". It explains how to calculate the matrix of pairwise attentions in a sequence. This is a technique used in NLP models, like transformers, to determine the relevance or importance of each element in a sequence relative to other elements. The attention score is calculated for each possible pair of embedded tokens, and this attention matrix is considered the heart of the transformer block in these models. The slide includes visual aids like color-coded squares to help understand the concept of context length and position in relation to the attention mechanism. [IDE]


The transformer block
Consists of two main components:
A filter for what should be
computed.
A classic neural network layer.
This calculation of all attention-
scores can be heavily
parallelized, and that's why
Transformers are so popular!
10

[IDS] The image is a visual representation of the Transformer block, a key component in transformer-based neural networks, particularly in the context of Natural Language Processing (NLP). The Transformer block is a fundamental part of the Transformer architecture, which has been widely used in various NLP tasks such as machine translation, question answering, and text generation. It consists of two main components: a multi-head attention mechanism and a feed-forward network. These components work together to process input sequences in parallel, allowing the model to capture long-range dependencies between words or tokens. The lecture you are attending is likely discussing how this architecture contributes to advancements in NLP, specifically focusing on retrieval-augmented generation, which combines the strengths of both retrieval-based and generative models for improved performance. [IDE]


Faster training means larger models in the same time
Much larger transformer networks can be trained in the same time as other
seq2seq models.
... and for language models, larger is (one parameter for) better.
See: Scaling Laws for Neural Language Models
11

[IDS] The image depicts a slide from an NLP lecture, specifically discussing the concept of "Faster training means larger models in the same time." It explains that bigger transformer networks can be trained simultaneously with seq2seq models and that for language models, larger is better. The slide includes graphs to illustrate this point, with one graph showing test loss over compute and another graph displaying dataset size vs. parameters. The lecture seems to emphasize the importance of model size in achieving better performance in neural language models. [IDE]


If transformers are so good
What's the problem?
12


Sequence length is the problem
Computing all attention-scores is quadratic in storage and runtime!
(You can battle that by parallelization, but this scales only linear.)
... and then there is also
Multi-Headed Attention
Multiple Transformer blocks in a row.
13


Have a look at the "Chat with your own data" use case
Present your data to the LLM and just chat with it.
Highly useful - very flexible - easy to implement!
However, prompts from this world can quickly become huge.
System prompt: You are a helpful assistant that helps Telekom with fiber optic
expansion and...
User prompt: Under what circumstances am I allowed to drill into a listed
building for a fiber optic connection?
Context documents: Attached are all building regulations for Telekom
employees...
... that's a lot of tokens...and it's going to make our attention matrix explode.
14


Academia is looking for ways to extend the context length
15


Some ideas are
State spaces instead of attention:
Mamba: Linear-Time Sequence Modeling with Selective State Spaces
Hierarchical attention:
Hierarchical Attention Networks for Document Classification
I/O aware attention to reduce the number of memory reads/writes:
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Parallelizable LSTMs:
Extended Long Short-Term Memory
16


Some more ideas are
Sparse attention
Generating Long Sequences with Sparse Transformers
Optimizing the attention calculation:
You Need to Pay Better Attention:
Rethinking the Mathematics of Attention Mechanism
Compressing attention:
Leave No Context Behind:
Efficient Infinite Context Transformers with Infini-attention
17


Practitioners use Information Retrieval methods
to reduce context size
18


RAG system
A clever way to reduce the context size of the prompt is to
not use all documents, but rather a selection.
This is known as a Retrieval Augmented Generator.
19

[IDS] In the context of a NLP lecture, the slide presents the RAG system as a method to enhance the context size for document retrieval and generation. It's a clever approach that avoids the limitations of using all documents by selecting a subset, which is then processed through a Retrieval-Augmented Generator. This process likely involves using a coordinator to manage the flow of information between the question, the document database, and the language model (LLM). The LLM would be responsible for generating the final output based on the information retrieved from the database and the input question. The slide number 19 suggests this is part of a larger presentation, and the visual elements help to explain the complex interaction between different components in the RAG system. [IDE]


How to retrieve the documents?
20


Vector search (index documents)
Embed text sequences into a vecor space. E.g. by using:
Token embedder, like BERT, or a headless LLM
Specially trained models, like SentenceBert
21

[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It specifically focuses on the topic of retrieval-augmented generation. The slide outlines the concept of embedding text sequences into a vector space, using token embeddings like BERT or a headless LLM. It also mentions specially trained models such as SentenceBert. These techniques are essential in NLP for tasks like language understanding and generation. [IDE]


Vector search (retrieve similar documents)
Retrieve semantically similar documents by comparing the vectors. (see nice blog post)
Angular/Cosine- or 
-distance
Karpathy's idea of kernel-distance
22

[IDS] The image you're seeing is a visual representation of the concept of vector search in the context of Natural Language Processing (NLP). It illustrates how vector search algorithms, such as the Sentence Embedding Algorithm shown here, can be used to retrieve semantically similar documents by comparing their vectors. The example given in the image uses the cosine or L1 distance as a measure to determine similarity between the vectors of two documents. The presence of terms like "nice blog post" suggests that the lecture might be discussing practical applications of this technique in retrieving relevant blog posts from a larger corpus of text. This is a fundamental concept in information retrieval and NLP, allowing systems to understand the meaning behind words and retrieve content that is semantically related. [IDE]


The retrieval based approach has many parameters
Which embedding algorithm  to use?
Which similarity measure  to use?
How do you chunk  the documents to get a good embedding?
How to include metadata ?
Process tables  in the document?
How to represent images ?
Do you want to embedd a summary ?
Retrieve surrounding chunks  as well?
Embed the query, or rather a hypothetical answer  to the query
23


You can also just use a keyword related search
Considering the RAG architecture, you don't need a VectorDB.
You just need to find documents fitting a given text query.
Algorithms that come to mind are:
Keyword search
TF-IDF
BM25
24

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on Retrieval-Augmented Generation. It explains that one can use a keyword-related search rather than the RAG architecture, which is not necessary for VectorDB. The slide emphasizes the need to find documents fitting a given text query using algorithms like keyword search, TF-IDF, and BM25. This concept is essential in NLP as it aids in retrieving relevant information from large datasets by matching keywords or phrases from a query with documents in a database. [IDE]


Hybrid search: Combining search results
Of course you can apply several retrieval strategies and merge search results:
Just use all retrieved documents (not recommended; adds to context size)
Use the top-  documents of each retrieval algorithm
Top-  mean reciprocal re-ranked documents
Top-  documents of a machine-learned ranking
Given feedback, you can mix utilizing multi-armed-bandit theory
25


Dynamic RAG with intermediate queries
Improve on the "ask-once, retrieve once" workflow.
Utilize LLMs with text understanding tasks:
Ask "Are these documents interesting for the following question?"
Ask "Is this an answer to the question?"
Generate sub-queries
You can use frameworks like instructor or autogen to process the LLMs answers.
26


RAG in a corporate environment is special
Data may not be allowed to leave the company
Who is paying for the hardware or the service?
Competing groups building the same thing
Networks inside company
User authentication / robot users
People abusing your service
Corporate internal certification
27
[Lecture End]

[Lecture Start]

------------Statistical_language_models------------
Dan Jurafsky
Probabilistic Language Models
‚Ä¢ Today‚Äôs goal: assign a probability to a sentence
‚Ä¢ Machine Translation:
‚Ä¢ P(high winds tonite) > P(large winds tonite)
‚Ä¢ Spell Correction
‚Ä¢ The office is about fifteen minuets from my house
‚Ä¢ P(about fifteen minutes from) > P(about fifteen minuets from)
‚Ä¢ Speech Recognition
‚Ä¢ P(I saw a van) >> P(eyes awe of an)
‚Ä¢ + Summarization, question-answering, etc., etc.!!
Why?

[IDS] The image is a snapshot from an NLP (Natural Language Processing) lecture by Dan Jurafsky at Stanford University. The slide is titled "Probabilistic Language Models" and focuses on the concept of assigning a probability to a sentence. It outlines today's goal, which is to assign probabilities to sentences using machine translation as an example. The slide mentions the P(high winds tonight) and P(large winds tonight) as examples of assigning probabilities. Additionally, it introduces spell correction and speech recognition as other topics covered in the lecture. The office hours are mentioned for fifteen minutes, with a note about fifteen-minute meetings. The slide concludes with a reference to summarization, question-answering, etc., indicating these are additional topics or activities related to the lecture. [IDE]


Dan Jurafsky
Probabilistic Language Modeling
‚Ä¢ Goal: compute the probability of a sentence or 
sequence of words:
     P(W) = P(w1,w2,w3,w4,w5‚Ä¶wn)
‚Ä¢ Related task: probability of an upcoming word:
      P(w5|w1,w2,w3,w4)
‚Ä¢ A model that computes either of these:
          P(W)     or     P(wn|w1,w2‚Ä¶wn-1)         is called a language model.
‚Ä¢ Better: the grammar       But language model or LM is standard

[IDS] In this NLP lecture, we're discussing Probabilistic Language Modeling. The goal is to compute the probability of a sentence or sequence of words. This involves a related task: predicting an upcoming word given a sequence of previous words. A model that computes either of these is called a language model. It's worth noting that while the grammar model is standard, the language model or LM is considered better. [IDE]


Dan Jurafsky
How to compute P(W)
‚Ä¢ How to compute this joint probability:
‚Ä¢ P(its, water, is, so, transparent, that)
‚Ä¢ Intuition: let‚Äôs rely on the Chain Rule of Probability

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on statistical language models. It explains how to compute the probability P(W) of a sequence of words W in a given context, emphasizing the importance of intuition and understanding the Chain Rule of Probability. The slide serves as a visual aid to help students grasp the concept of joint probabilities and their application in NLP tasks. [IDE]


Dan Jurafsky
Reminder: The Chain Rule
‚Ä¢ Recall the definition of conditional probabilities
p(B|A) = P(A,B)/P(A) 
Rewriting:   P(A,B) = P(A)P(B|A)
‚Ä¢ More variables:
 P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)
‚Ä¢ The Chain Rule in General
  P(x1,x2,x3,‚Ä¶,xn) = P(x1)P(x2|x1)P(x3|x1,x2)‚Ä¶P(xn|x1,‚Ä¶,xn-1)

[IDS] The image displays a slide from an NLP (Natural Language Processing) lecture, specifically discussing statistical language models. It emphasizes the concept of conditional probabilities and their role in defining the Chain Rule, which is a fundamental principle in probability theory and statistics. The slide also highlights the importance of understanding variables and their relationships within the context of language modeling. This topic is crucial for NLP as it helps in predicting the likelihood of words or phrases occurring in a given sequence, which is essential for tasks such as speech recognition, machine translation, and text generation. [IDE]


Dan Jurafsky
The Chain Rule applied to compute 
joint probability of words in sentence
P(‚Äúits water is so transparent‚Äù) =
 P(its) √ó P(water|its) √ó  P(is|its water) 
         √ó  P(so|its water is) √ó  P(transparent|its water is so)
  
 
P(w1w2‚Ä¶wn) =
P(wi | w1w2‚Ä¶wi‚àí1)
i
‚àè

[IDS] In this NLP lecture, we are discussing the Chain Rule applied to compute joint probabilities of words in a sentence. This is a concept from statistical language models, which are used to predict the likelihood of sequences of words in natural language. The slide illustrates the Chain Rule with an equation that shows how to calculate the probability of a sequence of words (P(w1...wi|wi-1)) by multiplying the individual probabilities of each word given the previous word. [IDE]


Dan Jurafsky
How to estimate these probabilities
‚Ä¢ Could we just count and divide?
‚Ä¢ No!  Too many possible sentences!
‚Ä¢ We‚Äôll never see enough data for estimating these
P(the |its water is so transparent that) =
Count(its water is so transparent that the)
Count(its water is so transparent that)

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide that discusses the challenges and limitations of estimating probabilities in statistical language models. The slide is likely part of a presentation by Dan Jurafsky from Stanford University's NLP program, emphasizing the difficulty in counting and dividing to estimate probabilities when dealing with data such as water, which is considered transparent and abundant. This illustrates the complexity of modeling language patterns and the need for innovative methods to overcome these challenges in NLP. [IDE]


Dan Jurafsky
Markov Assumption
‚Ä¢ Simplifying assumption:
‚Ä¢ Or maybe
P(the |its water is so transparent that) ‚âàP(the |that)
P(the |its water is so transparent that) ‚âàP(the |transparent that)
Andrei Markov

[IDS] The image displays a slide from an NLP lecture, specifically focusing on the Markov Assumption. This assumption is crucial in statistical language models, which are used to predict the likelihood of sequences of words or characters in natural language. The slide simplifies the concept by suggesting that the probability of a sequence of words (P(the lit water is so transparent that)) is equal to the product of individual probabilities (P(the | that) * P(transparent | that)). This is a common approach in Markov models, where the future state is determined by the current state, and it's applied to NLP tasks like text generation and speech recognition. [IDE]


Dan Jurafsky
Markov Assumption
‚Ä¢ In other words, we approximate each 
component in the product
  
P(w1w2‚Ä¶wn) ‚âà
P(wi | wi‚àík‚Ä¶wi‚àí1)
i
‚àè
  
P(wi | w1w2‚Ä¶wi‚àí1) ‚âàP(wi | wi‚àík‚Ä¶wi‚àí1)

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept known as Markov Assumption. It illustrates how in NLP models, we often approximate the probability of a sequence of words by considering the probability of each word given its immediate predecessor, ignoring the rest of the context. This is a simplification that allows us to work with large amounts of data and complex language patterns. The equation shown is a key component of many statistical language models used in NLP to predict the next word in a sentence or sequence. [IDE]


Dan Jurafsky
Simplest case: Unigram model
fifth, an, of, futures, the, an, incorporated, a, 
a, the, inflation, most, dollars, quarter, in, is, 
mass
thrift, did, eighty, said, hard, 'm, july, bullish
that, or, limited, the
Some automatically generated sentences from a unigram model
  
‚Ç¨ 
P(w1w2‚Ä¶wn) ‚âà
P(wi)
i
‚àè

[IDS] The image is a slide from an NLP lecture that focuses on the topic of statistical language models. It introduces the Unigram model as a simple case of such models, which predicts the probability of a sentence by considering each word independently. The slide provides a formula for the probability of a sentence using the unigram model and includes an example sentence with its corresponding probability calculation. The context of the lecture is to educate students about how these models work and their application in natural language processing tasks. [IDE]


Dan Jurafsky
Condition on the previous word:
Bigram model
texaco, rose, one, in, this, issue, is, pursuing, growth, in, 
a, boiler, house, said, mr., gurria, mexico, 's, motion, 
control, proposal, without, permission, from, five, hundred, 
fifty, five, yen
outside, new, car, parking, lot, of, the, agreement, reached
this, would, be, a, record, november
  
P(wi | w1w2‚Ä¶wi‚àí1) ‚âàP(wi | wi‚àí1)

[IDS] The image is a visual representation of a concept from Natural Language Processing (NLP), specifically related to Statistical Language Models. It illustrates the probability of a word given the previous word in a sequence, which is a key component of understanding language patterns and predicting the next word in a sentence or text. This model helps NLP systems to make more informed decisions when processing and generating human language. [IDE]


Dan Jurafsky
N-gram models
‚Ä¢ We can extend to trigrams, 4-grams, 5-grams
‚Ä¢ In general this is an insufficient model of language
‚Ä¢ because language has long-distance dependencies:
‚ÄúThe computer which I had just put into the machine room on 
the fifth floor crashed.‚Äù
‚Ä¢ But we can often get away with N-gram models

[IDS] In the image, we see a slide from a lecture on NLP (Natural Language Processing) that specifically focuses on statistical language models. The slide is titled "N-gram models" and discusses the limitations of these models when extended to longer sequences of words. It mentions the issue of long-distance dependencies and how they can cause computational problems, such as a computer crashing. The slide also suggests that these problems can be mitigated by using N-gram models. This information is presented in bullet points for clarity and emphasis. [IDE]


Introduction to N-grams
Language 
Modeling

[IDS] In this NLP lecture, you are likely learning about statistical language models. These models use probabilities to predict the likelihood of a sequence of words or characters in a given text. They are fundamental in understanding how language is structured and can be used for tasks such as language translation, speech recognition, and text generation. The image serves as an introduction to the topic, using a word cloud to visually represent the complexity and diversity of concepts within the field of language modeling. [IDE]


Estimating N-gram 
Probabilities
Language 
Modeling

[IDS] The image is a visual representation of the topic "Statistical Language Models," which is a key concept in Natural Language Processing (NLP). It features a word cloud with various terms related to NLP and language modeling, emphasizing the complexity and interdisciplinary nature of the field. The word cloud serves as a metaphor for the interconnectedness of ideas within the subject, highlighting the importance of understanding how language works at both the sentence and paragraph levels. This kind of visualization helps students grasp the vast scope of topics that fall under the umbrella of NLP, especially statistical language models, which are used to estimate the probability of sequences of words in a given language. The presence of this image in a lecture setting suggests that the instructor is likely discussing the fundamentals of language modeling, its applications, and its role in advancing NLP technologies. [IDE]


Dan Jurafsky
Estimating bigram probabilities
‚Ä¢ The Maximum Likelihood Estimate
‚Ç¨ 
P(wi | wi‚àí1) = count(wi‚àí1,wi)
count(wi‚àí1)
P(wi | wi‚àí1) = c(wi‚àí1,wi)
c(wi‚àí1)

[IDS] In this NLP lecture, we are discussing the concept of "Estimating bigram probabilities" which is a fundamental aspect of statistical language models. These models use probability distributions to predict the likelihood of sequences of words in natural language. The slide presents the maximum likelihood estimate formula for calculating bigram probabilities, which is a key technique used in language modeling to determine how likely it is to see a sequence of words in a given context. [IDE]


Dan Jurafsky
An example
<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>
P(wi | wi‚àí1) = c(wi‚àí1,wi)
c(wi‚àí1)

[IDS] The image displays an example from a Natural Language Processing (NLP) lecture, specifically related to the topic of Statistical language models. It illustrates how probabilities are calculated in such models, using the context of a person named Sam and their statements about green eggs and ham. The equations provided show how the probability of a sequence of words is determined based on the individual probabilities of each word within that sequence, which is a fundamental concept in NLP for understanding and generating human-like language. [IDE]


Dan Jurafsky
More examples: 
Berkeley Restaurant Project sentences
‚Ä¢ can you tell me about any good cantonese restaurants close by
‚Ä¢ mid priced thai food is what i‚Äôm looking for
‚Ä¢ tell me about chez panisse
‚Ä¢ can you give me a listing of the kinds of food that are available
‚Ä¢ i‚Äôm looking for a good place to eat breakfast
‚Ä¢ when is caffe venezia open during the day

[IDS] The image is a slide from an NLP lecture, discussing examples of Berkeley Restaurant Project sentences. These sentences are used to illustrate the topic of statistical language models in NLP. The examples include various questions and statements about food preferences and availability at restaurants, such as asking for recommendations on good Cantonese restaurants close by, inquiring about mid-priced Thai food options, and seeking information on places to eat breakfast or have coffee in Venice. These sentences serve as practical examples of how statistical language models can be applied in real-world scenarios, particularly in understanding user queries and providing relevant responses. [IDE]


Dan Jurafsky
Raw bigram counts
‚Ä¢ Out of 9222 sentences

[IDS] The image displays a table representing the raw bigram counts in the context of a Natural Language Processing (NLP) lecture on Statistical language models. It illustrates how to calculate the probability of a sentence using bigrams, which are pairs of adjacent words in a sequence. This is a fundamental concept in NLP for modeling the structure of language and predicting the next word in a sentence based on the previous one. The table shows the number of times each bigram appears in a given set of sentences, emphasizing the importance of frequency in statistical language modeling. [IDE]


Dan Jurafsky
Raw bigram probabilities
‚Ä¢
Normalize by unigrams:
‚Ä¢
Result:

[IDS] In the context of an NLP lecture, the image represents a practical demonstration of statistical language models. It illustrates the concept of probability normalization and how it is applied to predict the likelihood of different words appearing in a sequence. The table shows the raw probabilities for various words such as "i", "want", "to", "eat", "chinese", "food", "lunch", and "spend". These probabilities are then normalized to ensure they sum up to 1, reflecting the model's confidence in each word given the context. This process is crucial for understanding the context in which words are used and for making accurate predictions about what might come next in a text. As we delve deeper into NLP, these statistical models form the foundation for tasks like language translation, sentiment analysis, and text generation. [IDE]


Dan Jurafsky
Bigram estimates of sentence probabilities
P(<s> I want english food </s>) =
 P(I|<s>)   
 √ó  P(want|I)  
 √ó  P(english|want)   
 √ó  P(food|english)   
 √ó  P(</s>|food)
       =  .000031

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a fundamental concept in statistical language modeling. The slide is titled "Bigram estimates of sentence probabilities," indicating that it discusses how to estimate the probability of a sequence of two words in a sentence, also known as bigrams. The equation P(<s> I want english food <s>) = P(I|<s>) x P(want|I) x P(english|want) x P(food|english) x P(<s>/|food) =.000031 illustrates how these probabilities are calculated and combined to determine the overall probability of a given sentence. This is a crucial aspect of understanding how computers can predict the next word in a sentence or evaluate the likelihood of a sentence being part of a certain language or text corpus. [IDE]


Dan Jurafsky
What kinds of knowledge?
‚Ä¢ P(english|want)  = .0011
‚Ä¢ P(chinese|want) =  .0065
‚Ä¢ P(to|want) = .66
‚Ä¢ P(eat | to) = .28
‚Ä¢ P(food | to) = 0
‚Ä¢ P(want | spend) = 0
‚Ä¢ P (i | <s>) = .25

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing statistical language models. It provides examples of how probability is used to represent the likelihood of different linguistic structures, such as sentences or phrases, within a language. The slide is likely part of a larger presentation that would delve into the applications and methodologies of statistical language models in NLP tasks like language generation or sentiment analysis. [IDE]


Dan Jurafsky
Practical Issues
‚Ä¢ We do everything in log space
‚Ä¢ Avoid underflow
‚Ä¢ (also adding is faster than multiplying)
log(p1 √ó p2 √ó p3 √ó p4) = log p1 + log p2 + log p3 + log p4

[IDS] In the context of a NLP lecture, this image represents a slide discussing practical issues related to statistical language models. It emphasizes the need to handle these challenges in log space, avoid underflow, and mentions that addition is faster than multiplication when dealing with logarithms. The formula provided demonstrates how to compute the logarithm of a product using properties of logarithms. This information is crucial for understanding and implementing efficient algorithms in natural language processing tasks that involve statistical models. [IDE]


Dan Jurafsky
Language Modeling Toolkits
‚Ä¢ SRILM
‚Ä¢ http://www.speech.sri.com/projects/srilm/
‚Ä¢ KenLM
‚Ä¢ https://kheafield.com/code/kenlm/

[IDS] The image displays a slide from an NLP lecture, specifically focusing on "Statistical language models". It highlights three language modeling toolkits: SRI LM, KenLM, and https://kheafield.com/code/kenlm/. These toolkits are likely used for generating statistical models of language to improve various NLP tasks. [IDE]


Dan Jurafsky
Google N-Gram Release, August 2006
‚Ä¶

[IDS] In the image, you see a screenshot of an announcement for the release of the Google N-Gram Release from August 2006. The announcement is made by Alex Franz and Thorsten Brants from the Google Machine Translation team. The context of this lecture is likely related to Natural Language Processing (NLP), specifically focusing on statistical language models. The Google N-Gram data set is a significant resource in NLP research, providing information about word frequencies across various corpora. In this case, they have processed over a trillion words, which is a testament to the scale of data used in modern NLP applications. [IDE]


Dan Jurafsky
Google N-Gram Release
‚Ä¢
serve as the incoming 92
‚Ä¢
serve as the incubator 99
‚Ä¢
serve as the independent 794
‚Ä¢
serve as the index 223
‚Ä¢
serve as the indication 72
‚Ä¢
serve as the indicator 120
‚Ä¢
serve as the indicators 45
‚Ä¢
serve as the indispensable 111
‚Ä¢
serve as the indispensible 40
‚Ä¢
serve as the individual 234
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html

[IDS] The image represents a Google N-Gram Release, which is a dataset containing information about the frequency of words and phrases in books from Google's massive book digitization project. In the context of an NLP (Natural Language Processing) lecture, this data can be used to study language trends and patterns over time. It is particularly useful for statistical language models, which aim to predict the likelihood of sequences of words in a given text by analyzing their historical usage. The presence of this image in the lecture suggests that the topic might involve discussing how such datasets contribute to the development and training of statistical language models. [IDE]


Dan Jurafsky
Google Book N-grams
‚Ä¢ http://ngrams.googlelabs.com/

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a reference to a resource related to statistical language models. The Google Book N-grams data set is a significant resource in NLP, providing insights into how frequently sequences of n items occur together. This can be particularly useful for understanding language patterns and trends. The URL provided leads to a Google Labs project that likely offers access to this valuable data set, which would be essential for students or researchers working on statistical language modeling tasks. [IDE]


Estimating N-gram 
Probabilities
Language 
Modeling

[IDS] In the image, you can see a visual representation of a lecture focused on "Statistical Language Models," which is a topic within the field of Natural Language Processing (NLP). The word cloud in the background is composed of various words related to language and modeling, such as "probability," "grammar," "modeling," "estimating," and "probabilities." These words highlight the key concepts discussed in the lecture. The presence of these terms suggests that the lecture might cover how statistical models are used to estimate probabilities in language processing tasks. The visual design effectively conveys the central theme of the lecture and serves as an engaging backdrop for the topic. [IDE]


Language 
Modeling
Evaluation and Perplexity


How to evaluate N-gram models
"Extrinsic (in-vivo) Evaluation"
To compare models A and B
1. Put each model in a real task
‚Ä¢ Machine Translation, speech recognition, etc. 
2. Run the task, get a score for A and for B
‚Ä¢ How many words translated correctly
‚Ä¢ How many words transcribed correctly
3. Compare accuracy for A and B


Intrinsic (in-vitro) evaluation 
Extrinsic evaluation not always possible
‚Ä¢ Expensive, time-consuming 
‚Ä¢ Doesn't always generalize to other applications
Intrinsic evaluation: perplexity
‚Ä¢ Directly measures language model performance at 
predicting words.
‚Ä¢ Doesn't necessarily correspond with real application 
performance
‚Ä¢ But gives us a single general metric for language models
‚Ä¢ Useful for large language models (LLMs) as well as n-grams


Training sets and test sets
We train parameters of our model on a training set.
We test the model‚Äôs performance on data we 
haven‚Äôt seen.
‚ó¶A test set is an unseen dataset; different from training set.
‚ó¶Intuition: we want to measure generalization to unseen data
‚ó¶An evaluation metric (like perplexity) tells us how well 
our model does on the test set.


Choosing training and test sets
‚Ä¢ If we're building an LM for a specific task
‚Ä¢ The test set should reflect the task language we 
want to use the model for
‚Ä¢ If we're building a general-purpose model
‚Ä¢ We'll need lots of different kinds of training 
data
‚Ä¢ We don't want the training set or the test set to 
be just from one domain or author or language.


Training on the test set
We can‚Äôt allow test sentences into the training set
‚Ä¢ Or else the LM will assign that sentence an artificially 
high probability when we see it in the test set
‚Ä¢ And hence assign the whole test set a falsely high 
probability.
‚Ä¢ Making the LM look better than it really is
This is called ‚ÄúTraining on the test set‚Äù
Bad science! 
33


Dev sets
‚Ä¢If we test on the test set many times we might 
implicitly tune to its characteristics
‚Ä¢Noticing which changes make the model better. 
‚Ä¢So we run on the test set only once, or a few times
‚Ä¢That means we need a third dataset: 
‚Ä¢ A development test set or, devset. 
‚Ä¢ We test our LM on the devset until the very end
‚Ä¢ And then test our LM on the test set once


Intuition of perplexity as evaluation metric: 
How good is our language model?
Intuition: A good LM prefers "real" sentences
‚Ä¢
Assign higher probability to ‚Äúreal‚Äù or ‚Äúfrequently 
observed‚Äù sentences 
‚Ä¢
Assigns lower probability to ‚Äúword salad‚Äù or 
‚Äúrarely observed‚Äù sentences?


Intuition of perplexity 2: 
Predicting upcoming words
The Shannon Game: How well can we 
predict the next word?
‚Ä¢  Once upon a ____
‚Ä¢  That is a picture of a  ____
‚Ä¢  For breakfast I ate my usual ____
Unigrams are terrible at this game (Why?)
 
 
 
time 
0.9
dream 
0.03
midnight 0.02
‚Ä¶
and 
1e-100
Picture credit: Historiska bildsamlingen
https://creativecommons.org/licenses/by/2.0/
Claude Shannon
A good LM is one that assigns a higher probability 
to the next word that actually occurs

[IDS] The image is a slide from a lecture on statistical language models, specifically focusing on the Shannon Game. It demonstrates the concept of perplexity, which measures how well a model predicts the next word in a sequence. The slide features a picture of Claude Shannon and includes a text-based example of the game, along with a graph that illustrates the perplexity values over time. This visual aid helps students understand how a higher likelihood (lower perplexity) correlates with a better prediction of upcoming words in a text. [IDE]


Intuition of perplexity 3: The best language model 
is one that best predicts the entire unseen test set
‚Ä¢ We said: a good LM is one that assigns a higher 
probability to the next word that actually occurs. 
‚Ä¢ Let's generalize to all the words!
‚Ä¢ The best LM assigns high probability to the entire test 
set.
‚Ä¢ When comparing two LMs, A and B
‚Ä¢ We compute PA(test set) and PB(test set)
‚Ä¢ The better LM will give a higher probability to (=be less 
surprised by) the test set than the other LM.


‚Ä¢ Probability depends on size of test set
‚Ä¢ Probability gets smaller the longer the text
‚Ä¢ Better: a metric that is per-word, normalized by length
‚Ä¢ Perplexity is the inverse probability of the test set, 
normalized by the number of words
Intuition of perplexity 4: Use perplexity instead of 
raw probability
PP(W)
=
P(w1w2...wN )
‚àí1
N
           
=
1
P(w1w2...wN )
N


Perplexity is the inverse probability of the test set, 
normalized by the number of words
(The inverse comes from the original definition of perplexity 
from cross-entropy rate in information theory)
Probability range is  [0,1], perplexity range is [1,‚àû]
Minimizing perplexity is the same as maximizing probability
Intuition of perplexity 5: the inverse
PP(W)
=
P(w1w2...wN )
‚àí1
N
           
=
1
P(w1w2...wN )
N


Intuition of perplexity 6: N-grams
PP(W)
=
P(w1w2...wN )
‚àí1
N
           
=
1
P(w1w2...wN )
N
Bigrams:
Chain rule:

[IDS] The image is a visual aid for a lecture on "Statistical language models," specifically focusing on the concept of perplexity and N-grams. Perplexity is a measure of how well a statistical model predicts a sample. A lower perplexity score indicates that the model has a better fit to the data. In this context, N-grams are sequences of n items from a given sample of text or speech. The slide provides a mathematical formula for calculating PP(W) in terms of P(w1w2...wn), which represents the probability of a sequence of words. This formula is a part of the larger equation for PP(W), which involves summing over all possible sequences of words up to length N, and then dividing by the number of such sequences. The slide also introduces the chain rule and Bigrams, which are special cases of N-grams where n=2. Understanding these concepts is crucial for developing effective language models that can predict the next word in a sentence based on the context provided by previous words. [IDE]


Intuition of perplexity 7: 
Weighted average branching factor
Perplexity is also the weighted average branching factor of a language. 
Branching factor: number of possible next words that can follow any word
Example: Deterministic language L = {red,blue, green} 
 
Branching factor = 3 (any word can be followed by red, blue, green)
Now assume LM A where each word follows any other word with equal probability ‚Öì
Given a test set T = "red red red red blue"
PerplexityA(T) = PA(red red red red blue)-1/5 = 
But now suppose red was very likely in training set, such that for LM B:
‚ó¶
P(red) = .8   p(green) = .1  p(blue) = .1
We would expect the probability to be higher, and hence the perplexity to be smaller:
PerplexityB(T) = PB(red red red red blue)-1/5 
((‚Öì)5)-1/5 = (‚Öì)-1
=3
= (.8 * .8 * .8 * .8 * .1) -1/5
=.04096 -1/5
= .527-1
= 1.89


Holding test set constant:
Lower perplexity = better language model
Training 38 million words, test 1.5 million words, WSJ
N-gram 
Order
Unigram
Bigram
Trigram
Perplexity 962
170
109
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>N-gram
Order</th>
      <th>Unigram</th>
      <th>Bigram</th>
      <th>Trigram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Perplexity</td>
      <td>962</td>
      <td>170</td>
      <td>109</td>
    </tr>
  </tbody>
</table>

Language 
Modeling
Evaluation and Perplexity


Language 
Modeling
Sampling and Generalization


The Shannon (1948) Visualization Method
Sample words from an LM
Unigram:
REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME 
CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO 
OF TO EXPERT GRAY COME TO FURNISHES THE LINE 
MESSAGE HAD BE THESE. 
Bigram:
THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER 
THAT THE CHARACTER OF THIS POINT IS THEREFORE 
ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO 
EVER TOLD THE PROBLEM FOR AN UNEXPECTED.
Claude Shannon

[IDS] The image you're seeing is likely a slide from a lecture on NLP (Natural Language Processing), specifically focusing on statistical language models. It discusses two methods: the Shannon (1948) Visualization Method and the Bigram method, both used to represent and analyze the probability of word sequences in text. The slide features a quote from Claude Shannon, emphasizing the importance of representing and analyzing natural language effectively, which is crucial for understanding how language works and for developing advanced NLP applications. [IDE]


How Shannon sampled those words in 1948
"Open a book at random and select a letter at random on the page. 
This letter is recorded. The book is then opened to another page 
and one reads until this letter is encountered. The succeeding 
letter is then recorded. Turning to another page this second letter 
is searched for and the succeeding letter recorded, etc."

[IDS] The image you're viewing is a visual representation of the process described in the text, which is related to statistical language models. These models are used in natural language processing (NLP) to predict the next word in a sequence based on statistical patterns learned from a large corpus of text data. The book in the image symbolizes the vast amount of data that such models are trained on, while the open pages represent the randomness and variability of language. The act of selecting a letter at random and recording it in the book illustrates how statistical language models learn by observing and predicting the frequency and co-occurrence of words in texts. This process helps the model to generate coherent and contextually relevant text, which is essential for tasks like language translation, speech recognition, and text completion. [IDE]


Sampling a word from a distribution
0
1
0.06
the
.06
0.03
of
0.02
a
0.02
to in
.09 .11 .13 .15
‚Ä¶
however
(p=.0003)
polyphonic
p=.0000018
‚Ä¶
0.02
.66
.99
‚Ä¶
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0-0.06</th>
      <th>1-0.03</th>
      <th>2-0.02</th>
      <th>3-0.02</th>
      <th>4-0.02</th>
      <th>Col5</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
[IDS] The image is a visual representation of a concept in statistical language models, specifically illustrating the process of sampling from a distribution. It demonstrates how words are sampled with probabilities proportional to their frequency in a text corpus. The polyphonic however, with a probability of.0003, is sampled as an example. This image helps learners understand how language models predict the likelihood of a word occurring in a sequence by assigning probabilities based on the frequency of word co-occurrence in large datasets. The slide serves as a practical explanation of the theoretical concepts taught in NLP lectures, making it easier for students to grasp the underlying principles of statistical language modeling. [IDE]


Visualizing Bigrams the Shannon Way
Choose a random bigram (<s>, w) 
         according to its probability p(w|<s>)
Now choose a random bigram        (w, x) 
according to its probability p(x|w)
And so on until we choose </s>
Then string the words together
<s> I
    I want
      want to
           to eat
              eat Chinese
                  Chinese food
                          food </s>
I want to eat Chinese food


Note: there are other sampling methods
Used for neural language models
Many of them avoid generating words from the very 
unlikely tail of the distribution
We'll discuss when we get to neural LM decoding:
‚ó¶Temperature sampling
‚ó¶Top-k sampling
‚ó¶Top-p sampling


Approximating Shakespeare
of modeling the training corpus as we increase the value of N.
We can use the sampling method from the prior section to visualize both of
these facts! To give an intuition for the increasing power of higher-order n-grams,
Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-
gram models trained on Shakespeare‚Äôs works.
1
‚ÄìTo him swallowed confess hear both. Which. Of save on trail for are ay device and
rote life have
gram
‚ÄìHill he late speaks; or! a more to leg less first you enter
2
‚ÄìWhy dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live
king. Follow.
gram
‚ÄìWhat means, sir. I confess she? then all sorts, he is trim, captain.
3
‚ÄìFly, and will rid me these news of price. Therefore the sadness of parting, as they say,
‚Äôtis done.
gram
‚ÄìThis shall forbid it should be branded, if renown made it empty.
4
‚ÄìKing Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A
great banquet serv‚Äôd in;
gram
‚ÄìIt cannot be but so.
Figure 3.4
Eight sentences randomly generated from four n-grams computed from Shakespeare‚Äôs works. All
characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected
for capitalization to improve readability.
The longer the context on which we train the model, the more coherent the sen-
tences. In the unigram sentences, there is no coherent relation between words or any
sentence-final punctuation. The bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). The tri-


Shakespeare as corpus
N=884,647 tokens, V=29,066
Shakespeare produced 300,000 bigram types out of 
V2= 844 million possible bigrams.
‚ó¶So 99.96% of the possible bigrams were never seen (have 
zero entries in the table)
‚ó¶That sparsity is even worse for 4-grams, explaining why 
our sampling generated actual Shakespeare.


The Wall Street Journal is not Shakespeare
3.5
‚Ä¢
GENERALIZATION AND ZEROS
13
1
Months the my and issue of year foreign new exchange‚Äôs september
were recession exchange new endorsed a acquire to six executives
gram
2
Last December through the way to preserve the Hudson corporation N.
B. E. C. Taylor would seem to complete the major central planners one
gram
point five percent of U. S. E. has already old M. X. corporation of living
on information such as more frequently fishing to keep her
3
They also point to ninety nine point six billion dollars from two hundred
four oh six three percent of the rates of interest stores as Mexico and
gram
Brazil on market conditions
Figure 3.5
Three sentences randomly generated from three n-gram models computed from
40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-
tion as words. Output was then hand-corrected for capitalization to improve readability.
we need a training corpus of legal documents. To build a language model for a
question-answering system, we need a training corpus of questions.
It is equally important to get training data in the appropriate dialect or variety,
especially when processing social media posts or spoken transcripts. For exam-


Can you guess the author? These 3-gram sentences 
are sampled from an LM trained on who?
1) They also point to ninety nine point 
six billion dollars from two hundred four 
oh six three percent of the rates of 
interest stores as Mexico and gram Brazil 
on market conditions 
2) This shall forbid it should be branded, 
if renown made it empty. 
3) ‚ÄúYou are uniformly charming!‚Äù cried he, 
with a smile of associating and now and 
then I bowed and they perceived a chaise 
and four to wish for. 
53


Choosing training data
If task-specific, use a training corpus that has a similar 
genre to your task.
‚Ä¢ If legal or medical, need lots of special-purpose documents
Make sure to cover different kinds of dialects and 
speaker/authors.
‚Ä¢ Example: African-American Vernacular English (AAVE)
‚Ä¢ One of many varieties that can be used by African Americans and others
‚Ä¢ Can include the auxiliary verb finna that marks immediate future tense:
‚Ä¢ "My phone finna die"


The perils of overfitting
N-grams only work well for word prediction if the 
test corpus looks like the training corpus
‚Ä¢ But even when we try to pick a good training 
corpus, the test set will surprise us!
‚Ä¢ We need to train robust models that generalize!
One kind of generalization: Zeros
‚Ä¢ Things that don‚Äôt ever occur in the training set
‚Ä¢ But occur in the test set


Zeros
Training set:
‚Ä¶ ate lunch
‚Ä¶ ate dinner
‚Ä¶ ate a
‚Ä¶ ate the
P(‚Äúbreakfast‚Äù | ate) = 0
‚Ä¢ Test set
‚Ä¶ ate lunch
‚Ä¶ ate breakfast


Zero probability bigrams
Bigrams with zero probability
‚ó¶Will hurt our performance for texts where those words 
appear!
‚ó¶And mean that we will assign 0 probability to the test set!
And hence we cannot compute perplexity (can‚Äôt 
divide by 0)!


Language 
Modeling
Sampling and Generalization


Smoothing: Add-one 
(Laplace) smoothing
Language 
Modeling

[IDS] The image is a visual representation of the concept of "Language Modeling" in Natural Language Processing (NLP). It emphasizes the importance of statistical language models in understanding and generating human language. The word cloud serves as a metaphor for the complexity and interconnectedness of words and their meanings within language. The term "Smoothing: Add-one (Laplace) smoothing" suggests that the lecture might cover techniques to improve the performance of language models by adding a small amount of probability mass to all events, which is a common method in statistical language modeling to handle the problem of unseen data. [IDE]


Dan Jurafsky
The intuition of smoothing (from Dan Klein)
‚Ä¢
When we have sparse statistics:
‚Ä¢
Steal probability mass to generalize better
P(w | denied the)
  3 allegations
  2 reports
  1 claims
  1 request
  7 total
P(w | denied the)
  2.5 allegations
  1.5 reports
  0.5 claims
  0.5 request
  2 other
  7 total
allegations
reports
claims
attack
request
man
outcome
‚Ä¶
allegations
attack
man
outcome
‚Ä¶
allegations
reports
claims
request

[IDS] In this NLP lecture, we are discussing the concept of "The intuition of smoothing" from Dan Klein. The slide explains how sparse statistics can be handled when working with language models. It introduces the concept of P(w | denied the) and mentions various statistics such as 3 allegations, 2 reports, 1 claim, 1 request, and a total count of 7. The slide also discusses the idea of stealing probability mass to generalize better, showing a probability distribution with different outcomes like allegations, claims, requests, and outcomes such as attack, man, and outcome. This is likely part of a larger discussion on how to improve the performance and robustness of statistical language models by considering the context and rarity of certain events or words in the data. [IDE]


Dan Jurafsky
Add-one estimation
‚Ä¢ Also called Laplace smoothing
‚Ä¢ Pretend we saw each word one more time than we did
‚Ä¢ Just add one to all the counts!
‚Ä¢ MLE estimate:
‚Ä¢ Add-1 estimate:
P
MLE(wi | wi‚àí1) = c(wi‚àí1,wi)
c(wi‚àí1)
P
Add‚àí1(wi | wi‚àí1) = c(wi‚àí1,wi)+1
c(wi‚àí1)+V

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image likely represents a slide discussing techniques for improving language models. The "Add-one estimation" method is a technique used to smooth the Laplace smoothing in NLP models. It's a way to prevent overfitting by adding a small value to the counts of each word in a document, which helps to deal with rare words or out-of-vocabulary (OOV) words. The formula shown calculates the probability of a word given its context in the model, taking into account the added value. This is an important concept in statistical language models as it improves the model's ability to predict the likelihood of a word appearing in a given context. [IDE]


Dan Jurafsky
Maximum Likelihood Estimates
‚Ä¢
The maximum likelihood estimate
‚Ä¢ of some parameter of a model M from a training set T
‚Ä¢ maximizes the likelihood of the training set T given the model M
‚Ä¢
Suppose the word ‚Äúbagel‚Äù occurs 400 times in a corpus of a million words
‚Ä¢
What is the probability that a random word from some other text will be 
‚Äúbagel‚Äù?
‚Ä¢
MLE estimate is 400/1,000,000 = .0004
‚Ä¢
This may be a bad estimate for some other corpus
‚Ä¢ But it is the estimate that makes it most likely that ‚Äúbagel‚Äù will occur 400 times in 
a million word corpus.

[IDS] In the image, we see a slide from a lecture on Natural Language Processing (NLP), specifically discussing statistical language models. The slide is titled "Maximum Likelihood Estimates" and explains the concept of likelihood estimation in NLP. It mentions that maximum likelihood estimation is used to estimate the parameters of a model M from a training set T and to maximize the likelihood of the training set given the model M. The slide also provides an example of a bad estimate for some other corpus and defines it as the estimate that makes it most likely that a "bagel" will occur 400 times in a million word corpus. This is likely part of a larger discussion on how these estimates are used to improve language understanding and generation in NLP applications. [IDE]


Dan Jurafsky
Berkeley Restaurant Corpus: Laplace 
smoothed bigram counts

[IDS] The image depicts a table illustrating the concept of bigram counts, which is a fundamental aspect of statistical language models in NLP. These models analyze patterns in large datasets to predict the likelihood of word sequences in human languages. The table shows how bigrams (pairs of adjacent words) can be counted and used to determine probabilities for predicting the next word in a sequence. This is essential for tasks like text prediction, machine translation, and speech recognition. [IDE]


Dan Jurafsky
Laplace-smoothed bigrams

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept from the field of statistical language models. Specifically, it illustrates how to compute the probability of a sequence of words (in this case, "Laplace-smoothed bigrams") using the formula P(w_n|w_{n-1}) = C(w_{n-1}w_n)/C(w_{n-1}) + V. This formula is used to predict the likelihood of a word given the previous word in a sequence, with the addition of a Laplace smoothing technique to avoid zero probabilities. The table shows an example of this computation for various words and categories like 'i want to eat' and 'chinese food', which helps learners understand how these models can be applied to predict text or speech. [IDE]


Dan Jurafsky
Reconstituted counts

[IDS] The image displays a slide from a lecture on Statistical Language Models, specifically focusing on the concept of reconstituted counts. It illustrates the calculation of C(w|n) using the formula provided in the slide's header and demonstrates how to apply this formula to different words and their respective counts. This is a fundamental concept in NLP, as it helps in understanding how language patterns are represented and analyzed statistically. [IDE]


Dan Jurafsky
Compare with raw bigram counts

[IDS] The image is a visual representation of bigram counts in the context of Natural Language Processing (NLP), which is a field within artificial intelligence that focuses on the interaction between computers and humans in natural language. Bigrams are pairs of words that occur together in a text or speech, and counting them can provide insights into language patterns and relationships between words. The table compares two different datasets: one with 827 instances and another with 527 instances, highlighting the number of times each word pair appears together. This information is crucial for NLP tasks such as language modeling, where understanding how words co-occur can improve machine learning algorithms' ability to predict the next word in a sequence. [IDE]


Dan Jurafsky
Add-1 estimation is a blunt instrument
‚Ä¢ So add-1 isn‚Äôt used for N-grams: 
‚Ä¢ We‚Äôll see better methods
‚Ä¢ But add-1 is used to smooth other NLP models
‚Ä¢ For text classification 
‚Ä¢ In domains where the number of zeros isn‚Äôt so huge.

[IDS] In the context of a NLP lecture, the slide discusses the concept of add-1 estimation as a blunt instrument. It emphasizes that this method is not used for N-grams and instead serves to smooth other NLP models. The slide also mentions that add-1 estimation is particularly useful in domains with a large number of zeros, which could be related to the scarcity of certain words or phrases in a dataset. [IDE]


Smoothing: Add-one 
(Laplace) smoothing
Language 
Modeling

[IDS] The image you've shared is a visual representation of the topic "Statistical Language Models" being discussed in an NLP lecture. It features a word cloud with terms related to language modeling, such as 'probability','sentence', 'word', 'language', and'model'. The central theme is emphasized by the large text "Language Modeling" at the top. Additionally, there's a highlighted section on "Smoothing: Add-one (Laplace) smoothing", which is a technique used in statistical language models to improve their performance. This particular method helps prevent zero probabilities in the model by adding a small constant value to the count of each word, making it more robust and accurate. [IDE]


Interpolation, Backoff, 
and Web-Scale LMs
Language 
Modeling

[IDS] The image represents a visual representation of the topic "Statistical language models" in a lecture context. The word "probability" is prominently displayed, indicating its significance in understanding how statistical models work in language processing. The other words such as "feature", "algorithms", "words", and "corpus" suggest that the lecture will cover various aspects of language modeling, including the extraction of features from text data, the algorithms used to build language models, and the importance of a large corpus for training these models. The presence of words like "tagging", "part-of-speech", and "language" further emphasizes the linguistic focus of the lecture. Overall, the image conveys that the lecture will delve into the intricacies of statistical language models, their applications, and their role in natural language processing. [IDE]


Dan Jurafsky
Backoff and Interpolation
‚Ä¢ Sometimes it helps to use less context
‚Ä¢ Condition on less context for contexts you haven‚Äôt learned much about 
‚Ä¢ Backoff: 
‚Ä¢ use trigram if you have good evidence,
‚Ä¢ otherwise bigram, otherwise unigram
‚Ä¢ Interpolation: 
‚Ä¢ mix unigram, bigram, trigram
‚Ä¢ Interpolation works better

[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Statistical language models". The slide is titled "Backoff and Interpolation" and discusses techniques used in NLP to handle out-of-vocabulary words or unknown contexts. It mentions that sometimes using less context can be helpful, and it introduces the concept of backoff as a method to use trigrams if there's evidence, otherwise bigrams, and unigrams as the last resort. Additionally, interpolation is discussed as a technique that mixes different n-grams to improve the model's performance. [IDE]


Dan Jurafsky
Linear Interpolation
‚Ä¢ Simple interpolation
‚Ä¢ Lambdas conditional on context:
from all the N-gram estimators, weighing and combining the trigram, bigram, and
unigram counts.
In simple linear interpolation, we combine different order N-grams by linearly
interpolating all the models. Thus, we estimate the trigram probability P(wn|wn‚àí2wn‚àí1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
by a l:
ÀÜ
P(wn|wn‚àí2wn‚àí1) = l1P(wn|wn‚àí2wn‚àí1)
+l2P(wn|wn‚àí1)
+l3P(wn)
(4.24)
such that the ls sum to 1:
X
i
li = 1
(4.25)
In a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. This way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in
ÀÜ
P(wn|wn‚àí2wn‚àí1) = l1P(wn|w
+l2P(wn|
+l3P(wn)
such that the ls sum to 1:
X
i
li = 1
In a slightly more sophisticated version of linear i
computed in a more sophisticated way, by condition
if we have particularly accurate counts for a particul
counts of the trigrams based on this bigram will be
make the ls for those trigrams higher and thus giv

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on statistical language models. It introduces the concept of linear interpolation, which is a method used to combine multiple language models into one more accurate model. The slide explains that in linear interpolation, multiple models are weighted and summed together to produce a final output probability. The lambda values (Œª) represent the weights for each model, and the sum of these weights is equal to 1, ensuring that the combined model's probabilities add up to 1 as well. The context of this topic is further elaborated by mentioning that lambdas can be conditioned on certain factors, such as previous words in a sequence, which is a crucial aspect of NLP when dealing with language modeling. [IDE]


Dan Jurafsky
How to set the lambdas?
‚Ä¢ Use a held-out corpus
‚Ä¢ Choose Œªs to maximize the probability of held-out data:
‚Ä¢ Fix the N-gram probabilities (on the training data)
‚Ä¢ Then search for Œªs that give largest probability to held-out set:
Training Data
Held-Out 
Data
Test 
Data
logP(w1...wn | M(Œª1...Œªk)) =
logP
M (Œª1...Œªk )(wi | wi‚àí1)
i
‚àë

[IDS] The image is a slide from a lecture on Statistical Language Models, focusing on the process of setting the lambdas. It explains that one should use a held-out corpus for training data and test data to maximize the probability of held-out data. The slide suggests fixing N-gram probabilities on the training data and then searching for Œªs that give the largest probability to held-out sets. This is represented by a mathematical formula that calculates the log probability of a sequence given a set of Œªs. [IDE]


Dan Jurafsky
Unknown words: Open versus closed 
vocabulary tasks
‚Ä¢
If we know all the words in advanced
‚Ä¢ Vocabulary V is fixed
‚Ä¢ Closed vocabulary task
‚Ä¢
Often we don‚Äôt know this
‚Ä¢ Out Of Vocabulary = OOV words
‚Ä¢ Open vocabulary task
‚Ä¢
Instead: create an unknown word token <UNK>
‚Ä¢ Training of <UNK> probabilities
‚Ä¢ Create a fixed lexicon L of size V
‚Ä¢ At text normalization phase, any training word not in L changed to  <UNK>
‚Ä¢ Now we train its probabilities like a normal word
‚Ä¢ At decoding time
‚Ä¢ If text input: Use UNK probabilities for any word not in training

[IDS] The image is a slide from a lecture on NLP (Natural Language Processing) focusing on statistical language models. It discusses the concept of "unknown words" and their handling in such models, particularly when vocabulary is either fixed or open. The slide suggests that in advanced language processing tasks like closed vocabulary tasks or tasks with a large number of unknown words (OOV), it's crucial to address the issue of OOV words. The slide provides strategies for dealing with these OOV words, such as creating a lexicon of sizes LV, training a fixed lexical size model at text normalization phase, and training the model with its probabilities like a normal word. Additionally, it mentions the use of decoding time and inputting UNK probabilities for any word not in training. [IDE]


Dan Jurafsky
Huge web-scale n-grams
‚Ä¢ How to deal with, e.g., Google N-gram corpus
‚Ä¢ Pruning
‚Ä¢ Only store N-grams with count > threshold.
‚Ä¢ Remove singletons of higher-order n-grams
‚Ä¢ Entropy-based pruning
‚Ä¢ Efficiency
‚Ä¢ Efficient data structures like tries
‚Ä¢ Bloom filters: approximate language models
‚Ä¢ Store words as indexes, not strings
‚Ä¢ Use Huffman coding to fit large numbers of words into two bytes
‚Ä¢ Quantize probabilities (4-8 bits instead of 8-byte float)

[IDS] In the context of a NLP lecture, the image likely represents a slide discussing various techniques used in statistical language models. These models are essential for tasks like language understanding and generation. The slide highlights methods such as pruning, which involves removing less important elements to save space, and efficiency strategies like Bloom filters and Huffman coding. These techniques help in managing large datasets and improving computational efficiency, which is crucial for effective NLP processing. [IDE]


Dan Jurafsky
Smoothing for Web-scale N-grams
‚Ä¢ ‚ÄúStupid backoff‚Äù (Brants et al. 2007)
‚Ä¢ No discounting, just use relative frequencies 
75
S(wi | wi‚àík+1
i‚àí1 ) =
count(wi‚àík+1
i
)
count(wi‚àík+1
i‚àí1 )   if  count(wi‚àík+1
i
) > 0
0.4S(wi | wi‚àík+2
i‚àí1 )      otherwise
"
#
$
$
%
$
$
S(wi) = count(wi)
N

[IDS] The image is from a lecture on NLP (Natural Language Processing), specifically focusing on "Statistical language models." It introduces the concept of "Smoothing for Web-scale N-grams," which is a technique used to handle the issue of "Stupid backoff" as mentioned in a paper by Brants et al. in 2007. The slide explains that without smoothing, just using relative frequencies can lead to the problem of "stupid backoff." To address this, the slide presents a formula for computing the count of an i-th order word sequence, considering both the immediate past and other sequences. This method aims to improve the accuracy of language models by distributing the probability mass more evenly across different n-gram sequences. [IDE]


Dan Jurafsky
N-gram Smoothing Summary
‚Ä¢ Add-1 smoothing:
‚Ä¢ OK for text categorization, not for language modeling
‚Ä¢ The most commonly used method:
‚Ä¢ Extended Interpolated Kneser-Ney
‚Ä¢ For very large N-grams like the Web:
‚Ä¢ Stupid backoff
76

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP). It discusses N-gram Smoothing, a technique used in statistical language models to improve the accuracy of predictions. The slide highlights the importance of adding 1 to smoothing and mentions two common methods: Extended Interpolated Kneser-Ney and For very large N-grams like the Web: Stupid backoff. These methods help in handling rare events or out-of-vocabulary words in text categorization tasks. [IDE]


Dan Jurafsky
Advanced Language Modeling
‚Ä¢ Discriminative models:
‚Ä¢  choose n-gram weights to improve a task, not to fit the  
training set
‚Ä¢ Parsing-based models
‚Ä¢ Caching Models
‚Ä¢ Recently used words are more likely to appear
‚Ä¢ These perform very poorly for speech recognition (why?)
P
CACHE(w | history) = ŒªP(wi | wi‚àí2wi‚àí1)+(1‚àíŒª) c(w ‚ààhistory)
| history |

[IDS] The image depicts a slide from an NLP (Natural Language Processing) lecture focused on the topic of "Statistical language models." The slide outlines the importance of choosing appropriate models for tasks in NLP. It emphasizes the use of discriminative models, which involve selecting n-gram weights to improve performance rather than fitting the training data. Additionally, it mentions parsing-based and caching models as alternatives. The slide also discusses the role of recently used words in improving model performance, suggesting that words like 'history' might be more likely to appear in future contexts. The formula provided is indicative of how such statistical models can predict the likelihood of word sequences based on historical data. [IDE]
[Lecture End]

[Lecture Start]

------------Static_embedding_OneHot_encoding------------

What are word embeddings?
Definition:
Representing a word as a vector in a vector space.
‚ÄùEmbedding‚Äù a word in a vector space.

[IDS] In this NLP lecture, the topic of discussion is "Static_embedding_OneHot_encoding," which refers to the concept of representing words as vectors in a vector space. This process is crucial for natural language processing tasks such as text classification, sentiment analysis, and language modeling. OneHot encoding is a technique used to convert categorical variables into numerical form that can be fed into machine learning algorithms. In the context of word embeddings, it helps in creating a dense vector representation of words that captures their semantic meaning. The slide likely contains further details about how one-hot encoding is applied to create these embeddings, and its importance in modern NLP applications. [IDE]



What are word embeddings?
goose

[IDS] The image is a visual representation of word embeddings, which are a method in Natural Language Processing (NLP) to convert words into vectors of numbers. The lecture topic "Static_embedding_OneHot_encoding" likely refers to the process of creating static embeddings using one-hot encoding, a technique where each word is represented as a binary vector indicating its presence or absence in a vocabulary. The x and y axes represent dimensions in the embedding space, with each word's position reflecting its semantic similarity to other words. This visualization helps in understanding how words are related semantically in a language model. [IDE]



What are word embeddings?
Why do we need to turn words into vectors?

[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It's likely part of a series, as indicated by the numbers on the left side. The main focus of this slide is to introduce the concept of word embeddings and one-hot encoding. Word embeddings are a way to represent words in a vector space where semantically similar words are closer together. One-hot encoding is a technique used to convert categorical data into a form that can be used by machine learning algorithms. In the context of NLP, it's often used to convert words into vectors for further processing. The slide poses a question about the necessity of turning words into vectors, which is a common practice in NLP to enable machines to understand and process language. [IDE]



What are word embeddings?
Why do we need to turn words into vectors?
- Machine learning models work with vectors
- We can do vector math
- We can quantify similarity between vectors

[IDS] The image is a slide from a Natural Language Processing (NLP) lecture, specifically focusing on the topic of word embeddings. It poses the question, "What are word embeddings?" as a starting point for the discussion. The slide emphasizes the need to turn words into vectors and lists several reasons why this is necessary. These reasons include working with machine learning models, performing vector math, and quantifying similarity between vectors. The slide also features logos for Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting that these institutions are involved in the lecture or the presentation. [IDE]



What are word embeddings?
How do we find the dimensions to use?

[IDS] The image displays a slide from a Natural Language Processing (NLP) lecture. The slide is titled "What are word embeddings?" and poses the question of how one would find the dimensions to use in word embeddings. The term "OneHot_encoding" is mentioned, which refers to a technique used to convert categorical data into a form that can be used by machine learning algorithms. The presence of logos suggests that the lecture is part of a collaboration between Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, and the name Tim Metzler might be associated with the lecture or the course. [IDE]



What are word embeddings?
How do we find the dimensions to use?
- Expert knowledge

[IDS] In the image, you see a slide from a Natural Language Processing (NLP) lecture that focuses on the topic of word embeddings and their dimensions. The slide poses the question "What are word embeddings?" and prompts the audience to consider how they find the dimensions to use in these embeddings. It also mentions "Expert knowledge" as a potential approach for answering this question. The logos at the bottom suggest that this lecture is part of a series or event involving Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and is presented by Tim Metzler. [IDE]



What are word embeddings?
How do we find the dimensions to use?
- Expert knowledge
- Domain dependent

[IDS] In this NLP lecture, the focus is on understanding word embeddings and their dimensions. The slide titled "What are word embeddings?" introduces the concept of finding the dimensions to use for these embeddings. It emphasizes the importance of expertise in knowledge domains and the need for domain-dependent approaches to effectively utilize word embeddings. [IDE]



What are word embeddings?
How do we find the dimensions to use?
- Expert knowledge
- Domain dependent
- What can be dimensions for all English words?

[IDS] In the context of a NLP lecture, the image represents a discussion about word embeddings and how to find dimensions to use for different types of language models. The slide outlines three main points: Expert knowledge, Domain dependent, and What can be dimensions for all English words? These points likely serve as a framework for understanding the complexity of word embeddings and the various approaches to selecting appropriate dimensions for language processing tasks. [IDE]



One Hot Encoding
What can be dimensions for all English words?

[IDS] The image displays a slide from an NLP lecture focused on "One Hot Encoding". It poses the question: "What can be dimensions for all English words?" This suggests that the topic is about representing categorical data in a form that can be input into machine learning models. One hot encoding is a technique used to convert non-numerical data into a format that can be used by statistical algorithms. The presence of logos indicates the lecture is likely associated with the Hochschule Bonn-Rhein and Tim Metzler, who may be the lecturer or a contributor to the course material. [IDE]



One Hot Encoding
What can be dimensions for all English words?
Simplest approach: Using the index of the word

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of "Static_embedding_OneHot_encoding". It seems to be focusing on a method for encoding words into numerical vectors, which is a common technique in NLP to enable machine learning algorithms to process and analyze text data. The slide poses the question of what dimensions can be used for all English words and suggests a simple approach using the index of the word as a one-hot encoding. This is likely part of a larger discussion on how to represent words in a way that computers can understand and use in models. [IDE]



One Hot Encoding
What can be dimensions for all English words?
Simplest approach: Using the index of the word
‚Üí Each word is its own ‚Äúdimension‚Äù

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP). It discusses the concept of "One Hot Encoding," which is a technique used in NLP and machine learning to convert categorical variables into a form that can be provided to machine learning algorithms. The slide poses the question, "What can be dimensions for all English words?" suggesting that it will explore ways to represent English words in a numerical format that can be used for computational purposes. The slide also mentions "Simplest approach: Using the index of the word ‚Üí Each word is its own 'dimension'," indicating that one way to encode words is by assigning them a unique index as their representation. Additionally, there are logos at the bottom, likely representing the institutions or sponsors involved in the lecture: Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik Tim Metzler. [IDE]



One Hot Encoding
Example corpus:
A dog is an animal. A cat is an animal. My dog is 
playful. A cat is playful.

[IDS] The image you're seeing is likely a slide from an NLP (Natural Language Processing) lecture, specifically discussing the concept of "One Hot Encoding." One Hot Encoding is a technique used in machine learning and data preprocessing where categorical variables are converted into a form that can be provided to machine learning algorithms. It's represented here through an example of encoding a simple statement about a dog and a cat, indicating how each category is converted into a binary vector. This is a fundamental concept in preparing text data for models like decision trees or neural networks. [IDE]



One Hot Encoding
Example corpus:
A dog is an animal. A cat is an animal. My dog is 
playful. A cat is playful.
Index (lower case):
0 ‚Üí a
1 ‚Üí an
2 ‚Üí animal
3 ‚Üí cat
4 ‚Üí dog
5 ‚Üí is
6 ‚Üí my
7 ‚Üí playful

[IDS] The image is a visual representation of a concept from NLP, specifically related to "Static_embedding_OneHot_encoding". It showcases a "One Hot Encoding" approach where each word in the corpus is represented as a binary vector. The lecture likely explains how this encoding technique is used to convert categorical variables into numerical form, which is essential for machine learning models that require numerical inputs. The example provided in the image demonstrates the process of converting words into their corresponding one-hot encoded vectors, which can then be used to train and evaluate NLP models. [IDE]



One Hot Encoding
Example corpus:
A dog is an animal. A cat is an animal. My dog is 
playful. A cat is playful.
Index (lower case):
0 ‚Üí a
1 ‚Üí an
2 ‚Üí animal
3 ‚Üí cat
4 ‚Üí dog
5 ‚Üí is
6 ‚Üí my
7 ‚Üí playful
[([1, 0, 0, 0, 0, 0, 0, 0], 'a'),
 ([0, 1, 0, 0, 0, 0, 0, 0], 'an'),
 ([0, 0, 1, 0, 0, 0, 0, 0], 'animal'),
 ([0, 0, 0, 1, 0, 0, 0, 0], 'cat'),
 ([0, 0, 0, 0, 1, 0, 0, 0], 'dog'),
 ([0, 0, 0, 0, 0, 1, 0, 0], 'is'),
 ([0, 0, 0, 0, 0, 0, 1, 0], 'my'),
 ([0, 0, 0, 0, 0, 0, 0, 1], 'playful')]
Embeddings

[IDS] The image displays a slide from an NLP (Natural Language Processing) lecture that focuses on the concept of "One Hot Encoding". It serves as an example to illustrate how categorical variables can be converted into a form suitable for machine learning algorithms. The slide is structured to show the transformation of a simple sentence about a dog and its actions into a numerical representation using one-hot encoding. This process is essential in NLP, as it allows models to interpret and process text data more effectively. [IDE]



One Hot Encoding
Embed a document:
‚ÄùA dog is a playful animal‚Äù 
sum up the word embeddings for each word in the sentence
[([1, 0, 0, 0, 0, 0, 0, 0], 'a'),
 ([0, 1, 0, 0, 0, 0, 0, 0], 'an'),
 ([0, 0, 1, 0, 0, 0, 0, 0], 'animal'),
 ([0, 0, 0, 1, 0, 0, 0, 0], 'cat'),
 ([0, 0, 0, 0, 1, 0, 0, 0], 'dog'),
 ([0, 0, 0, 0, 0, 1, 0, 0], 'is'),
 ([0, 0, 0, 0, 0, 0, 1, 0], 'my'),
 ([0, 0, 0, 0, 0, 0, 0, 1], 'playful')]
Embeddings
[2, 0, 1, 0, 1, 1, 0, 1]

[IDS] In the image, you can see a slide from an NLP lecture that discusses "Static_embedding_OneHot_encoding." It provides an example of how to encode words into vectors using one-hot encoding. The slide illustrates this concept with a simple sentence: "A dog is a playful animal," and shows how each word in the sentence is represented as a vector in a high-dimensional space where all dimensions except for one are zero (the one corresponding to the index of the word in the vocabulary) and one dimension is set to 1. This method is used to convert categorical data into a form that can be inputted into machine learning models. [IDE]
[Lecture End]

[Lecture Start]

------------Static_embedding_TFIDF------------

Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.
Why?
Make embeddings that make it easy to distinguish 
between documents.
Used in information retrieval (find relevant documents to a 
search query)



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.
Two parts:
- tf ‚Üí How often does a term appear in a document



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.
Two parts:
- tf ‚Üí How often does a term appear in a document
- df ‚Üí In how many documents does a term appear



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.
Two parts:
- tf ‚Üí How often does a term appear in a document
- df ‚Üí In how many documents does a term appear
- idf ‚Üí Inverse of df. idf = N/df



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Idea:
Take a corpus (collection of documents), count occurences of 
terms within documents. 
Normalize over documents.
Two parts:
- tf ‚Üí How often does a term appear in a document
- df ‚Üí In how many documents does a term appear
- idf ‚Üí Inverse of df. idf = N/df
Intuition:
A term is important if it appears often in a document.
A term is important if it only appears in a few documents.



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Example corpus:
corpus = [
    "A dog is an animal. A dog is not a cat.",
    "A cat is an animal.",
    "My dog is playful.",
    "I like animals. Linux is not an animal.",
    "Cat is a linux command. Dog is not.",
    "Dog is not a linux command.",
    "My dog likes linux. my dog is playful"
]



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Count occurences of words (terms) in each document
(term frequency tf)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Count in how many documents a term appears 
(document frequency df)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Document 
Frequency
4
3
3
1
3
2
5
1
6
1
1
4
2
4
2



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Take inverse of document frequency by dividing number of documents by df
(idf)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Inverse Document 
Frequency
6/4
6/3
6/3
6/1
6/3
6/2
6/5
6/1
6/6
6/1
6/1
6/4
6/2
6/4
6/2



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Take inverse of document frequency by dividing number of documents by df
(idf)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Inverse Document 
Frequency
1.5
2
2
6
2
3
1.2
6
1
6
6
1.5
3
1.5
3



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Normalize idf with logarithm (log10)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Inverse Document 
Frequency (log)
0.18
0.3
0.3
0.78
0.3
0.48
0.08
0.78
0
0.78
0.78
0.18
0.48
0.18
0.48



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Normalize idf with logarithm (log10)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Inverse Document 
Frequency (log)
0.18
0.3
0.3
0.78
0.3
0.48
0.08
0.78
0
0.78
0.78
0.18
0.48
0.18
0.48



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Normalize idf with logarithm (log10)
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
3
1
1
0
1
0
2
0
2
0
0
0
0
1
0
A cat is an animal.
1
1
1
0
1
0
0
0
1
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
1
0
1
0
0
0
1
0
1
I like animals. Linux is not an 
animal.
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
Cat is a linux command. Dog 
is not.
1
0
0
0
1
1
1
0
2
0
0
1
0
1
0
Dog is not a linux command. 
1
0
0
0
0
1
1
0
1
0
0
1
0
1
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
2
0
1
0
1
1
2
0
1
Inverse Document 
Frequency (log)
0.18
0.3
0.3
0.78
0.3
0.48
0.08
0.78
0
0.78
0.78
0.18
0.48
0.18
0.48
idf of 0 ‚Üí can not be used to distinguish documents. 
Appears in all of them!
idf of 0 ‚Üí can not be used to distinguish documents. 
Appears in all of them!



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Multiply tf with idf 
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
.54
.30
.30
0
.30
0
.16
0
0
0
0
0
0
.18
0
A cat is an animal.
.18
.30
.30
0
.30
0
0
0
0
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
.08
0
0
0
0
0
.48
0
.48
I like animals. Linux is not an 
animal.
0
.30
.30
.78
0
0
0
.78
0
.78
0
.18
0
.18
0
Cat is a linux command. Dog 
is not.
.18
0
0
0
.30
.48
.08
0
0
0
0
.18
0
.18
0
Dog is not a linux command. 
.18
0
0
0
0
.48
.08
0
0
0
0
.18
0
.18
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
.16
0
0
0
.78
.18
.96
0
.48



Term Frequency ‚Äì Inverse Document Frequency (TFIDF)
Document embeddings with words as dimensions
a
an
animal
animals
cat
command
dog
i
is
like
likes
linux
my
not 
playful
A dog is an animal. A dog is 
not a cat.
.54
.30
.30
0
.30
0
.16
0
0
0
0
0
0
.18
0
A cat is an animal.
.18
.30
.30
0
.30
0
0
0
0
0
0
0
0
0
0
My dog is playful.
0
0
0
0
0
0
.08
0
0
0
0
0
.48
0
.48
I like animals. Linux is not an 
animal.
0
.30
.30
.78
0
0
0
.78
0
.78
0
.18
0
.18
0
Cat is a linux command. Dog 
is not.
.18
0
0
0
.30
.48
.08
0
0
0
0
.18
0
.18
0
Dog is not a linux command. 
.18
0
0
0
0
.48
.08
0
0
0
0
.18
0
.18
0
My dog likes linux. My dog is 
playful.
0
0
0
0
0
0
.16
0
0
0
.78
.18
.96
0
.48
[Lecture End]

[Lecture Start]

------------Static_embedding_word2Vec------------

What have we seen so far?
1) One Hot Encoding: 
‚óèRepresentation of words as binary vectors
‚óèSparse
‚óèLack of semantic information
‚óèSize: |V|
2) TFIDF Embedding
‚óèTerm Frequency-Inverse Document Frequency
‚óèReflects importance of word in document relative 
to document collection
‚óèWeighs down frequent terms, scales up rare ones
‚óèStill lacks contextual understanding
‚óèSize: |D| (number of documents)

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of "Static_embedding_word2Vec". It highlights various aspects of Word2Vec, such as one-hot encoding, which uses binary vectors to represent words. It also mentions the term frequency-inverse document frequency (TF-IDF) embedding, reflecting the importance of words in documents and their relative frequency. The slide notes that Word2Vec still lacks contextual understanding and discusses the size of the embedding, mentioning that it's measured in terms of the number of documents. This information suggests that the lecture is focused on understanding how word embeddings, like those created by Word2Vec, can be used to represent words in a way that captures their semantic meaning within a given context. [IDE]



Recap: Similarity and Relatedness
‚óèSimilarity: Likeliness between two words in 
meaning or context. (Can I replace the word in the 
sentence with the other word?)
‚óèRelatedness: How strongly are two words 
associated. (Are the two words likely to appear 
together?)

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on word embeddings using the Word2Vec model. It highlights the concept of "Static_embedding_word2Vec," which refers to the process of creating fixed representations of words in a vector space, where semantically similar words are close to each other. The slide emphasizes the importance of similarity and relatedness in understanding how these word embeddings capture the meaning and context of words. It also mentions that the strength of association between two words can be determined by their proximity in this vector space, with closely associated words appearing together. This concept is fundamental in NLP tasks such as text classification, machine translation, and language modeling. [IDE]



Recap: Similarity and Relatedness
‚óèSimilarity: Likeliness between two words in 
meaning or context. (Can I replace the word in the 
sentence with the other word?)
‚óèRelatedness: How strongly are two words 
associated. (Are the two words likely to appear 
together?)
‚ÄùI like to eat pizza. I like to eat stew‚Äù 
‚Üí pizza and stew are similar in this context
‚ÄùPizza is baked‚Äù
‚Üí  pizza and baked are related since they often 
appear together
There is no clear distinction between these two!

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Static_embedding_word2Vec." This slide aims to explain the concept of similarity and relatedness in the context of word embeddings, which are a method for representing words as vectors in a high-dimensional space. The slide emphasizes the importance of understanding how closely related two words are by discussing likeliness and relatedness. It uses examples like "pizza" and "stew" to illustrate that although these words are associated, they are not strongly related, whereas words like "pizza" and "baked" are more strongly related since they often appear together. This is likely part of a larger discussion on how word embeddings can be used to capture semantic relationships between words, which is a fundamental aspect of NLP tasks such as text classification, sentiment analysis, and machine translation. [IDE]



Distributional Structure of Language
Consider the following sentence:
Berlin is the capital of Germany.
What does this tell us about Berlin?
Obvious: Berlin is the capital of Germany.

[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of word embeddings. The title "Static_embedding_word2Vec" suggests that the lecture is discussing static word embeddings, which are a type of representation for words in a vector space, and it mentions Word2Vec as a particular algorithm used to create these embeddings. The bullet points provide further context, stating that Berlin is the capital of Germany and asking the audience to consider what this tells us about Berlin, with the answer being that Berlin is also the capital of Germany. This seems to be an example or exercise related to understanding the context and semantic relationships between words in NLP. [IDE]



Distributional Structure of Language
Consider the following sentence:
Walala is the capital of Lampukistan.
What does this tell us about Walala?

[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of word embeddings using the Word2Vec algorithm. The title "Distributional Structure of Language" suggests that the lecture is discussing how words in a language are used together, which is a foundational concept in understanding how word embeddings work. [IDE]



Distributional Structure of Language
Consider the following sentence:
Walala is the capital of Lampukistan.
What does this tell us about Walala?
‚Üí Walala is similar to words that appear in the
    context of ‚Äúis the capital of‚Äù.
‚Üí Walala is a city.

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image is likely discussing the topic of "Static_embedding_word2Vec," which refers to a method for converting words into fixed-length vectors in a high-dimensional space. These vectors capture semantic relationships between words and can be used for various NLP tasks such as text classification, clustering, or language translation. The slide might be introducing this concept, explaining how word embeddings work, or discussing their applications in NLP. [IDE]



Distributional Structure of Language
Words that appear in a similar context are similar.
How do we learn embeddings that capture these 
semantic relationships?

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Static_embedding_word2Vec". The title of the slide is "Distributional Structure of Language", which suggests that the lecture might be discussing how words appear in similar contexts and how we can learn embeddings that capture these semantic relationships. The slide also includes logos and names of institutions or individuals, such as Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler, indicating their involvement in the lecture or the course. [IDE]



Distributional Structure of Language
Words that appear in a similar context are similar.
How do we learn embeddings that capture these 
semantic relationships?
Requirements:
‚óèFixed vector size
‚óèSimilar words should have similar representations in 
the vector space
‚óèGeneral vectors, not optimized for a specific domain
‚óèEasy to learn
‚óèCan learn from vast amounts of data (e.g. Wikipedia, 
Common Crawl, etc)

[IDS] In this NLP lecture, we are exploring the concept of static embeddings using Word2Vec. This involves learning vector representations for words that capture their semantic relationships in a similar context. The requirements for such a model include fixed vector size and similar representations in the vector space for semantically similar words. General vectors should not be optimized for a specific domain, making it easy to learn and transferable across different domains like Wikipedia, Common Crawl, or others. [IDE]



Distributional Structure of Language
We need a learning task that will produce these 
vectors.
Idea: Train a simple classifier to predict word from 
context or predict context from word.

[IDS] The image you're seeing is likely a slide from a Natural Language Processing (NLP) lecture, specifically focusing on the topic of word embeddings using the Word2Vec algorithm. The title "Distributional Structure of Language" suggests that the lecture will discuss how language can be represented in a mathematical form based on its distributional properties. The main idea presented is the need for a learning task that produces vectors to capture the semantic relationships between words. To achieve this, the lecture proposes training a simple classifier to predict a word from its context or predict the context from a word, which is essentially what Word2Vec does. The institutions mentioned at the bottom, Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, indicate that the lecture might be part of a course offered by these institutions. The name Tim Metzler could be the instructor or author of the lecture material. [IDE]



Word2Vec
Developed by Mikolov et al, Google (2013)
‚ÄúEfficient Estimation of Word Representations in 
Vector Space‚Äù 
(https://arxiv.org/abs/1301.3781)
‚ÄúDistributed Representations of Words and 
Phrases and their Compositionality‚Äù in Advances 
in Neural Information Processing Systems 
(doi:10.48550/arXiv.1310.4546)
Idea: Train a simple classifier to predict word from 
context or predict context from word.

[IDS] The image displays a slide from a Natural Language Processing (NLP) lecture that focuses on the topic of static embeddings using the Word2Vec algorithm. The slide introduces the concept of "Efficient Estimation of Word Representations in Vector Space" developed by Mikolov et al., as presented in their 2013 paper published in Google. It highlights the significance of distributed representations of words and phrases, which are advanced in neural information processing systems. The slide also emphasizes the idea of training a simple classifier to predict word context or context from word, which is an essential aspect of understanding how Word2Vec works in NLP tasks. [IDE]



Word2Vec
Example Sentence:

[IDS] The image displays a slide from a lecture on Word2Vec, a popular technique in Natural Language Processing (NLP) for learning vector representations of words. The slide presents an example sentence to illustrate the concept of static embedding in Word2Vec. Static embedding refers to the process of mapping each word in a vocabulary to a fixed-length vector, which captures its semantic and syntactic properties. In this case, the example sentence is "The quick brown fox jumps over the lazy dog." This sentence is often used as a test phrase in NLP tasks because it contains all the letters of the English alphabet. By analyzing such sentences, researchers can evaluate how well Word2Vec captures the meaning of individual words and their relationships with one another. [IDE]



Word2Vec
Example Sentence:

[IDS] The image is a visual representation of how Word2Vec, a popular technique in Natural Language Processing (NLP), embeds words into vectors. It illustrates the concept of static embeddings, which are fixed-length vectors that represent words in a high-dimensional space. In this context, each word is assigned a unique vector, and words with similar meanings or usage patterns are placed closer together in this space. The example sentence provided is "The quick brown fox jumps over the lazy dog," which is used to demonstrate how words can be mapped to their corresponding vectors in a 2-dimensional space. This visualization helps NLP practitioners understand how Word2Vec captures semantic relationships between words by positioning them in proximity based on their semantic similarities. [IDE]



Word2Vec
Continuous Bag-Of-Word Model
Predict center word from context

[IDS] The image depicts a Word2Vec model, which is a technique for creating word embeddings in Natural Language Processing (NLP). It's designed to map words into a vector space where semantically similar words are closer together. In this visualization, we see the continuous bag-of-words (CBOW) architecture of Word2Vec, where the goal is to predict the target word based on its context. The input layer consists of a classifier that processes the context words and predicts the target word. This model is commonly used in NLP tasks like language translation, sentiment analysis, and text generation. [IDE]



Word2Vec
Skip Gram Model
Predict context from center word.
We will use this approach for our 
examples!

[IDS] The image is a visual representation of Word2Vec, a popular technique in Natural Language Processing (NLP) that maps words to vectors of real numbers. In this context, we are discussing the Skip Gram Model, which is a method used by Word2Vec to learn vector representations of words. The model predicts the context words surrounding a target word, and these predictions are used to train a classifier. The classifier then learns to associate certain properties with words, such as 'brown' being associated with 'dog', 'fox', and 'jumps'. This approach allows for the creation of word embeddings, which are dense vectors that capture semantic relationships between words. The lecture likely covers how these embeddings can be utilized in various NLP tasks, such as text classification or language translation. [IDE]



Word2Vec ‚Äì Creating Training Examples
Create training examples from each sentence in the corpus.
Training examples are of the form (word1, word2).
Our context never crosses sentence boundaries!

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of Word2Vec. Word2Vec is a technique for converting words into vectors, which are numerical representations that capture semantic and syntactic features of the words. The slide provides an example of how to create training examples for Word2Vec by using sentences from a corpus. It explains that training examples are formed by taking individual words (word1, word2) from a sentence. This process helps in understanding the context and relationships between words, which is crucial for NLP tasks like language modeling, text classification, and machine translation. The slide also includes logos of the Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, indicating the institutions involved or associated with the lecture. [IDE]



Word2Vec ‚Äì Creating Training Examples

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide focused on teaching how to create training examples for Word2Vec, which is a method for converting text into vector space. The slide is likely part of a presentation aimed at explaining the concept of static embeddings in Word2Vec. The diagram illustrates the process of taking a sentence, splitting it into individual words, and then creating pairs of words to be used as training examples. These examples are essential for the Word2Vec algorithm to learn how to represent each word as a vector in the vector space. The lecture aims to educate the audience on the mechanics behind this technique and its importance in NLP tasks such as text classification, language translation, and semantic analysis. [IDE]



Word2Vec ‚Äì Creating Training Examples

[IDS] The image represents a slide from a lecture on Natural Language Processing (NLP), specifically focusing on Word2Vec, which is a method for converting text into vectors of real numbers that capture the semantic meaning of words. The slide aims to provide training examples and context to help understand how Word2Vec works in creating embeddings. The embedding process is visualized through arrows connecting words like "quick", "brown", "fox", and "dog" to their corresponding vectors, indicating how each word is represented in the vector space. This representation allows machines to understand the relationships between words, such as synonymy or antonymy, and can be used in various NLP tasks like language translation, sentiment analysis, and text classification. [IDE]



Word2Vec ‚Äì Creating Training Examples

[IDS] The image illustrates a concept from NLP (Natural Language Processing) known as Word2Vec, specifically focusing on static embeddings. The diagram provides an example of how words are converted into vectors that capture their semantic meaning. In this context, the word "brown" is used to show how it can be represented as a vector in a higher-dimensional space, where similar words like "quick" and "fox" might also have similar representations. This technique helps computers understand the relationships between words and is a crucial step in various NLP tasks such as language translation, text classification, and sentiment analysis. [IDE]



Word2Vec ‚Äì Creating Training Examples

[IDS] The image represents a concept from the lecture on static embeddings in Word2Vec, which is a technique used in natural language processing (NLP) to map words into vectors of real numbers. These vectors capture semantic and syntactic features of the words and are used for various NLP tasks such as text classification, machine translation, and question-answering systems. The diagram shows how the model learns to represent similar words in close proximity in the vector space, enabling word analogies and semantic searches. The context provided by the surrounding words is crucial for understanding the meaning of each word in isolation, highlighting the importance of word embeddings in capturing the nuances of language. [IDE]



Word2Vec ‚Äì Skip Gram Architecture

[IDS] The image is a visual representation of the Word2Vec algorithm's skip-gram architecture, which is a type of neural network model used in natural language processing (NLP). In this context, we are learning about how static embeddings work within the Word2Vec framework. The diagram illustrates the process of training the model using an example input "fox" and its corresponding output "jumps." The architecture consists of an input layer, a hidden layer (W1), and an output layer (W2). The goal of this model is to learn vector representations of words that capture their semantic meaning, allowing for better performance in tasks such as language translation or text classification. [IDE]



Word2Vec ‚Äì Skip Gram Architecture
y=softmax (xW1W 2)

[IDS] The image is a visual representation of the skip-gram architecture used in the Word2Vec model, which is a technique for representing words in a high-dimensional vector space. This architecture is designed to predict the context words given a target word, with the goal of learning embeddings that capture semantic relationships between words. The skip-gram model depicted here uses two layers: an input layer and an output layer, connected by a matrix W1, which represents the learned embeddings. During training, the model aims to predict the surrounding words (jumps) given the current word (fox), using the softmax function to compute the probabilities. This process helps in capturing the semantic meaning of words and their relationships in the context of natural language processing. [IDE]



Word2Vec ‚Äì Extracting the Embeddings

[IDS] The image illustrates a concept from Word2Vec, a popular algorithm in Natural Language Processing (NLP) that assigns numerical vectors to words in a way that captures their semantic meaning. The figure represents the process of extracting embeddings, which are the numerical representations of words or phrases. In this case, it's showing how individual word embeddings are derived from a larger context, such as a sentence or paragraph. This is an essential step in NLP tasks like sentiment analysis, language translation, and text classification, where machine learning algorithms can process and analyze these numerical representations more efficiently than raw text data. [IDE]



Word2Vec ‚Äì Extracting the Embeddings

[IDS] The image represents a visual aid from an NLP lecture, specifically focusing on the concept of "Static_embedding_word2Vec". It illustrates the process of extracting embeddings from word vectors using the Word2Vec algorithm. The slide shows a bar chart with a highlighted blue section labeled 'W1', which signifies a specific embedding for the word 'word 4'. This is likely a demonstration of how words are represented as vectors in a high-dimensional space and how certain dimensions (or features) of these vectors can be extracted to capture the meaning or context of the word within the language model. [IDE]



Word2Vec ‚Äì Skip Gram Architecture
y=softmax (xW1W 2)
Recap Softmax:
Softmax turns vector into probability distribution s.t. it sums to 1.
softmax(z)j=
e
z j
‚àëk=1
K
e
zk

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the Word2Vec algorithm, which is a method for converting words into vectors of real numbers in a high-dimensional space. The slide explains that Word2Vec uses two architectures: Continuous Bag of Words (CBOW) and Skip-Gram. The focus here is on the Skip-Gram architecture, which is used to learn vector representations of words by predicting the missing word in a context window. The slide also mentions the Softmax function and its role in converting the vector into a probability distribution. This is part of the NLP topic "Static_embedding_word2Vec," which likely covers various aspects of how static embeddings are created using Word2Vec. [IDE]



Word2Vec ‚Äì Skip Gram Architecture
y=softmax (xW1W 2)
Recap Softmax:
Softmax turns vector into probability distribution s.t. it sums to 1.
Problem: 
Vector z has vocabulary size (10,000; 100,000; 1,000,000?)
‚Üí Computationally expensive!!!
softmax(z)j=
e
z j
‚àëk=1
K
e
zk

[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the Word2Vec algorithm. This algorithm is used for creating vector representations of words, which helps in understanding the semantic relationships between them. The slide explains how the softmax function is applied to transform the probability distribution into a vector, which is then used in the skip-gram architecture of Word2Vec. The problem highlighted on the slide is related to the computational cost and memory requirements of using a vocabulary size of 100,000 words or more in this context. [IDE]



Word2Vec - Negative Sampling Skip Gram Model

[IDS] The image is a diagram of the Word2Vec model, which is a popular technique in Natural Language Processing (NLP) for creating word embeddings. In this context, we are discussing static embeddings, meaning that the embeddings are not updated during training and remain fixed. The diagram shows two main components: the input layer and the output layer. The input layer takes in words as vectors, represented by circles with the word and its corresponding index. These words are then processed through a window of size 1, indicating that only one word at a time is being considered. This processed word is then passed through a dot product operation to produce an output vector. The output layer shows two dimensions, W1 and W2, which represent the learned weight matrices for the skip-gram model, a type of Word2Vec architecture that predicts the surrounding words given a target word. The entire process is depicted in a simple flowchart format, typical of NLP lectures to help students understand how word embeddings are created using the Word2Vec algorithm. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
y=œÉ (x1W 1W 2
T x2
T)

[IDS] The image depicts a visualization of the Word2Vec model's Negative Sampling Skip Gram Model, which is a concept in Natural Language Processing (NLP). This model is used to learn word embeddings by predicting whether two words co-occur in a given context. The diagram illustrates the architecture of this model with input layers for words and their context, as well as output layers that determine if the predicted word and the target word are likely to appear together in a sentence. The lecture titled "Static_embedding_word2Vec" likely covers the basics of this model and its applications in NLP tasks such as language translation and text classification. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
Previous approach vs this approach:
Time for one forward pass in a notebook:
Previous: 
~150 ms for vocabulary size of 1,000,000!
This approach:
~30 ¬µs for vocabulary size of 1,000,000!
~5000 times faster!

[IDS] The image displays a comparison between two approaches to word embeddings, specifically focusing on the Word2Vec model. The first approach is the traditional method which involves one forward pass in a notebook and takes approximately 150 milliseconds for a vocabulary size of 1,000,000. The second approach, referred to as "Negative Sampling Skip Gram Model," significantly reduces the time required, taking around 30 microseconds for the same vocabulary size. This suggests that the Negative Sampling Skip Gram Model is much faster, reducing the time by about 5000 times compared to the traditional method. As we are in an NLP lecture, this information is particularly relevant as it highlights advancements in natural language processing techniques and the importance of efficient computational methods in handling large datasets. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
This is not the full truth!
So far we have only created positive examples where:
(word1, word2) ‚Üí 1
Without negative examples our classifier could always predict 1 
and achieve a 100% accuracy.

[IDS] The image displays a slide from an NLP lecture, specifically focusing on the Word2Vec algorithm and its application in creating negative sampling skip gram models. The content suggests that the current model is not complete as it only contains positive examples (word1, word2) without any negative examples. This lack of negative examples hinders the model's ability to predict accurately, as evidenced by the claim that even with 100% accuracy on positive examples, the classifier would still predict 1 without considering negative examples. The slide seems to be emphasizing the importance of including negative examples in the training data to improve the performance of the Word2Vec model. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
Loss function for a batch of N examples (p is output, t is target label):
Single positive example:
Single negative example:
L=‚àí1
N [‚àëj=1
N
[t j log( p j)+(1‚àít j)log(1‚àíp j)]]
L=‚àílog( p j)
L=‚àílog(1‚àíp j)

[IDS] The image is a slide from an NLP lecture, specifically discussing the Word2Vec model and its Negative Sampling Skip Gram Model. The slide outlines the loss function used for a batch of N examples where the output is a target label. It explains the concept of a single positive example and a single negative example in the context of this model. The slide also includes logos of the Hochschule Bonn-Rhein-Sieg and Tim Metzler, presumably the instructor or author of the lecture. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
How do we create negative examples?
For one positive example, do we need to create (|V| - 1) negative 
examples?

[IDS] The image displays a slide from an NLP lecture, specifically discussing the Word2Vec model and negative sampling skip gram model. It poses the question of how to create negative examples for this model, suggesting that they are essential for training the model. The slide also includes logos indicating the involvement of Hochschule Bonn-Rhein-Sieg and Tim Metzler, likely the presenter or instructor of the lecture. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
How do we create negative examples?
For one positive example, do we need to create (|V| - 1) negative 
examples?
No! Experiments show 5-20 negative examples per positive example are 
enough.

[IDS] The image is a slide from an NLP lecture, specifically focusing on the Word2Vec model and its application in creating negative examples for training. The title "Word2Vec - Negative Sampling Skip Gram Model" suggests that the slide is discussing a method for generating negative examples using Word2Vec, which is a word embedding algorithm used to convert words or phrases into vectors of real numbers that capture their semantic meaning. The content of the slide indicates that the audience is being asked to think about how they would create negative examples, with a specific example given (IV - 1 negative examples) and a statement suggesting that experiments show a range of negative examples per positive example are necessary. This implies that understanding the concept of negative sampling is crucial for fine-tuning the performance of the Word2Vec model. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
How do we create negative examples?
We sample according to frequency of word.

[IDS] The image displays a graph titled "Word2Vec - Negative Sampling Skip Gram Model," which is a technique used in natural language processing (NLP) to learn word embeddings. The graph illustrates how negative examples are created and sampled according to the frequency of words. This is a fundamental concept in NLP, particularly in the context of static embeddings like Word2Vec, where the goal is to represent words as vectors in a high-dimensional space such that semantically similar words are close together. The graph's x-axis represents the token index, and the y-axis shows the frequency of words. The lecture is likely discussing how this model helps in understanding the relationships between words and improving the performance of NLP tasks such as language translation or sentiment analysis. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
How do we create negative examples?
We sample according to frequency of word.

[IDS] The image displays a visualization of negative sampling skip gram model used in the Word2Vec algorithm, which is a popular technique for creating word embeddings in Natural Language Processing (NLP). This model aims to capture the semantic relationships between words by predicting the surrounding words in a given context. The graph shows the frequency distribution of tokens, where the x-axis represents the token index and the y-axis represents the frequency(index) of the tokens. The two lines represent the actual frequency distribution and the negative sampling distribution, indicating how the model adjusts the frequency of words to better capture their relationships. [IDE]



Word2Vec - Negative Sampling Skip Gram Model
How do we create negative examples?
We sample according to frequency of word.
f
3
4

[IDS] The image is a visual representation of how negative examples are used to create a skip gram model in the Word2Vec algorithm, which is a technique used in Natural Language Processing (NLP) to improve word embeddings. The left graph shows the frequency distribution of words, where higher frequencies indicate more common words. The right graph demonstrates the negative sampling probability, which is a method of creating synthetic negative examples by sampling from a noise distribution, like a normal distribution with mean 0 and standard deviation 1. This process helps the model learn more about the context of words and their relationships. In NLP, such models are crucial for tasks like language translation, sentiment analysis, and text classification. [IDE]
[Lecture End]

[Lecture Start]

------------Static_embeding_Vector_semeantic_embeddings------------
What do words mean?
N-gram or text classification methods we've seen so far
‚ó¶Words are just strings (or indices wi in a vocabulary list)
‚ó¶That's not very satisfactory!
Introductory logic classes:
‚ó¶The meaning of "dog" is DOG;  cat is CAT
‚àÄx DOG(x) ‚ü∂MAMMAL(x)
Old linguistics joke by Barbara Partee in 1967:
‚ó¶Q: What's the meaning of life?
‚ó¶A: LIFE


Desiderata
What should a theory of word meaning do for us?
Let's look at some desiderata
From lexical semantics, the linguistic study of word 
meaning


mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls 
a cursor... 
Lemmas and senses
sense
lemma
Modified from the online thesaurus WordNet


Relations between senses: Synonymy
Synonyms have the same meaning in some or all 
contexts.
‚ó¶filbert / hazelnut
‚ó¶couch / sofa
‚ó¶big / large
‚ó¶automobile / car
‚ó¶vomit / throw up
‚ó¶water / H20


Relations between senses: Synonymy
Note that there are probably no examples of perfect 
synonymy.
‚ó¶Even if many aspects of meaning are identical
‚ó¶Still may differ based on politeness, slang, register, genre, 
etc.


Relation: Synonymy?
water/H20
"H20" in a surfing guide?
big/large
my big sister != my large sister


The Linguistic Principle of Contrast
Difference in form √† difference in meaning


Abb√© Gabriel Girard 1718
 [I do not believe that there 
is a synonymous word in any 
language]
"
"
Re: "exact" synonyms

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image serves as an illustration of how language can be represented and analyzed. The text from the 1718 book by Abb√© Gabriel Girard is an example of the early stages of linguistic research, focusing on the precise meanings of words in different languages. This historical perspective is crucial for understanding how NLP has evolved over time. In modern NLP, techniques like static embedding and vector semantics play a significant role in capturing the essence of words and their relationships within a language. These methods involve representing words as vectors in a high-dimensional space where semantically similar words are closer together. This enables NLP systems to understand word meanings and use them effectively in tasks such as language translation, sentiment analysis, and text generation. The image of the old book serves as a reminder of the rich history and foundational work that has led to the development of these advanced NLP techniques. [IDE]


Relation: Similarity
Words with similar meanings.  Not synonyms, but sharing 
some element of meaning
car, bicycle
cow, horse


Ask humans how similar 2 words are
word1
word2
similarity
vanish
disappear
9.8 
behave
obey
7.3 
belief
impression 
5.95 
muscle
bone 
3.65 
modest
flexible
0.98 
hole
agreement
0.3 
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word1</th>
      <th>word2</th>
      <th>similarity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>vanish</td>
      <td>disappear</td>
      <td>9.8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>behave</td>
      <td>obey</td>
      <td>7.3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>belief</td>
      <td>impression</td>
      <td>5.95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>muscle</td>
      <td>bone</td>
      <td>3.65</td>
    </tr>
    <tr>
      <th>4</th>
      <td>modest</td>
      <td>flexible</td>
      <td>0.98</td>
    </tr>
    <tr>
      <th>5</th>
      <td>hole</td>
      <td>agreement</td>
      <td>0.3</td>
    </tr>
  </tbody>
</table>

Relation: Word relatedness
Also called "word association"
Words can be related in any way, perhaps via a semantic 
frame or field
‚ó¶coffee, tea:    similar
‚ó¶coffee, cup:   related, not similar


Semantic field
Words that 
‚ó¶cover a particular semantic domain 
‚ó¶bear structured relations with each other. 
hospitals
surgeon, scalpel, nurse, anaesthetic, hospital
restaurants
waiter, menu, plate, food, menu, chef
houses


Relation: Antonymy
Senses that are opposites with respect to only one 
feature of meaning
Otherwise, they are very similar!
dark/light   short/long fast/slow
rise/fall
hot/cold
up/down
in/out
More formally: antonyms can
‚ó¶define a binary opposition or be at opposite ends of a scale
‚ó¶
long/short, fast/slow


Connotation (sentiment)
‚Ä¢ Words have affective meanings
‚Ä¢
Positive connotations (happy) 
‚Ä¢
Negative connotations (sad)
‚Ä¢ Connotations can be subtle:
‚Ä¢
Positive connotation: copy, replica, reproduction 
‚Ä¢
Negative connotation: fake, knockoff, forgery
‚Ä¢ Evaluation (sentiment!)


Connotation
Words seem to vary along 3 affective dimensions:
‚ó¶valence: the pleasantness of the stimulus
‚ó¶arousal: the intensity of emotion provoked by the stimulus
‚ó¶dominance: the degree of control exerted by the stimulus
Osgood et al. (1957)
Word
Score
Word
Score
Valence
love
1.000
toxic
0.008
happy
1.000
nightmare
0.005
Arousal
elated
0.960
mellow
0.069
frenzy
0.965
napping
0.046
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>1-Word</th>
      <th>2-Score</th>
      <th>Col3</th>
      <th>4-Word</th>
      <th>5-Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Valence</td>
      <td>love</td>
      <td>1.000</td>
      <td></td>
      <td>toxic</td>
      <td>0.008</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>happy</td>
      <td>1.000</td>
      <td></td>
      <td>nightmare</td>
      <td>0.005</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Arousal</td>
      <td>elated</td>
      <td>0.960</td>
      <td></td>
      <td>mellow</td>
      <td>0.069</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>frenzy</td>
      <td>0.965</td>
      <td></td>
      <td>napping</td>
      <td>0.046</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dominance</td>
      <td>powerful</td>
      <td>0.991</td>
      <td></td>
      <td>weak</td>
      <td>0.045</td>
    </tr>
    <tr>
      <th>5</th>
      <td>None</td>
      <td>leadership</td>
      <td>0.983</td>
      <td></td>
      <td>empty</td>
      <td>0.081</td>
    </tr>
  </tbody>
</table>

So far
Concepts or word senses
‚ó¶Have a complex many-to-many association with words (homonymy, 
multiple senses)
Have relations with each other
‚ó¶Synonymy
‚ó¶Antonymy
‚ó¶Similarity
‚ó¶Relatedness
‚ó¶Connotation


Vector 
Semantics & 
Embeddings
Word Meaning


Vector 
Semantics & 
Embeddings
Vector Semantics


Computational models of word meaning
Can we build a theory of how to represent word 
meaning, that accounts for at least some of the 
desiderata?
We'll introduce vector semantics
The standard model in language processing!
Handles many of our goals!


Ludwig Wittgenstein
PI #43: 
"The meaning of a word is its use in the language"


Let's define words by their usages
One way to define "usage": 
words are defined by their environments (the words around them)
Zellig Harris (1954): 
If A and B have almost identical environments we say that they 
are synonyms.


What does recent English borrowing ongchoi mean?
Suppose you see these sentences:
‚Ä¢ Ong choi is delicious saut√©ed with garlic. 
‚Ä¢ Ong choi is superb over rice
‚Ä¢ Ong choi leaves with salty sauces
And you've also seen these:
‚Ä¢ ‚Ä¶spinach saut√©ed with garlic over rice
‚Ä¢ Chard stems and leaves are delicious
‚Ä¢ Collard greens and other salty leafy greens
Conclusion:


Ongchoi: Ipomoea aquatica "Water Spinach"
!"#
kangkong
rau mu·ªëng
‚Ä¶

[IDS] The image you provided is related to a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Static_embedding_Vector_semeantic_embeddings." The text in the image seems to be discussing a concept or term from this field. NLP is a subfield of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. In the context of static embeddings, it refers to a technique where words or phrases are represented as vectors in a high-dimensional space, capturing their semantic meaning.

The term "Vector" in this context likely refers to mathematical vectors used to represent words or phrases in a way that captures their meaning and relationships. These vectors can be used for tasks such as text classification, sentiment analysis, and machine translation.

The image itself serves as a visual representation of the concept being discussed in the lecture. It's a photograph of a bunch of green leaves, which could symbolize growth, nature, or freshness ‚Äì concepts often associated with NLP's goal of understanding and generating human language in a meaningful way.

As a helpful assistant, I would explain that static embeddings are an essential part of NLP because they allow machines to understand the meaning behind words and phrases by mapping them to numerical vectors. This process enables various NLP tasks, including text classification, where the machine can determine the category of a piece of text based on its content.

In summary, the image is a visual aid for a lecture on NLP, specifically discussing static embeddings and vector-based semantic embeddings. These concepts are crucial for teaching machines to understand and interpret human language effectively. [IDE]


Idea 1: Defining meaning by linguistic distribution
Let's define the meaning of a word by its 
distribution in language use, meaning its 
neighboring words or grammatical environments. 


Idea 2: Meaning as a point in space (Osgood et al. 1957)
3 affective dimensions for a word
‚ó¶valence: pleasantness 
‚ó¶arousal: intensity of emotion 
‚ó¶dominance: the degree of control exerted
Word
Score
Word
Score
Valence
love
1.000
toxic
0.008
happy
1.000
nightmare
0.005
Arousal
elated
0.960
mellow
0.069
frenzy
0.965
napping
0.046
Dominance
powerful
0.991
weak
0.045
NRC VAD Lexicon 
 (Mohammad 2018)
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>1-Word</th>
      <th>2-Score</th>
      <th>Col3</th>
      <th>4-Word</th>
      <th>5-Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Valence</td>
      <td>love</td>
      <td>1.000</td>
      <td></td>
      <td>toxic</td>
      <td>0.008</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>happy</td>
      <td>1.000</td>
      <td></td>
      <td>nightmare</td>
      <td>0.005</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Arousal</td>
      <td>elated</td>
      <td>0.960</td>
      <td></td>
      <td>mellow</td>
      <td>0.069</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>frenzy</td>
      <td>0.965</td>
      <td></td>
      <td>napping</td>
      <td>0.046</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dominance\n‚ó¶</td>
      <td>powerful</td>
      <td>0.991</td>
      <td></td>
      <td>weak</td>
      <td>0.045</td>
    </tr>
    <tr>
      <th>5</th>
      <td>None</td>
      <td>leadership</td>
      <td>0.983</td>
      <td></td>
      <td>empty</td>
      <td>0.081</td>
    </tr>
  </tbody>
</table>

Idea 1: Defining meaning by linguistic distribution
Idea 2: Meaning as a point in multidimensional space


ER 6
‚Ä¢
VECTOR SEMANTICS AND EMBEDDINGS
bad
worst
not good
dislike
worse
incredibly bad
now
you
i
that
with
by
to
‚Äôs
are
is
a
than
Defining meaning as a point in space based on distribution
Each word = a vector   (not just "good" or "w45")
Similar words are "nearby in semantic space"
We build this space automatically by seeing which words are 
nearby in text
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>not good
bad
to by dislike
worst
‚Äôs
incredibly bad
that now
are worse
a i
you
than
with
is
incredibly good
very good
amazing fantastic
wonderful
terrific
nice
good</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

We define meaning of a word as a vector
Called an "embedding" because it's embedded into a 
space (see textbook)
The standard way to represent meaning in NLP
Every modern NLP algorithm uses embeddings as 
the representation of word meaning
Fine-grained model of meaning for similarity 


Intuition: why vectors?
Consider sentiment analysis:
‚ó¶With words,  a feature is a word identity
‚ó¶Feature 5: 'The previous word was "terrible"'
‚ó¶requires exact same word to be in training and test
‚ó¶With embeddings: 
‚ó¶Feature is a word vector
‚ó¶'The previous word was vector [35,22,17‚Ä¶]
‚ó¶Now in the test set we might see a similar vector [34,21,14]


We'll discuss 2 kinds of embeddings
tf-idf
‚ó¶Information Retrieval workhorse!
‚ó¶A common baseline model
‚ó¶Sparse vectors
‚ó¶Words are represented by (a simple function of) the counts of nearby 
words
Word2vec
‚ó¶Dense vectors
‚ó¶Representation is created by training a classifier to predict whether a 


From now on:
Computing with meaning representations
instead of string representations
R
Vector Semantics and
Embeddings
C‚áß@√Ç(|Ôºå√≥|ËÄå√øC Nets are for fish;
Once you get the fish, you can forget the net.
Ë®Ä‚áß@√Ç(‚úèÔºå√≥‚úèËÄå√øË®ÄWords are for meaning;
Once you get the meaning, you can forget the words
√ëP(Zhuangzi), Chapter 26

[IDS] The image is likely a slide from a lecture on Natural Language Processing (NLP). It discusses the transition from computing with meaning representations to using string representations, emphasizing the importance of understanding both the words and their meanings. The slide also touches upon the distinction between Nets for fish, which are easily forgotten, and Words for meaning, which can be more persistent in memory. This suggests that the lecture is focused on how semantic embeddings, like static and embedded vectors, play a crucial role in capturing and retaining the meaning of words in computational models. [IDE]


Vector 
Semantics & 
Embeddings
Vector Semantics


Vector 
Semantics & 
Embeddings
Words and Vectors


Term-document matrix
6.3
‚Ä¢
WORDS AND VECTORS
7
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.2
The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
just so they fit on the page; in real term-document matrices, the vectors representing
each document would have dimensionality |V|, the vocabulary size.
The ordering of the numbers in a vector space indicates different meaningful di-
mensions on which documents vary. Thus the first dimension for both these vectors
corresponds to the number of times the word battle occurs, and we can compare
each dimension, noting for example that the vectors for As You Like It and Twelfth
Night have similar values (1 and 0, respectively) for the first dimension.
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.3
The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.
Each document is represented by a vector of words
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV\nbbaattttllee 11 00 77 1133\nggoooodd 111144 8800 6622 8899\nffooooll 3366 5588 11 44\nwwiitt 2200 1155 22 33\nFFiigguurree 66..32 TThhee tteerrmm--ddooccuummeenntt mmaattrriixx ffoorr ffoouurr wwoorrddss iinn ffoouurr S Shhaakkeessppeeaarree p plalayyss. .E Tachh\nbcooxnetasi nshs otwhe tnhuatm ebaecrh o dfo tcimumese ntht ei s( rroewpr)e wseonrtde do cacsu ar sc oinlu tmhen (vceoclutomr no)f dleoncgutmh efnotu.r.\nreprWesee nctaend tahsi nak c oofu ntht ev evcetcotro, ra f coor lau mdonc iunm Feingt. a6s.3 a. point in V -dimensional sp\n| |\nthusT toh er edvoiceuwm seonmtse i nb Fasigic. 6li.n3e aarre a plogienbtrsa i,n a 4 v-deicmtoern sisio, naat lh sepaarct,e .j uSsitn ac el i4s-t doirm aernrsa\nsnpuamcebse rasr.e Shoa rAds t oY ovuis Luiakleiz Iet, iFsi rge. p6r.e4s eshnotewds a as vthiseu lailsitz [a1ti,o1n1 4in,3 t6w,2o0 d] i(mtheen sfiirosnt sc;o wl</td>
      <td>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>bbaattttllee 11 00 77 1133\nggoooodd 111144 8800 6622 8899\nffooooll 3366 5588 11 44\nwwiitt 2200 1155 22 33</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>FFiigguurree 66..32</td>
      <td>None</td>
    </tr>
  </tbody>
</table>

Visualizing document vectors
thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensio
spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we‚Äôv
arbitrarily chosen the dimensions corresponding to the words battle and fool.
5
10
15
20
25
30
5
10
Henry V [4,13]
As You Like It [36,1]
Julius Caesar [1,7]
battle
Twelfth Night [58,0]
15
40
35
40
45
50
55
60


Vectors are the basis of information retrieval
mensions on which documents vary. Thus the first dimension for both these vectors
corresponds to the number of times the word battle occurs, and we can compare
each dimension, noting for example that the vectors for As You Like It and Twelfth
Night have similar values (1 and 0, respectively) for the first dimension.
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.3
The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.
We can think of the vector for a document as a point in |V|-dimensional space;
thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional
spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we‚Äôve
Vectors are similar for the two comedies
But comedies are different than the other two 
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>As You Like It Twelfth Night Julius Caesar Henry V</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>As You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. Th\nboxes show that each document is represented as a column vector of length four.\nVectors are similar for the two comedies\nWe can think of the vector for a document as a point in V -dimensional sp\n| |\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimens\nBut comedies are different than the other two\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; w\narbitrarily chosen the dimensions corresponding to the words battle and fool.\nComedies have more fools and wit and fewer battles.\n40</td>
      <td>As You Like It Twelfth Night Julius Caesar Henry V</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>battle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>Figure 6.3</td>
      <td>None</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>40</td>
      <td></td>
    </tr>
  </tbody>
</table>

Idea for word meaning: Words can be vectors too!!!
6.3
‚Ä¢
WORDS AND VECTORS
7
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.2
The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array of
tor
battle is "the kind of word that occurs in Julius Caesar and Henry V"
fool is "the kind of word that occurs  in comedies, especially Twelfth Night"
 row vector
vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions
of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word
counts in the same four dimensions are used to form the vectors for the other 3
words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89].
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.5
The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each word is represented as a row vector of length four.
For documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. This same principle applies to words:
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>As You Like It Twelfth Night Julius Caesar Henry V
As You Like It Twelfth Night Julius Caesar Henry V</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>bbaattttllee 11 00 77 1133\nggoooodd 111144 8800 6622 8899\nffooooll 3366 5588 11 44\nwwiitt 2200 1155 22 33</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td></td>
      <td>FFiigguurree 66..52</td>
      <td>None</td>
    </tr>
  </tbody>
</table>

More common: word-word matrix
(or "term-context matrix")
Two words are similar in meaning if their context vectors are similar
 ||‚á•||
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. The context could be the document, in which case the cell
represents the number of times the two words appear in the same document. It is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
the cell represents the number of times (in some training corpus) the column word
occurs in such a ¬±4 word window around the row word. For example here is one
example each of some words in their windows:
is traditionally followed by cherry
pie, a traditional dessert
often mixed, such as strawberry
rhubarb pie. Apple pie
computer peripherals and personal digital
assistants. These devices usually
a computer. This includes information available on the internet
If we then take every occurrence of each word (say strawberry) and count the con-
text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a
simplified subset of the word-word co-occurrence matrix for these four words com-
dow around the word, for example of 4 words to the left and 4 words to the right,
in which case the cell represents the number of times (in some training corpus) the
column word occurs in such a ¬±4 word window around the row word. For example
here is one example each of some words in their windows:
is traditionally followed by cherry
pie, a traditional dessert
often mixed, such as strawberry
rhubarb pie. Apple pie
computer peripherals and personal digital
assistants. These devices usually
a computer. This includes information available on the internet
If we then take every occurrence of each word (say strawberry) and count the
context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a
simplified subset of the word-word co-occurrence matrix for these four words com-
puted from the Wikipedia corpus (Davies, 2015).
aardvark
...
computer
data
result
pie
sugar
...
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows
aardvark ... computer data result pie sugar ...</th>
      <th>Col1</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>simplified subset of the word-word co-occurrence matrix for these four words co\ncherry 0 ... 2 8 9 442 25 ...\nputed from the Wikipedia corpus (Davies, 2015).\nstrawberry 0 ... 0 0 1 60 19 ...\nNote in Fig. 6.5 that the two words cherry and strawberry are more similar\ndigital 0 ... 1670 1683 85 5 4 ...\neach other (both pie and sugar tend to occur in their window) than they are to oth\ninformation 0 ... 3325 3982 378 5 13 ...</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>None</td>
      <td>ig. 6.5 that the two words cherry and strawberry are more similar\n0 ... 1670 1683 85 5 4 ...\noth pie and sugar tend to occur in their window) than they are to oth\n0 ... 3325 3982 378 5 13 ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wor</td>
      <td>ds like di</td>
      <td>None</td>
    </tr>
  </tbody>
</table>

1000 2000 3000 4000
1000
2000
digital
 [1683,1670]
computer
information
 [3982,3325] 
3000
4000


Vector 
Semantics & 
Embeddings
Words and Vectors


Vector 
Semantics & 
Embeddings
Cosine for computing word similarity


Computing word similarity: Dot product and cosine
The dot product between two vectors is a scalar:
The dot product tends to be high when the two 
vectors have large values in the same dimensions
Dot product can thus be a useful similarity metric 
between vectors
hence of length |V|, or both with documents as dimensions as documents, of length
|D|) and gives a measure of their similarity. By far the most common similarity
metric is the cosine of the angle between the vectors.
The cosine‚Äîlike most measures for vector similarity used in NLP‚Äîis based on
the dot product operator from linear algebra, also called the inner product:
dot product(v,w) = v¬∑w =
N
X
i=1
viwi = v1w1 +v2w2 +...+vNwN
(6.7)
As we will see, most metrics for similarity between vectors are based on the dot
product. The dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. Alternatively,
vectors that have zeros in different dimensions‚Äîorthogonal vectors‚Äîwill have a


Problem with raw dot-product
Dot product favors long vectors
Dot product is higher if a vector is longer (has higher 
values in many dimension)
Vector length:
Frequent words (of, the, you) have long vectors (since 
 will see, most metrics for similarity between vectors are based on the dot
ct. The dot product acts as a similarity metric because it will tend to be high
hen the two vectors have large values in the same dimensions. Alternatively,
s that have zeros in different dimensions‚Äîorthogonal vectors‚Äîwill have a
oduct of 0, representing their strong dissimilarity.
is raw dot product, however, has a problem as a similarity metric: it favors
ectors. The vector length is defined as
|v| =
v
u
u
t
N
X
i=1
v2
i
(6.8)
ot product is higher if a vector is longer, with higher values in each dimension.


Alternative: cosine for computing word similarity
|~
a||
b|
The cosine similarity metric between two vectors~
v and ~
w thus can be computed
s:
cosine(~
v,~
w) = ~
v¬∑~
w
|~
v||~
w| =
N
X
i=1
viwi
v
u
u
t
N
X
i=1
v2
i
v
u
u
t
N
X
i=1
w2
i
(6.10)
For some applications we pre-normalize each vector, by dividing it by its length,
reating a unit vector of length 1. Thus we could compute a unit vector from ~
a by
ividing it by |~
a|. For unit vectors, the dot product is the same as the cosine.
This raw dot product, however, has a problem as a similarity metric: it favors
long vectors. The vector length is defined as
ector length
|v| =
v
u
u
t
N
X
i=1
v2
i
(6.8)
The dot product is higher if a vector is longer, with higher values in each dimension.
More frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. The raw dot product
thus will be higher for frequent words. But this is a problem; we‚Äôd like a similarity
metric that tells us how similar two words are regardless of their frequency.
We modify the dot product to normalize for the vector length by dividing the
dot product by the lengths of each of the two vectors. This normalized dot product
turns out to be the same as the cosine of the angle between the two vectors, following
from the definition of the dot product between two vectors a and b:
Based on the definition of the dot product between two vectors a and b 


Cosine as a similarity metric
-1: vectors point in opposite directions 
+1:  vectors point in same directions
0: vectors are orthogonal
But since raw frequency values are non-negative, the 

[IDS] The image is a slide from an NLP lecture that focuses on the concept of cosine as a similarity metric. It explains how cosine can be used to measure the similarity between vectors in a high-dimensional space. The slide provides two key points: 1) vectors pointing in opposite directions have a negative cosine value, and 2) vectors pointing in the same direction have a positive cosine value. It also mentions that since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0 to 1. This information is crucial for understanding how cosine can be applied in natural language processing tasks such as text classification or document clustering, where measuring the similarity between documents or terms is essential. [IDE]


Cosine examples
pie
data
computer
cherry
442
8
2
digital
5
1683
1670
information 5
3982
3325
cos(ÔÅ≤
v, ÔÅ≤
w) =
ÔÅ≤
v ‚Ä¢ ÔÅ≤
w
ÔÅ≤
v ÔÅ≤
w =
ÔÅ≤
v
ÔÅ≤
v ‚Ä¢
ÔÅ≤
w
ÔÅ≤
w =
viwi
i=1
N
‚àë
vi
2
i=1
N
‚àë
wi
2
i=1
N
‚àë
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction
But raw frequency values are non-negative, so the cosine for these vectors rang
from 0‚Äì1.
Let‚Äôs see how the cosine computes which of the words cherry or digital is clos
in meaning to information, just using raw counts from the following shortened tabl
pie
data computer
cherry
442
8
2
digital
5
1683
1670
information
5
3982
3325
cos(cherry,information) =
442‚á§5+8‚á§3982+2‚á§3325
p
4422 +82 +22p
52 +39822 +33252 = .017
cos(digital,information) =
5‚á§5+1683‚á§3982+1670‚á§3325
p
52 16832 16702p
52 39822 33252 = .996
from 0‚Äì1.
Let‚Äôs see how the cosine computes which of the words cherry or digital is c
in meaning to information, just using raw counts from the following shortened t
pie
data computer
cherry
442
8
2
digital
5
1683
1670
information
5
3982
3325
cos(cherry,information) =
442‚á§5+8‚á§3982+2‚á§3325
p
4422 +82 +22p
52 +39822 +33252 = .017
cos(digital,information) =
5‚á§5+1683‚á§3982+1670‚á§3325
p
52 +16832 +16702p
52 +39822 +33252 = .
The model decides that information is way closer to digital than it is to cher
t raw frequency values are non-negative, so the cosine for these vectors ranges
m 0‚Äì1.
Let‚Äôs see how the cosine computes which of the words cherry or digital is closer
meaning to information, just using raw counts from the following shortened table:
pie
data computer
cherry
442
8
2
digital
5
1683
1670
information
5
3982
3325
os(cherry,information) =
442‚á§5+8‚á§3982+2‚á§3325
p
4422 +82 +22p
52 +39822 +33252 = .017
os(digital,information) =
5‚á§5+1683‚á§3982+1670‚á§3325
p
  p
   = .996
cosine value ranges from 1 for vectors pointing in the same direction, through
ctors that are orthogonal, to -1 for vectors pointing in opposite directions.
 frequency values are non-negative, so the cosine for these vectors ranges
1.
 see how the cosine computes which of the words cherry or digital is closer
ng to information, just using raw counts from the following shortened table:
pie
data computer
cherry
442
8
2
digital
5
1683
1670
information
5
3982
3325
erry,information) =
442‚á§5+8‚á§3982+2‚á§3325
p
4422 +82 +22p
52 +39822 +33252 = .017
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>Col1</th>
      <th>Col2</th>
      <th>Col3</th>
      <th>ollowi
igital
data</th>
      <th>ng shortene
is closer
computer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>None</td>
      <td>None</td>
      <td>counts from\nords cherr\n42 8</td>
      <td>the f\ny or d\npi2e</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td></td>
      <td>horten\n8</td>
      <td>ed table:\n2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>None</td>
      <td>computer\n5 the1 f6o8l3low\ncherry\ne data com</td>
      <td>i1n6g7 0s\n442\nputer</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>4</th>
      <td>w</td>
      <td>N N\nch‚àëerry2inf‚àëor4m42a2tion8\nv w</td>
      <td></td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>5</th>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td></td>
      <td>1683</td>
      <td>1670</td>
    </tr>
    <tr>
      <th>6</th>
      <td>None</td>
      <td>None</td>
      <td>5 39282\n2digit8al</td>
      <td>3325\n25</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>7</th>
      <td>None</td>
      <td>None</td>
      <td>puter</td>
      <td></td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <th>8</th>
      <td>None</td>
      <td>None</td>
      <td>1670\n1683 1\ninformation\n2\n2 53+3285 39</td>
      <td>670\n5\n82 +</td>
      <td>3982\n2 332</td>
      <td>3325\n5</td>
    </tr>
  </tbody>
</table>

Visualizing cosines 
(well, angles)
‚Ä¢
VECTOR SEMANTICS AND EMBEDDINGS
500
1000
1500
2000
2500
3000
500
digital
cherry
information
Dimension 1: ‚Äòpie‚Äô


Vector 
Semantics & 
Embeddings
Cosine for computing word 
similarity


Vector 
Semantics & 
Embeddings
TF-IDF


But raw frequency is a bad representation
‚Ä¢ The co-occurrence matrices we have seen represent each 
cell by word frequencies.
‚Ä¢ Frequency is clearly useful; if sugar appears a lot near 
apricot, that's useful information.
‚Ä¢ But overly frequent words like the, it, or they are not very 
informative about the context
‚Ä¢ It's a paradox! How can we balance these two conflicting 
constraints? 


Two common solutions for word weighting
tf-idf:     tf-idf value for word t in document d:
PMI: (Pointwise mutual information)
‚ó¶PMI ùíòùüè, ùíòùüê= ùíçùíêùíà
ùíë(ùíòùüè,ùíòùüê)
ùíëùíòùüèùíë(ùíòùüê)
fool
36
0.012
good
37
0
sweet
37
0
eighting of the value for word t in document d, wt,d thus combines
with idf:
wt,d = tft,d ‚á•idft
(6.13)
f-idf weighting to the Shakespeare term-document matrix in Fig. 6.2.
idf values for the dimension corresponding to the word good have
 0; since this word appears in every document, the tf-idf algorithm
ored in any comparison of the plays. Similarly, the word fool, which
t of the 37 plays, has a much lower weight.
Words like "the" or "it" have very low idf
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fool</th>
      <th>36</th>
      <th>0.012</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>good</td>
      <td>37</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>commo\nsweet</td>
      <td>n so\n37</td>
      <td>lutio\n0</td>
    </tr>
  </tbody>
</table>

Term frequency (tf) in the tf-idf algorithm
We could imagine using raw count:
tft,d = count(t,d)
But instead of using raw count, we usually squash a bit:
tft,d = count(t,d)
e commonly we squash the raw frequency a bit, by using the l
ncy instead. The intuition is that a word appearing 100 times
sn‚Äôt make that word 100 times more likely to be relevant to the 
ument. We also need to do something special with counts of 0,
 the log of 0.2
tft,d =
(
1+log10 count(t,d)
if count(t,d) > 0


Document frequency (df)
dft is the number of documents t occurs in.
(note this is not collection frequency: total count across 
all documents)
"Romeo" is very distinctive for one Shakespeare play:
for discriminating those documents from the rest of the collection; terms that occur
frequently across the entire collection aren‚Äôt as helpful. The document frequency
nt
y
dft of a term t is the number of documents it occurs in. Document frequency is
not the same as the collection frequency of a term, which is the total number of
times the word appears in the whole collection in any document. Consider in the
collection of Shakespeare‚Äôs 37 plays the two words Romeo and action. The words
have identical collection frequencies (they both occur 113 times in all the plays) but
very different document frequencies, since Romeo only occurs in a single play. If
our goal is to find documents about the romantic tribulations of Romeo, the word
Romeo should be highly weighted, but not action:
Collection Frequency Document Frequency
Romeo 113
1
action
113
31
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>Collection Frequency</th>
      <th>Document Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Romeo</td>
      <td>113</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>action</td>
      <td>113</td>
      <td>31</td>
    </tr>
  </tbody>
</table>

Inverse document frequency (idf)
occur in a few like salad or Falstaff, to those which are very 
common as to be completely non-discriminative since they o
good or sweet.3
Word
df
idf
Romeo
1
1.57
salad
2
1.27
Falstaff
4
0.967
forest
12
0.489
battle
21
0.246
wit
34
0.037
fool
36
0.012
umber of documents in many collections, this measure
th a log function. The resulting definition for inverse
 thus
idft = log10
‚úìN
dft
‚óÜ
(6.13)
or some words in the Shakespeare corpus, ranging from
s which occur in only one play like Romeo, to those that
N is the total number of documents 
in the collection
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Word</th>
      <th>df</th>
      <th>idf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Romeo</td>
      <td>1</td>
      <td>1.57</td>
    </tr>
    <tr>
      <th>1</th>
      <td>salad</td>
      <td>2</td>
      <td>1.27</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Falstaff</td>
      <td>4</td>
      <td>(6.\n0.967</td>
    </tr>
    <tr>
      <th>3</th>
      <td>forest</td>
      <td>12</td>
      <td>0.489</td>
    </tr>
    <tr>
      <th>4</th>
      <td>battle</td>
      <td>21</td>
      <td>0.246</td>
    </tr>
    <tr>
      <th>5</th>
      <td>e corpus,\nwit</td>
      <td>rang\n34</td>
      <td>ing fr\n0.037</td>
    </tr>
    <tr>
      <th>6</th>
      <td>fool\nke Romeo,</td>
      <td>36\nto t</td>
      <td>0.012\nhose t</td>
    </tr>
    <tr>
      <th>7</th>
      <td>good</td>
      <td>37</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>common l\nsweet</td>
      <td>ike f\n37</td>
      <td>ool or\n0</td>
    </tr>
  </tbody>
</table>

What is a document?
Could be a play or a Wikipedia article
But for the purposes of tf-idf, documents can be 
anything; we often call each paragraph a document!


Final tf-idf weighted value for a word
Raw counts:
tf-idf:
(defined either by Eq. 6.11 or by Eq. 6.12) with id
wt,d = tft,d ‚á•idft
 tf-idf weighting to the Shakespeare term-documen
uation Eq. 6.12. Note that the tf-idf values for the
 word good have now all become 0; since this wor
f-idf algorithm leads it to be ignored. Similarly, th
6.3
‚Ä¢
WORDS AND VECTORS
7
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.2
The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array of
CHAPTER 6
‚Ä¢
VECTOR SEMANTICS AND EMBEDDINGS
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
0.246
0
0.454
0.520


Vector 
Semantics & 
Embeddings
TF-IDF


Vector 
Semantics & 
Embeddings
Word2vec


Sparse versus dense vectors
tf-idf (or PMI) vectors are
‚ó¶long (length |V|= 20,000 to 50,000)
‚ó¶sparse (most elements are zero)
Alternative: learn vectors which are
‚ó¶short (length 50-1000)
‚ó¶dense (most elements are non-zero)


Sparse versus dense vectors
Why dense vectors?
‚ó¶Short vectors may be easier to use as features in machine 
learning (fewer weights to tune)
‚ó¶Dense vectors may generalize better than explicit counts
‚ó¶Dense vectors may do better at capturing synonymy:
‚ó¶car and automobile are synonyms; but are distinct dimensions
‚ó¶a word with car as a neighbor and a word with automobile as a 
neighbor should be similar, but aren't


Common methods for getting short dense vectors
‚ÄúNeural Language Model‚Äù-inspired models
‚ó¶Word2vec (skipgram, CBOW), GloVe
Singular Value Decomposition (SVD)
‚ó¶A special case of this is called LSA ‚Äì Latent Semantic 
Analysis
Alternative to these "static embeddings":
‚Ä¢
Contextual Embeddings (ELMo, BERT)
‚Ä¢
Compute distinct embeddings for a word in its context


Simple static embeddings you can download!
Word2vec (Mikolov et al)
https://code.google.com/archive/p/word2vec/
GloVe (Pennington, Socher, Manning)
http://nlp.stanford.edu/projects/glove/


Word2vec
Popular embedding method
Very fast to train
Code available on the web
Idea: predict rather than count
Word2vec provides various options. We'll do:
skip-gram with negative sampling (SGNS)


Word2vec
Instead of counting how often each word w occurs near "apricot"
‚ó¶Train a classifier on a binary prediction task:
‚ó¶Is w likely to show up near "apricot"?
We don‚Äôt actually care about this task
‚ó¶But we'll take the learned classifier weights as the word embeddings
Big idea:  self-supervision: 
‚ó¶A word c that occurs near apricot in the corpus cats as the gold "correct 
answer" for supervised learning


Approach: predict if candidate word c is a "neighbor"
1. Treat the target word t and a neighboring context word c
as positive examples.
2. Randomly sample other words in the lexicon to get 
negative examples
3. Use logistic regression to train a classifier to distinguish 
those two cases
4. Use the learned weights as the embeddings


Skip-Gram Training Data
Assume a +/- 2 word window, given training sentence:
‚Ä¶lemon, a [tablespoon of  apricot  jam,   a]  pinch‚Ä¶
c1                   c2 
c3      c4
 
                                [target]


Skip-Gram Classifier
(assuming a +/- 2 word window)
‚Ä¶lemon, a [tablespoon of  apricot  jam,   a]  pinch‚Ä¶
c1                   c2 [target]
c3      c4
Goal: train a classifier that is given a candidate (word, context) pair
(apricot, jam)
(apricot, aardvark)
‚Ä¶
And assigns each pair a probability:
P(+|w, c) 


Similarity is computed from dot product
Remember: two vectors are similar if they have a high 
dot product
‚ó¶Cosine is just a normalized dot product
So:
‚ó¶Similarity(w,c)  ‚àùw ¬∑ c
We‚Äôll need to normalize to get a probability 
‚ó¶(cosine isn't a probability either)


Turning dot products into probabilities
Sim(w,c) ‚âà w ¬∑ c
To turn this into a probability 
We'll use the sigmoid from logistic regression:
6.8
‚Ä¢
WORD2VEC
model the probability that word c is a real context word for target word w 
P(+|w,c) = s(c¬∑w) =
1
1+exp(‚àíc¬∑w)
igmoid function returns a number between 0 and 1, but to make it a proba
odel the probability that word c is a real context word for target word w
P(+|w,c) = s(c¬∑w) =
1
1+exp(‚àíc¬∑w)
gmoid function returns a number between 0 and 1, but to make it a prob
also need the total probability of the two possible events (c is a context
isn‚Äôt a context word) to sum to 1. We thus estimate the probability that 
a real context word for w as:
P(‚àí|w,c) = 1‚àíP(+|w,c)


How Skip-Gram Classifier computes P(+|w, c) 
This is for one context word, but we have lots of context words.
We'll assume independence and just multiply them:
6.8
‚Ä¢
WORD2VEC
19
We model the probability that word c is a real context word for target word w as:
P(+|w,c) = s(c¬∑w) =
1
1+exp(‚àíc¬∑w)
(6.28)
The sigmoid function returns a number between 0 and 1, but to make it a probability
we‚Äôll also need the total probability of the two possible events (c is a context word,
and c isn‚Äôt a context word) to sum to 1. We thus estimate the probability that word c
is not a real context word for w as:
P(‚àí|w,c) = 1‚àíP(+|w,c)
= s(‚àíc¬∑w) =
1
1+exp(c¬∑w)
(6.29)
Equation 6.28 gives us the probability for one word, but there are many context
P(‚àí|w,c) = 1‚àíP(+|w,c)
= s(‚àíc¬∑w) =
1
1+exp(c¬∑w)
(6.29)
uation 6.28 gives us the probability for one word, but there are many context
rds in the window. Skip-gram makes the simplifying assumption that all context
rds are independent, allowing us to just multiply their probabilities:
P(+|w,c1:L) =
L
Y
i=1
s(ci ¬∑w)
(6.30)
L
X
s


Skip-gram classifier: summary
A probabilistic classifier, given 
‚Ä¢
a test target word w 
‚Ä¢
its context window of L words c1:L
Estimates probability that w occurs in this window based 
on similarity of w (embeddings) to c1:L (embeddings).
To compute this, we just need embeddings for all the 


These embeddings we'll need: a set for w, a set for c
1
W
C
aardvark
zebra
aardvark
apricot
apricot
|V|
|V|+1
& =
target words
context & noise
‚Ä¶
1..d
‚Ä¶


Vector 
Semantics & 
Embeddings
Word2vec


Vector 
Semantics & 
Embeddings
Word2vec: Learning the 
embeddings


Skip-Gram Training data
‚Ä¶lemon, a [tablespoon of  apricot  jam,   a]  pinch‚Ä¶
c1                   c2 [target]
c3      c4
6.8.2
Learning skip-gram embeddings
Word2vec learns embeddings by starting with an initial set of embedding vecto
and then iteratively shifting the embedding of each word w to be more like the em
beddings of words that occur nearby in texts, and less like the embeddings of word
that don‚Äôt occur nearby. Let‚Äôs start by considering a single piece of training data:
... lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
This example has a target word t (apricot), and 4 context words in the L = ¬±
window, resulting in 4 positive training instances (on the left below):
positive examples +
t
c
apricot tablespoon
negative examples -
t
c
t
c
apricot aardvark apricot seven


Skip-Gram Training data
‚Ä¶lemon, a [tablespoon of  apricot  jam,   a]  pinch‚Ä¶
c1                   c2 [target]
c3      c4
For each positive 
example we'll grab k 
negative examples, 
6.8.2
Learning skip-gram embeddings
Word2vec learns embeddings by starting with an initial set of embedding vecto
and then iteratively shifting the embedding of each word w to be more like the em
beddings of words that occur nearby in texts, and less like the embeddings of word
that don‚Äôt occur nearby. Let‚Äôs start by considering a single piece of training data:
... lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
This example has a target word t (apricot), and 4 context words in the L = ¬±
window, resulting in 4 positive training instances (on the left below):
positive examples +
t
c
apricot tablespoon
negative examples -
t
c
t
c
apricot aardvark apricot seven


Skip-Gram Training data
‚Ä¶lemon, a [tablespoon of  apricot  jam,   a]  pinch‚Ä¶
c1                   c2 [target]
c3      c4
6.8.2
Learning skip-gram embeddings
Word2vec learns embeddings by starting with an initial set of embedding vecto
and then iteratively shifting the embedding of each word w to be more like the em
beddings of words that occur nearby in texts, and less like the embeddings of word
that don‚Äôt occur nearby. Let‚Äôs start by considering a single piece of training data:
... lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
This example has a target word t (apricot), and 4 context words in the L = ¬±
window, resulting in 4 positive training instances (on the left below):
positive examples +
t
c
apricot tablespoon
negative examples -
t
c
t
c
apricot aardvark apricot seven
Word2vec learns embeddings by starting with an initial set of embedding vectors
nd then iteratively shifting the embedding of each word w to be more like the em-
eddings of words that occur nearby in texts, and less like the embeddings of words
hat don‚Äôt occur nearby. Let‚Äôs start by considering a single piece of training data:
.. lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
This example has a target word t (apricot), and 4 context words in the L = ¬±2
window, resulting in 4 positive training instances (on the left below):
positive examples +
t
c
apricot tablespoon
negative examples -
t
c
t
c
apricot aardvark apricot seven


Word2vec: how to learn vectors
Given the set of positive and negative training instances, 
and an initial set of embedding vectors 
The goal of learning is to adjust those word vectors such 
that we:
‚ó¶Maximize the similarity of the target word, context word pairs 
(w , cpos) drawn from the positive data
‚ó¶Minimize the similarity of the (w , cneg) pairs drawn from the 
. 


Loss function for one w with cpos , cneg1 ...cnegk
Maximize the similarity of the target with the actual context words, 
and minimize the similarity of the target with the k negative sampled 
non-neighbor words. 
‚Ä¢ Minimize the similarity of the (w,cneg) pairs from the negative examples.
If we consider one word/context pair (w,cpos) with its k noise words cneg1...cnegk,
we can express these two goals as the following loss function L to be minimized
(hence the ‚àí); here the first term expresses that we want the classifier to assign the
real context word cpos a high probability of being a neighbor, and the second term
expresses that we want to assign each of the noise words cnegi a high probability of
being a non-neighbor, all multiplied because we assume independence:
LCE = ‚àílog
"
P(+|w,cpos)
k
Y
i=1
P(‚àí|w,cnegi)
#
= ‚àí
"
logP(+|w,cpos)+
k
X
i=1
logP(‚àí|w,cnegi)
#
= ‚àí
"
logP(+|w,cpos)+
k
X
log
ÔøΩ
1‚àíP(+|w,cnegi)
ÔøΩ
#


Learning the classifier
How to learn?
‚ó¶Stochastic gradient descent!
We‚Äôll adjust the word weights to
‚ó¶make the positive pairs more likely 
‚ó¶and the negative pairs less likely, 
‚ó¶over the entire training set.


Intuition of one step of gradient descent
W
C
move apricot and jam closer,
increasing cpos ÔøΩ w
aardvark
move apricot and matrix apart
decreasing cneg1 ÔøΩ w
‚Äú‚Ä¶apricot jam‚Ä¶‚Äù
w
zebra
aardvark
jam
apricot
cpos
matrix
!
c
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>Col1</th>
      <th>Col2</th>
      <th>Col3</th>
      <th>Col4</th>
      <th>Col5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>apricot</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>w</td>
    </tr>
    <tr>
      <th>1</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
    <tr>
      <th>3</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>jam</td>
      <td></td>
      <td>c\npos</td>
    </tr>
    <tr>
      <th>4</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
    <tr>
      <th>5</th>
      <td>None</td>
      <td>None</td>
      <td>matrix</td>
      <td>None</td>
      <td></td>
      <td>c\nneg1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
    <tr>
      <th>7</th>
      <td>None</td>
      <td>Tolstoy</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>c\nneg2</td>
    </tr>
    <tr>
      <th>8</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td></td>
      <td>None</td>
    </tr>
  </tbody>
</table>

Reminder: gradient descent
‚Ä¢ At each step
‚Ä¢ Direction: We move in the reverse direction from the 
gradient of the loss function
‚Ä¢ Magnitude: we move the value of this gradient 
!
!" ùêø(ùëìùë•; ùë§, ùë¶) weighted by a learning rate Œ∑ 
‚Ä¢ Higher learning rate means move w faster
GISTIC REGRESSION
  d

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing gradient descent in the context of static embedding and vector semantic embeddings. The slide emphasizes the importance of understanding how gradients influence the learning process, particularly in relation to the loss function's direction and magnitude. It also highlights the role of a learning rate and its impact on the speed of convergence during the learning process. This concept is crucial for NLP tasks that involve training models with embeddings, as it helps in optimizing the model's parameters for better performance in tasks such as language modeling or sentiment analysis. [IDE]


The derivatives of the loss function
‚Ä¢
VECTOR SEMANTICS AND EMBEDDINGS
oof as an exercise at the end of the chapter):
‚àÇLCE
‚àÇcpos
= [s(cpos ¬∑w)‚àí1]w
‚àÇLCE
‚àÇcneg
= [s(cneg ¬∑w)]w
‚àÇL
k
X
 context word cpos a high probability of being a neighbor, and the second ter
resses that we want to assign each of the noise words cnegi a high probability o
ng a non-neighbor, all multiplied because we assume independence:
LCE = ‚àílog
"
P(+|w,cpos)
k
Y
i=1
P(‚àí|w,cnegi)
#
= ‚àí
"
logP(+|w,cpos)+
k
X
i=1
logP(‚àí|w,cnegi)
#
= ‚àí
"
logP(+|w,cpos)+
k
X
i=1
log
ÔøΩ
1‚àíP(+|w,cnegi)
ÔøΩ
#
"
#
= ‚àí
"
logP(+|w,cpos)+
k
X
i=1
log
ÔøΩ
1‚àíP(+|w,cnegi)
ÔøΩ
#
= ‚àí
"
logs(cpos ¬∑w)+
k
X
i=1
logs(‚àícnegi ¬∑w)
#
(6.34
 is, we want to maximize the dot product of the word with the actual contex
s, and minimize the dot products of the word with the k negative sampled non
hbor words.
We minimize this loss function using stochastic gradient descent.
Fig. 6.1
s the intuition of one step of learning.
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col0</th>
      <th>[s
g s (</th>
      <th>Col2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>apricot</td>
      <td></td>
      <td>w</td>
    </tr>
  </tbody>
</table>

Update equation in SGD
‚àÇLCE
‚àÇw
= [s(cpos ¬∑w)‚àí1]cpos +
X
i=1
[s(cnegi ¬∑w)]cnegi
The update equations going from time step t to t + 1 in stochastic gradient de
are thus:
ct+1
pos
= ct
pos ‚àíh[s(ct
pos ¬∑wt)‚àí1]wt
ct+1
neg = ct
neg ‚àíh[s(ct
neg ¬∑wt)]wt
wt+1 = wt ‚àíh
"
[s(cpos ¬∑wt)‚àí1]cpos +
k
X
i=1
[s(cnegi ¬∑wt)]cnegi
#
Start with randomly initialized C and W matrices, then incrementally do updates


Two sets of embeddings
SGNS learns two sets of embeddings
Target embeddings matrix W
Context embedding matrix C 
It's common to just add them together, 
representing word i as the vector  wi + ci


Summary: How to learn word2vec (skip-gram) 
embeddings
Start with V random d-dimensional vectors as initial 
embeddings
Train a classifier based on embedding similarity
‚ó¶Take a corpus and take pairs of words that co-occur as positive 
examples
‚ó¶Take pairs of words that don't co-occur as negative examples
‚ó¶Train the classifier to distinguish these by slowly adjusting all 


Vector 
Semantics & 
Embeddings
Word2vec: Learning the 
embeddings


Vector 
Semantics & 
Embeddings
Properties of Embeddings


The kinds of neighbors depend on window size
Small windows (C= +/- 2) : nearest words are syntactically 
similar words in same taxonomy
‚ó¶Hogwarts nearest neighbors are other fictional schools
‚ó¶Sunnydale, Evernight, Blandings
Large windows (C= +/- 5) :  nearest words are related 
words in same semantic field
‚ó¶Hogwarts nearest neighbors are Harry Potter world:


Analogy/Relational Similarity:
Another semantic property of embeddings is their
ability to capture relational meanings. In an important early vector space model of
cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model
for solving simple analogy problems of the form a is to b as a* is to what?. In such
problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as
grape is to
, and must fill in the word vine. In the parallelogram model, illus-
trated in Fig. 6.15, the vector from the word apple to the word tree (= #       ¬ª
apple‚àí#   ¬ª
tree)
is added to the vector for grape (#        ¬ª
grape); the nearest word to that point is returned.
tree
apple
Analogical relations
The classic parallelogram model of analogical reasoning 
(Rumelhart and Abrahamson 1973)
To solve: "apple is to tree as grape is to  _____"
Add tree ‚Äì apple  to grape to get vine


Analogical relations via parallelogram
The parallelogram method can solve analogies with 
both sparse and dense embeddings (Turney and 
Littman 2005, Mikolov et al. 2013b)
king ‚Äì man + woman is close to queen
Paris ‚Äì France + Italy is close to Rome
For a problem a:a*::b:b*, the parallelogram method is:
meaning could solve such analogy problems (Turney and Littman
arallelogram method received more modern attention because of
word2vec or GloVe vectors (Mikolov et al. 2013b, Levy and Gold
ngton et al. 2014). For example, the result of the expression (#    
kin
¬ª
n is a vector close to #         ¬ª
queen. Similarly, #      ¬ª
Paris ‚àí#           ¬ª
France + #     ¬ª
Italy) 
 that is close to #         ¬ª
Rome. The embedding model thus seems to be ex
ations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or
TIVE/SUPERLATIVE, as shown in Fig. 6.16 from GloVe.
or a a:b::a*:b* problem, meaning the algorithm is given a, b, and 
*, the parallelogram method is thus:


Structure in GloVE Embedding space

[IDS] The image depicts a visualization of word embeddings, which are vectors that represent words in a high-dimensional space. In this context, the lecture is likely discussing static embeddings, which are fixed representations of words and do not change over time. The GloVe (Global Vectors for Word Representation) embedding model is known for its ability to capture semantic relationships between words. This plot shows how words with similar meanings or usage patterns are closer together in the vector space, such as 'niece' and 'aunt', indicating their relatedness in the context of family relationships. The x-axis and y-axis represent the dimensions of the vector space where each word is plotted, showing the distribution of these words within the semantic space. [IDE]


Caveats with the parallelogram method
It only seems to work for frequent words, small 
distances and certain relations (relating countries to 
capitals, or parts of speech), but not others. (Linzen
2016, Gladkova et al. 2016, Ethayarajh et al. 2019a) 
Understanding analogy is an open area of research 
(Peterson et al. 2020)


Train embeddings on different decades of historical text to see meanings shift
~30 million books, 1850-1990, Google Books data
Embeddings as a window onto historical semantics

[IDS] The image represents a visual explanation of how word embeddings, specifically static and diachronic embeddings, reveal semantic changes in language over time. The left part of the image illustrates static embeddings by showing words like "gay" and "gays" with arrows pointing to their meanings. The right part of the image demonstrates diachronic embeddings, which track changes in meaning over time. The example given is the word "awful," which has shifted from a positive to a negative connotation between 1850 and 1990. This kind of analysis helps in understanding how language evolves and how certain words can change their meaning based on societal shifts and historical context. In an NLP lecture, this would be an important topic as it showcases how computational models can capture and analyze linguistic changes to improve natural language processing tasks. [IDE]


Embeddings reflect cultural bias!
Ask ‚ÄúParis : France :: Tokyo : x‚Äù 
‚ó¶x = Japan
Ask ‚Äúfather : doctor :: mother : x‚Äù 
‚ó¶x = nurse
Ask ‚Äúman : computer programmer :: woman : x‚Äù 
‚ó¶x = homemaker
Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. "Man is to computer 
programmer as woman is to homemaker? debiasing word embeddings." In NeurIPS, pp. 4349-4357. 2016.


Historical embedding as a tool to study cultural biases
‚Ä¢ Compute a gender or ethnic bias for each adjective: e.g., how 
much closer the adjective is to "woman" synonyms than 
"man" synonyms, or names of particular ethnicities
‚Ä¢ Embeddings for competence adjective (smart, wise, 
brilliant, resourceful, thoughtful, logical) are biased toward 
men, a bias slowly decreasing 1960-1990
‚Ä¢ Embeddings for dehumanizing adjectives (barbaric, 
monstrous, bizarre)  were biased toward Asians in the 
Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. 
Proceedings of the National Academy of Sciences 115(16), E3635‚ÄìE3644.
[Lecture End]

[Lecture Start]

------------Feedforward_neural_anguage_models------------
Plan (Preliminary)
Date
Lec. Topics
2024-04-09
1
Org, Introduction to NLP, Text Processing
2024-04-16
2
Text Processing and Tokenization
2024-04-23
3
Statistical Language Models
2024-04-30
4
Static Embeddings (1-Hot, TF-IDF)
2024-05-07
5
Static Embeddings (Word2Vec)
2024-05-14
6
Feedforward & Recurrent Neural Language Models
2024-05-21
7
Attention and the Transformer Architecture
2024-05-28
8
Transformers and Applications
2024-06-04
9
Generative Pretrained Transformers and Large Language Models
2024-06-11
10 Instruction-Following Language Models
2024-06-18
11 Model Adaptation, Retrieval Augmented Generation
2024-06-25
12 Project: Run your own LLM
2024-07-02
13 Project: Presentations
2024-07-09
14 Summary & Questions
Plan updated!


Feedforward Neural
Language Models


Quiz: How would you build a simple 
sentiment classifier?
‚Ä¢ Input: Document (e.g., amazon review)
‚Ä¢ Examples:
‚Ä¢ ‚ÄúThis blender is great, it shredded my iphone without any problems‚Äù
‚Ä¢ ‚ÄúThe blender is no good L‚Äù
‚Ä¢ ‚ÄúBroken after first use!‚Äù
‚Ä¢ ‚ÄúThe dessert was delicious‚Äù
‚Ä¢ ‚ÄúI wouldn‚Äôt recommend the desserts at this place‚Äù
‚Ä¢ Output: Sentiment classification (binary: positive / negative)


Simple Text Classifiers: Ideas
‚Ä¢ Manual Feature Engineering
‚Ä¢ Embeddings


Sentiment Classifier: Manual Feature Engineering
‚Ä¢ Feature ideas:
‚Ä¢ Count positive words
‚Ä¢ Count negative words
‚Ä¢ Count positive emoticons
‚Ä¢ Count negative emoticons
‚Ä¢ Does the review contain negations?
‚Ä¢ Does the review include ‚Äú!‚Äù?
‚Ä¢ Length of review
1.
‚ÄúThis blender is great, it shredded my iphone without any problems‚Äù
2.
‚ÄúThe blender is no good L‚Äù
3.
‚ÄúBroken after first use!‚Äù
4.
‚ÄúThe dessert was delicious‚Äù
5.
‚ÄúI wouldn‚Äôt recommend the desserts at this place‚Äù
R. 1
R. 2
R. 3
R. 4
R. 5
1
1
1
1
2
1
1
1
1
1
1
11
6
5
4
8


‚Ä¢ Manual Feature Engineering turned each doc into a feature vector
‚Ä¢ Let‚Äôs plug them into a simple Feed Forward Neural Network
R. 1
R. 2
R. 3
R. 4
R. 5
1
1
1
1
2
1
1
1
1
1
1
11
6
5
4
8
Sentiment Classifier: Manual Feature Engineering
R. 1
R. 2
R. 3
R. 4
R. 5
1
1
1
1
2
1
1
1
1
1
1
11
6
5
4
8
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R. 1</th>
      <th>R. 2</th>
      <th>R. 3 R. 4 R. 5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1 1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>11</td>
      <td>6</td>
      <td>5 4 8</td>
    </tr>
  </tbody>
</table>

Sentiment Classifier: Embeddings
‚Ä¢ Hand crafted features
‚Ä¢ often useful
‚Ä¢ might reach their limits
‚Ä¢ Why not let the network learn what‚Äôs important?
‚Ä¢ Textual feature representations / Embeddings:
‚Ä¢ One-Hot
‚Ä¢ TF-IDF
‚Ä¢ Word2Vec
‚Ä¢ ‚Ä¶


Sentiment Classifier: NN based on Embeddings
h1
h2
h3
hdh
‚Ä¶
U
W
y
3d‚®â1
Hidden layer
Output layer
sigmoid
The
...
dessert
is
wt-1
w2
w1
dh‚®â3d
dh‚®â1
|V|‚®âdh
Projection layer
embeddings
p(positive sentiment|The dessert is‚Ä¶)
^
embedding for
word 7
embedding for 
word 23864
embedding for
word 534
w3
E
‚Ä¶
Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>The</th>
      <th>dessert</th>
      <th>is</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

Sentiment Classifier: NN based on Embeddings
h1
h2
h3
hdh
‚Ä¶
U
W
y
3d‚®â1
Hidden layer
Output layer
sigmoid
The
...
dessert
is
wt-1
w2
w1
dh‚®â3d
dh‚®â1
|V|‚®âdh
Projection layer
embeddings
p(positive sentiment|The dessert is‚Ä¶)
^
embedding for
word 7
embedding for 
word 23864
embedding for
word 534
w3
E
‚Ä¶
Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>The</th>
      <th>dessert</th>
      <th>is</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

Sentiment Classifier: NN based on Embeddings
h1
h2
h3
hdh
‚Ä¶
U
W
y
3d‚®â1
Hidden layer
Output layer
sigmoid
The
...
dessert
is
wt-1
w2
w1
dh‚®â3d
dh‚®â1
|V|‚®âdh
Projection layer
embeddings
p(positive sentiment|The dessert is‚Ä¶)
^
embedding for
word 7
embedding for 
word 23864
embedding for
word 534
w3
E
‚Ä¶
Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7


Sentiment Classifier: NN based on Embeddings
‚Ä¢ Issues?
‚Ä¢ Only works for fixed length inputs!
‚Ä¢ In this case: 3 words
‚Ä¢ Workarounds?
‚Ä¢ Set input size to fixed length (e.g., that of longest review)
‚Ä¢ Pad shorter with 0 vectors
‚Ä¢ Truncate longer ones
‚Ä¢ Create single fixed size review embedding (sentence / paragraph / document)
‚Ä¢ Mean of all word embeddings
‚Ä¢ Element-wise max of all word embeddings
h1
h2
h3
hdh
‚Ä¶
U
W
y
3d‚®â1
Hidden layer
Output layer
sigmoid
The
...
dessert
is
wt-1
w2
w1
dh‚®â3d
dh‚®â1
|V|‚®âdh
Projection layer
embeddings
p(positive sentiment|The dessert is‚Ä¶)
^
embedding for
word 7
embedding for 
word 23864
embedding for
word 534
w3
E
‚Ä¶
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>embe
w</th>
      <th>dding for
ord 7</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>The</th>
      <th>dessert</th>
      <th>is</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

Simple Text Classifiers: More Classes
‚Ä¢ General idea applicable not only to (binary) sentiment classification


Simple Language Modeling Task


Neural Language Models (LMs)
Language Modeling: Calculating the probability of the 
next word in a sequence given some history. 
‚Ä¢
We've seen N-gram based LMs
‚Ä¢
But neural network LMs far outperform n-gram 
language models
State-of-the-art neural LMs are based on more 
powerful neural network technology like Transformers
But simple feedforward LMs can do almost as well!


Simple feedforward Neural Language Models
Task: predict next word wt 
 
 
  given prior words wt-1, wt-2, wt-3, ‚Ä¶
Problem: Now we‚Äôre dealing with sequences of 
arbitrary length.
Solution: Sliding windows (of fixed length)

[IDS] In the context of a lecture on NLP, we are discussing the concept of predicting the next word in a sequence given prior words, which is a common task in language modeling. The lecture emphasizes the use of simple feedforward neural language models to solve this problem. The solution presented involves sliding windows of fixed length, indicating that the model considers a certain number of preceding words to predict the next one. This approach is essential for understanding how neural networks can be trained to generate coherent text or respond appropriately in natural language processing tasks. [IDE]


Neural Language Model 

[IDS] The image represents a fundamental concept in Natural Language Processing (NLP) known as Feedforward Neural Language Models. These models are designed to process and understand human language by learning from large datasets of text. The diagram illustrates the architecture of such a model, which typically consists of an input layer that accepts text data, multiple hidden layers where complex patterns are detected, and an output layer that generates predictions or continuations of the input text. In this specific model, there's also a projection layer and an embedding layer for word representations, which are crucial for capturing the semantic meaning of words. The presence of mathematical notations like p(z|...), p(for|...), and V√ó1 indicates the probabilistic nature of these models and their focus on statistical prediction. This type of model is widely used in applications such as language translation, speech recognition, and text generation. [IDE]


Why Neural LMs work better than N-gram LMs
Training data:
We've seen:  I have to make sure that the cat gets fed. 
Never seen:   dog gets fed
Test data:
I forgot to make sure that the dog gets ___
N-gram LM can't predict "fed"!
Neural LM can use similarity of "cat" and "dog" 
embeddings to generalize and predict ‚Äúfed‚Äù after dog
[Lecture End]

[Lecture Start]

------------recurrent_neural_networks_for_nlp------------

What do we need them for? (I/II)
Fig. 1: High level representation of an encoder
ÔÅ¨
General case: 
We need RNNs for representing a sequence of variable length as a single 
vector (encoder) OR generating a sequence of variable length from a 
single vector (decoder)

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents the concept of using Recurrent Neural Networks (RNNs) for tasks in NLP. RNNs are a type of neural network architecture that is particularly well-suited for processing sequences of data, such as text or speech. The diagram illustrates how an RNN can be used to generate a sequence of variable length from a single vector (encoder) or by encoding a sequence of variable length as a single vector (decoder). This process involves taking input sequences of varying lengths and converting them into a fixed-size representation that can be processed by the model. The figure shows the flow of information from the input sequence through the RNN, which generates an output vector (latent representation), indicating how the RNN processes and encodes the input data in NLP applications. [IDE]



What do we need them for? (II/II)
Fig. 2: High level representation of a decoder

[IDS] In the context of a lecture on Recurrent Neural Networks for Natural Language Processing (NLP), the image illustrates the process of generating text using an RNN. The input vector represents the initial state or prompt that the decoder will use to generate the output sequence. The high-level representation shows the flow from the input to the final output, emphasizing how the RNN processes and generates text based on the input provided. This is a fundamental concept in NLP, where RNNs are trained to understand and produce human-like language. [IDE]



Types of RNNs relevant to us
ÔÅ¨
Gated Recurrent Unit (GRU)
ÔÅ¨
Long Short Term Memory (LSTM)

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) that focuses on the application of Recurrent Neural Networks (RNNs) in this field. The title "Types of RNNs relevant to us" suggests that the slide will discuss specific types of RNNs that are particularly useful or interesting for NLP tasks. Two examples provided are "Gated Recurrent Unit (GRU)" and "Long Short Term Memory (LSTM)", which are both architectures of RNNs known for their ability to handle sequences of data, like text or speech. These architectures are essential for tasks such as language modeling, machine translation, and text generation. The logos at the bottom indicate that the lecture might be part of a course offered by Hochschule Bonn-Rhein-Sieg and is related to Fachbereich Informatik, which translates to Faculty of Computer Science. This suggests an academic setting where students are likely learning about the theoretical and practical aspects of using RNNs in NLP. [IDE]



Gated Recurrent Unit
Fig. 3.1: High level represenation of GRU cell

[IDS] The image depicts a fundamental component of Recurrent Neural Networks (RNNs), specifically the Gated Recurrent Unit (GRU). In the context of Natural Language Processing (NLP) lectures, GRUs are a type of RNN that can handle variable-length sequences, making them suitable for tasks such as language modeling or sequence prediction. The diagram illustrates how the GRU processes information through gates, which control the flow of information and help in learning long-term dependencies in data. This is crucial for NLP applications where understanding context over long sequences is essential. The presence of Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik suggests that this image may be part of educational material from an informatics department, emphasizing the practical application of GRUs in NLP tasks. [IDE]



Gated Recurrent Unit
Fig. 3.2: High level represenation of GRU cell
Hidden states

[IDS] The image is a visual representation of a Gated Recurrent Unit (GRU), which is a type of recurrent neural network architecture. In the context of NLP, or Natural Language Processing, GRUs are commonly used for modeling sequences in data, such as text or speech. The diagram shows how the hidden states are passed through time, allowing the model to remember and utilize information from previous time steps. This is particularly useful in NLP tasks where understanding the context and dependencies between words is crucial. The lecture you're attending is likely discussing how these GRUs can be applied to NLP problems, leveraging their ability to capture long-term dependencies and handle variable-length input sequences. [IDE]



Gated Recurrent Unit
Fig. 3.3: High level represenation of GRU cell
Output of the previous time 
step. ht=0 = [0, ‚Ä¶, 0]

[IDS] The image depicts a Gated Recurrent Unit (GRU), which is a type of recurrent neural network architecture. In the context of NLP, or Natural Language Processing, GRUs are often used to process sequential data, such as text or speech. They are designed to handle the vanishing gradient problem that can occur in traditional RNNs by using gating mechanisms. These gates help control the flow of information through the network, allowing it to remember important information for longer periods. The diagram shows how the output at one time step is determined by the input at the current time step and the hidden state from the previous time step, illustrating the sequence-to-sequence nature of GRUs in NLP tasks. [IDE]



Gated Recurrent Unit
Fig. 3.4: High level represenation of GRU cell
Input for the current 
time step. 
Embedding for 
token t

[IDS] The image represents a fundamental concept in Natural Language Processing (NLP), specifically the use of Recurrent Neural Networks (RNNs). In the context of NLP, RNNs are often used to process sequences of data such as text. The Gated Recurrent Unit (GRU) is a type of RNN that helps address the vanishing gradient problem by introducing gating mechanisms. These gates control the flow of information within the network, allowing it to learn long-term dependencies effectively.

The diagram illustrates the GRU's ability to maintain and update the hidden state (ht) over time steps (xt). This is crucial for tasks like language modeling or sequence-to-sequence translation where the context from previous tokens is essential. The input for the current time step is shown to be embedded into a high-level representation of the GRU cell, indicating how the model processes and retains information from previous inputs to inform its current output.

As we are in a NLP lecture, this slide would serve as an educational tool to help students understand how RNNs, particularly GRUs, work internally and how they can be applied to NLP tasks. [IDE]



Gated Recurrent Unit
Fig. 3.5: High level represenation of GRU cell
Output for the 
current time step.

[IDS] The image is a visual representation of a Gated Recurrent Unit (GRU), which is a type of neural network architecture commonly used in Natural Language Processing (NLP). In the context of NLP, GRUs are employed to handle sequential data, such as text or speech, by capturing dependencies and patterns across time. The diagram illustrates how the GRU processes input over time steps, using gates to control the flow of information, which helps in learning long-term dependencies. This is crucial for tasks like language modeling, machine translation, and text generation. As we are in a NLP lecture, this slide serves as an educational tool to explain how GRUs contribute to the field by enabling models to effectively process and understand sequences of data. [IDE]



Encoder Example

[IDS] In this NLP lecture, you are learning about the application of recurrent neural networks in natural language processing. The slide you're looking at is an example of how a recurrent neural network (RNN) can be used to encode sequences of words into fixed-length vectors, which can then be used for various NLP tasks such as language modeling or machine translation. The RNN shown processes input sequences like "I like to eat pizza" and outputs fixed-size vectors that capture the meaning of the sentence. This is a fundamental concept in NLP, as it allows computers to understand and interpret human language more effectively. [IDE]



Encoder Example

[IDS] The image you're seeing is an illustration of a Recurrent Neural Network (RNN) being used for Natural Language Processing (NLP). In the context of NLP, RNNs are particularly useful because they can capture dependencies between words or phrases in text data. The diagram shows how an input sequence is processed by an RNN to produce an output. The RNN takes into account the previous output when generating the current output, which allows it to remember information over time. This property makes RNNs suitable for tasks such as language modeling, where predicting the next word in a sentence depends on the context provided by the words that came before it. [IDE]



Encoder Example

[IDS] The image represents a basic example of an encoder in the context of recurrent neural networks for natural language processing (NLP). The encoder is designed to process sequences of input data, such as text or words, and transform them into a fixed-size vector representation. In this example, the input sequence "I like to eat pizza" is being processed by an RNN (Recurrent Neural Network), which outputs a fixed-size vector representation h3. This vector h3 captures the essence of the input sequence and can be used for further tasks such as language modeling, machine translation, or sentiment analysis. The RNN architecture depicted includes an embedding layer that converts each word into a dense vector representation, followed by the recurrent layer that processes the sequence. The encoder's ability to handle sequential data and retain information over time makes it particularly suitable for NLP applications where understanding the context and meaning of text is crucial. [IDE]



Encoder Example

[IDS] In this NLP lecture, we are exploring the application of Recurrent Neural Networks (RNNs) in Natural Language Processing. The image illustrates a basic RNN architecture that is commonly used for sequence modeling tasks such as language translation, sentiment analysis, and text generation. The example shown demonstrates how an RNN processes input sequences to generate embeddings and outputs, which can be further utilized for various NLP tasks. This visual representation serves as a fundamental concept in understanding how RNNs work in the context of NLP. [IDE]



Encoder Example

[IDS] The image represents a fundamental concept in NLP, specifically the use of Recurrent Neural Networks (RNNs) for processing sequences of data. In this context, an RNN is shown as a component that takes input sequences, processes them through its hidden layers, and outputs processed information. The presence of a Hochschule Bonn-Rhein-Sieg logo suggests that this might be educational material from that institution. Understanding how RNNs work is crucial for NLP tasks such as language modeling, sequence prediction, and text generation, as they can capture dependencies between elements in a sequence. [IDE]



Encoder Example

[IDS] In the context of a lecture on recurrent neural networks for NLP, this image represents a fundamental concept in the field of natural language processing. It illustrates how an encoder network can process sequential data, such as text, and generate representations that capture the meaning of the input sequence. The RNN shown is a type of neural network architecture that is particularly well-suited for tasks involving sequences, such as language modeling or machine translation. The previous and current inputs, along with their corresponding outputs, demonstrate how the RNN processes information over time, which is essential for understanding and generating coherent language sequences. This is a crucial aspect of NLP, where the ability to analyze and generate text is key. [IDE]



Encoder Example

[IDS] The image depicts a recurrent neural network (RNN) architecture being used for natural language processing (NLP). The RNN is designed to process sequences of data, such as words in a sentence. In this specific example, the input sequence is "I like to eat pizza," and the task is to predict the next word in the sequence, which is "<s>" in this case. The RNN takes the previous output, which is represented by a hidden state h_6, and uses it along with the current input (the last processed word "x_7") to generate the next output, denoted as h_7. The final output is a probability distribution over possible next words, where the most likely next word is "<s>". This is a common approach in NLP to generate text or predict the next word in a sequence. [IDE]



Long Short Term Memory

[IDS] The image is a schematic representation of the Long Short-Term Memory (LSTM) memory cell, which is a type of recurrent neural network (RNN) architecture commonly used in Natural Language Processing (NLP). LSTMs are designed to handle the vanishing gradient problem that occurs in traditional RNNs by using memory cells that can store information for longer periods. This allows the network to remember information for longer durations, which is essential for tasks such as language modeling and sequence prediction.

In this context, the LSTM cell depicted in the image is part of a larger system that includes an input layer (Input X), hidden layers (Hidden state H_t-1), and an output layer (Output gate o, tanh activation function). The cell has several components: a forget gate, an input gate, and an output gate. These gates control the flow of information into and out of the cell, as well as what is forgotten or remembered from the previous time step.

Understanding LSTMs is crucial for NLP because they enable machines to process and analyze sequences of data, such as text or speech, with a level of context-awareness that goes beyond simple statistical models. [IDE]



Long Short Term Memory

[IDS] The image is a visual representation of the Long Short-Term Memory (LSTM) memory cell, which is a type of recurrent neural network (RNN) architecture commonly used in Natural Language Processing (NLP). In the context of an NLP lecture, this diagram would be used to explain how LSTMs can remember information for long periods of time, enabling them to process sequences of data, such as sentences or paragraphs. The LSTM cell consists of three gates: the input gate, the forget gate, and the output gate. These gates control the flow of information into and out of the cell state, allowing the LSTM to selectively retain or discard information over time. This is crucial for tasks like language modeling, machine translation, and text generation, where understanding and remembering the context of words and phrases is essential. [IDE]



Long Short Term Memory
Xt
Wxi
Whi
Ht-1
   bi
It
Xt
Wxf
Whf
Ht-1
   bf
Ft
Xt
Wxo
Who
Ht-1
   bo
Ot
Input Gate
Forget Gate
Output Gate

[IDS] The image illustrates the basic components of a recurrent neural network (RNN) used in Natural Language Processing (NLP). RNNs are designed to process sequences of data, such as text or speech. The figure shows how input gates, forget gates, and output gates control the flow of information through the network, allowing it to remember past information and update its state based on new inputs. This is crucial for tasks like language modeling, where understanding the context of words in a sentence is essential. In an NLP lecture, this diagram would be used to explain how these components work together to enable the network to learn and generate sequences that reflect the structure and meaning of natural language. [IDE]



Long Short Term Memory

[IDS] The image depicts a diagram illustrating the architecture of a Long Short-Term Memory (LSTM) network, which is a type of recurrent neural network. In the context of Natural Language Processing (NLP), LSTMs are particularly useful for handling sequences of data, such as text or speech, because they can maintain information over long periods by using memory cells that can be updated and reset based on the input. The diagram shows the flow of information through the network, with input being processed through gates to determine what information to forget, input, and output, ultimately resulting in an output gate that produces the final output. This mechanism allows LSTMs to remember information over long periods, which is essential for tasks like language modeling, machine translation, and sequence prediction in NLP. [IDE]



Long Short Term Memory
Xt
Ht-1
   bc
 Ct
Wxc
Whc
 ~

[IDS] The image represents a key concept in the field of Natural Language Processing (NLP) and Recurrent Neural Networks (RNNs). It illustrates the Long Short-Term Memory (LSTM) cell, which is a type of RNN that can learn order dependence in sequence prediction problems. In NLP, LSTMs are often used to process sequences of words or characters, allowing the model to retain information from previous time steps. The diagram shows how inputs (X_t), hidden states (H_t-1), and weights (W_xc, W_hc) are combined to produce an output (C_t) and a new hidden state (H_t), capturing both short-term and long-term dependencies in the data. This mechanism is crucial for tasks such as language modeling, machine translation, and text generation, where understanding the context of words over time is essential. [IDE]



Long Short Term Memory

[IDS] The image you see is a diagram that represents the concept of Long Short-Term Memory (LSTM), which is a type of recurrent neural network (RNN). RNNs are particularly useful in Natural Language Processing (NLP) tasks because they can remember information over long periods of time, allowing them to capture dependencies between words or phrases in text data. The LSTM architecture is designed to address the vanishing gradient problem that affects standard RNNs by using memory cells and gates to control the flow of information. In the context of an NLP lecture, this diagram would help students understand how LSTMs work internally and how they can be used to process sequences of data, such as text or speech, to perform tasks like language modeling, machine translation, or sentiment analysis. [IDE]



Long Short Term Memory
Update Cell state 
It
 Ct
 ~
Ct = It  ò Ct + Ft  ò Ct-1
Update Cell state 
 ~
Ft
Ct-1
Ct =
Element-wise multiplication

[IDS] The image represents a concept in Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP), specifically the mechanism of Long Short-Term Memory (LSTM). LSTMs are a type of RNN that can learn order dependence in sequence prediction problems. The diagram illustrates how an LSTM cell updates its state based on the current input and the previous cell state. This is crucial for NLP tasks, as it allows the model to remember information over long periods, which is essential for understanding the context and meaning of sequences of words or characters. [IDE]



Long Short Term Memory
Update Cell state 
It
 Ct
 ~
Ct = It  ò Ct + Ft  ò Ct-1
Update Cell state 
 ~
Ft
Ct-1
Ct =
Element-wise multiplication
How much of the candidate 
cell state do we take?

[IDS] The image is a visual representation of a concept in Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP). RNNs are a type of neural network architecture that can process sequences of data, such as text or speech. The slide specifically focuses on the "Long Short Term Memory" (LSTM) memory mechanism, which is a type of RNN that can learn order dependence in sequence prediction problems. 

The LSTM cell state is depicted as being updated by adding the output of the LSTM cell (Ct) and the input at time t (It). This is represented mathematically with the equation Ct = Lt ‚äô Ct-1 + Ft ‚äô Ct. Here, Lt and Ft are matrices that determine how much of the previous cell state (Ct-1) and the current input (It) are taken into account when updating the cell state.

The slide also includes elements like the Hochschule Bonn-Rhein-Sieg logo and the name Tim Metzler, which could indicate the institution and the lecturer responsible for the lecture. Additionally, there is a mention of "Element-wise multiplication" and "Fachbereich Informatik," suggesting that the topic might be related to computer science and specifically to the field of informatics or computer science departments.

In summary, the image is a teaching aid used in an NLP lecture to explain how LSTMs work in processing sequences of data, emphasizing the importance of the cell state in preserving information over time. [IDE]



Long Short Term Memory
Update Cell state 
It
 Ct
 ~
Ct = It  ò Ct + Ft  ò Ct-1
Update Cell state 
 ~
Ft
Ct-1
Ct =
Element-wise multiplication
How much of the previous 
cell state do we keep?

[IDS] The image you're seeing is a slide from a lecture on "Recurrent Neural Networks for NLP," which stands for Natural Language Processing. It explains the concept of long short-term memory (LSTM) in neural networks. LSTMs are a type of recurrent neural network that helps the network remember information over longer periods, which is crucial for tasks like language modeling or machine translation. The slide specifically shows how an LSTM updates its cell state and how much of the previous cell state is kept. This is a fundamental concept in understanding how LSTMs process sequences of data, such as sentences or paragraphs, in natural language processing tasks. [IDE]



Long Short Term Memory

[IDS] The diagram represents a Long Short-Term Memory (LSTM) cell, which is a type of recurrent neural network (RNN) architecture. RNNs are particularly useful for Natural Language Processing (NLP) tasks as they can maintain information over long sequences. The LSTM cell has a memory component that allows it to remember information for an extended period, which is crucial for NLP applications like language modeling, machine translation, and sequence prediction. In this context, the LSTM would be used to process and analyze sequences of text or other linguistic data, capturing dependencies and relationships between words or phrases. [IDE]



Long Short Term Memory
Compute hidden state (output) 
Ct
Ot
Ht =

[IDS] The image you're seeing is a slide from a lecture on "Recurrent Neural Networks for NLP," which stands for Natural Language Processing. It focuses on the concept of Long Short-Term Memory (LSTM), a type of recurrent neural network that is particularly good at processing sequences of data, like words in a sentence. The slide illustrates how an LSTM works by computing a hidden state output, represented as "Ht." This hidden state captures information from both previous states (Ot) and current inputs (Ct), allowing the network to remember information over long periods. The slide also includes logos from Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting that the lecture might be part of a course or program offered by these institutions. [IDE]



LSTM - Complexity
ÔÅ¨
Number of parameters:
ÔÅ¨
- Input dimensionality of d
- Output dimensionality of h
- 4 input weight matrices of (d x h)
- 4 hidden weight matrices of (h x h)
- 4 biases of (h x 1)
ÔÅ¨
‚Üí Number of parameters = 4*d*h + 4*h*h + 4*h = 4h*(d + h + 1)
ÔÅ¨
Example:
- Input size of 300
- Output size of 32
‚Üí 42,624 parameters

[IDS] The image depicts a slide from a lecture on recurrent neural networks for NLP, which stands for Natural Language Processing. The slide is focused on explaining the complexity of LSTM (Long Short-Term Memory) models. It lists several parameters that define the complexity of an LSTM network, such as the number of input dimensions, output dimensions, input weight matrices, hidden weight matrices, and biases. These parameters are crucial in determining the capacity of the model to learn and represent complex patterns in data, especially when dealing with sequential data like text or speech in NLP applications. The slide also provides an example calculation of these parameters for a specific model architecture, demonstrating how they contribute to the overall complexity of the network. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In this NLP lecture, we are exploring the concept of recurrent neural networks, specifically focusing on their application in natural language processing. The diagram illustrates a typical RNN architecture used for autoregressive generation with an RNN-based neural language model. This involves an input word being embedded into a vector space, which is then processed through a series of layers including an RNN and an embedding layer. The output from the RNN is fed back into itself, allowing for the modeling of sequences in data, such as text. The final softmax layer converts the output into a probability distribution over possible next words in a sequence, enabling the model to predict the most likely next word given the context provided by the input word. This process is fundamental in tasks like language translation, text summarization, and chatbots, where understanding and generating human-like language is crucial. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In this NLP lecture, we are presented with an illustration of a recurrent neural network (RNN) that is capable of autoregressive generation using a RNN-based neural language model. The diagram shows the flow of information through the RNN, starting with the embedding of input words, passing through a softmax function to generate the output word, and repeating this process to produce a sequence of words. This process is essential for understanding how RNNs can be used in natural language processing tasks such as language modeling and text generation. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In the context of a lecture on Recurrent Neural Networks for NLP (Natural Language Processing), the image illustrates the architecture of an RNN that has been trained to perform autoregressive generation. This means that the model generates the next word in a sequence based on the previous words. The diagram shows how the input word is processed through an embedding layer, which transforms it into a dense vector representation. These vectors are then passed through a recurrent neural network, which consists of a series of recurrent units. Each unit processes the information from the previous time step and uses it to influence the current output. The Softmax function at the end of the process converts the final vector into a probability distribution over all possible words, allowing the model to predict the most likely next word in the sequence. This type of model is commonly used in language modeling tasks where the goal is to generate text that is coherent and grammatically correct. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In this NLP lecture, you are learning about Recurrent Neural Networks (RNNs) and their application in Natural Language Processing. The image represents an RNN model that is used for autoregressive generation with a neural language model. The model takes input words and generates the next word in sequence using a softmax function to determine the probability distribution over the vocabulary. This process is repeated until a stop token (So) is encountered, indicating the end of the generated text. The lecture emphasizes the importance of understanding how RNNs can be trained to generate coherent and contextually relevant text, which is a fundamental concept in NLP. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In the context of a NLP lecture, the image illustrates the concept of RNNs (Recurrent Neural Networks) and their application in generating autoregressive text. The diagram shows how an RNN processes input words sequentially and uses its internal memory to generate predictions for the next word in the sequence. This process is repeated until the end of the sentence is reached, resulting in an autoregressive generation that builds upon previous outputs. The lecture likely covers how RNNs can be used to perform tasks such as language modeling, text prediction, and machine translation, which are essential topics in natural language processing. [IDE]



RNNs in NLP
Repeat until we 
generate </s>
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] In this NLP lecture, we are discussing Recurrent Neural Networks (RNNs) as a powerful tool for natural language processing. The diagram illustrates an RNN architecture that processes a sampled word and its context, such as "So long and?". The RNN uses embeddings to convert words into numerical representations, which are then processed through the network. The softmax function is used to generate a probability distribution over possible outputs. This process is repeated until a complete sentence is generated. The figure caption explains that the RNN can be autoregressive, using a language model like Jurafsky & Martin's "Speech and Language Processing" to generate text. [IDE]



RNNs in NLP
How do we train this model?

[IDS] The image represents a slide from an educational presentation on Recurrent Neural Networks for Natural Language Processing (NLP). The title "RRNs in NLP" is prominently displayed, indicating the focus of the lecture. The subtitle "How do we train this model?" suggests that the content will delve into the training process of RNNs specifically for NLP tasks. The logos of the Hochschule Bonn-Rhein and Fachbereich Informatik indicate the institutions involved or associated with the lecture. The name Tim Metzler likely refers to the lecturer who prepared or is delivering the lecture. The blue circle with a white border might be a visual element or logo related to the course or institution. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 7
Target: So long and thanks for all the ...

[IDS] In this NLP lecture, we are discussing the topic of recurrent neural networks for natural language processing. The slide presents a diagram that illustrates the architecture of an RNN and its components. It includes a section labeled "Input Embeddings," which is crucial for representing words as vectors in the neural network. There is also a section labeled "Softmax over Vocabulary" that is used to determine the most probable next word given the current context. The flow of data through the network is indicated by arrows, showing how the input embeddings are processed through the RNN and then passed to the softmax layer. The lecture emphasizes the importance of understanding RNNs as language models, which is a fundamental concept in NLP. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 7
Target: So long and thanks for all the ...
No matter what we generate here,
we always feed in the correct token as next input.
This is called ‚ÄûTeacher Forcing‚Äú

[IDS] In this NLP lecture, we are discussing the role of RNNs (Recurrent Neural Networks) in natural language processing. The slide presents a diagram that illustrates how RNNs process and generate sequences of words. It highlights the concept of "Teacher Forcing," which ensures that the correct token is always fed as the next input to the RNN during training. This technique helps in training RNNs as language models, as mentioned in Figure 9.6, sourced from Jurafsky & Martin's "Speech and Language Processing" book. The slide also includes references to Hochschule Bonn-Rhein-Sieg and Tim Metzler, indicating their contributions to the lecture content. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 7
Target: So long and thanks for all the ...

[IDS] In the image, we see a slide from a lecture on Recurrent Neural Networks for NLP (Natural Language Processing). The slide is titled "RNNs in NLP" and highlights the importance of long and thanks for all the words. It illustrates how an RNN processes input embeddings to generate output, which is a crucial aspect of language modeling. The equation provided is a mathematical representation of the softmax over vocabulary, which is a common technique used in NLP to convert a vector into a probability distribution over possible outputs. This slide serves as an educational tool to help students understand how RNNs can be applied to NLP tasks such as language processing. [IDE]



RNNs in NLP
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 7
Target: So long and thanks for all the ...
Loss for the sequence

[IDS] The image represents a visual explanation of how Recurrent Neural Networks (RNNs) can be used in Natural Language Processing (NLP). RNNs are a type of neural network architecture that is particularly suited for processing sequences of data, such as text. In this context, the RNN is being used to predict the next word in a sequence given the current word and the entire sequence so far. The diagram shows the flow of information from the input embeddings through the RNN layers and finally to the output, where the probability of each possible next word is calculated. This process is part of the training phase, where the model learns to predict the most likely next word based on the context provided by the entire sequence up to that point. [IDE]



More applications
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 8

[IDS] The image is a visual representation of a recurrent neural network architecture used in Natural Language Processing (NLP). It illustrates how the RNN processes input words, assigns tags to them, and uses a softmax layer to output the probability distribution over possible tags. This is a key concept in understanding how RNNs can be applied to tasks such as part-of-speech tagging, which is a fundamental task in NLP that involves assigning grammatical categories to each word in a sentence. The figure serves as an educational tool to help students and professionals grasp the structure and function of RNNs in the context of NLP. [IDE]



Machine Translation
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 19

[IDS] The image is a visual representation of the process of machine translation using recurrent neural networks (RNNs) for natural language processing (NLP). In this context, an RNN is trained to encode and decode sentences between two languages. The lecture likely covers how these networks can be used to translate text from one language to another by learning the statistical patterns and structures of each language. The diagram illustrates the flow of information through the network, starting with the input sentence in the source language, passing through the encoder, and then being decoded into the target language. This is a fundamental concept in NLP, where understanding how RNNs function is crucial for building efficient and accurate machine translation systems. [IDE]



Stacking RNN Layers
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] The image represents a stacked recurrent neural network architecture, which is a type of artificial neural network commonly used in natural language processing (NLP). The diagram shows multiple layers of RNNs, where each layer processes and passes information to the next. These networks are designed to handle sequential data, like text, by maintaining a hidden state that captures information from previous inputs. This allows the network to remember information over time, which is essential for tasks such as language modeling, machine translation, and text generation. The lecture titled "Recurrent Neural Networks for NLP" would likely cover the fundamentals of RNNs and their applications in natural language processing. [IDE]



Bidirectional RNN
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11

[IDS] The image is a schematic representation of a Bidirectional RNN, which is a type of recurrent neural network used in natural language processing (NLP). In the context of NLP, RNNs are particularly useful for tasks such as language modeling and sequence prediction. The bidirectional aspect of this RNN allows it to process data in both forward and backward directions, which is essential for capturing context from both past and future time points in a sequence, such as words in a sentence. This can help improve the performance of NLP models by providing richer information about the context in which words are used. The diagram illustrates how inputs (y1, y2, y3,..., yn) are processed through multiple layers of the RNN, with each layer consisting of nodes that represent the states at different time steps. The final outputs of the RNN (RNN 1 and RNN 2) are then concatenated, indicating how the model integrates information from both directions to produce its output. [IDE]



Bidirectional RNN
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11
Reading left to right

[IDS] The image you see is a diagram of a Bidirectional RNN, which is a type of recurrent neural network used in Natural Language Processing (NLP). This particular model is trained to generate bidirectional representations of text data. The diagram illustrates how the RNN processes information both forward and backward through time, with each direction feeding into two separate RNN units labeled as RNN 1 and RNN 2. The outputs from these units are then concatenated to produce the final output at each time point. This technique allows the model to capture context from both past and future time points, which is essential for tasks like language translation or text summarization. As part of an NLP lecture, this would be used to explain how such models can improve the understanding and processing of language data. [IDE]



Bidirectional RNN
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11
Reading left to right

[IDS] The image you're viewing is a schematic representation of a Bidirectional Recurrent Neural Network (BRNN), which is a type of recurrent neural network used in Natural Language Processing (NLP). In NLP, we deal with the complexity of human language, which often requires understanding context from both past and future sentences. This is where BRNNs come into play.

BRNNs are designed to process sequences of data, like text, in both forward and backward directions. They maintain a state for each time point, which is updated as it moves through the sequence. The outputs from the forward and backward passes are then concatenated at each time point to provide a richer context to the model.

The diagram shows two RNNs running in opposite directions, labeled as "RNN 1" and "RNN 2". Each RNN processes the input sequence from different ends - "RNN 1" from left to right and "RNN 2" from right to left. The outputs of these two RNNs are then combined at each time point, as indicated by the yellow rectangles, which represent the concatenation of outputs.

This architecture allows the model to capture dependencies in the input sequence from both directions, making it useful for tasks such as language translation, text classification, and machine reading comprehension.

In the context of your lecture on "recurrent_neural_networks_for_nlp", this image would serve as an illustrative example of how BRNNs can be applied to enhance the performance of NLP models by leveraging the context from both sides of the input sequence. [IDE]



Bidirectional RNN
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11
Reading left to right
Reading right to left

[IDS] In the context of a lecture on recurrent neural networks for natural language processing (NLP), the image you're seeing illustrates a Bidirectional RNN, which is a type of recurrent neural network used in NLP tasks. This diagram represents how the network processes input data (like sentences or words) and outputs predictions (such as the next word in a sentence). The yellow boxes labeled 'x1', 'x2', 'x3' represent the input sequences, while 'y1', 'y2', 'y3' indicate the corresponding output sequences. The directional arrows show the flow of information from inputs to outputs and back, which is characteristic of bidirectional RNNs. These networks are trained to capture context from both past and future time points, which is crucial for understanding the meaning of words in a sequence. In this specific diagram, the outputs are generated by concatenating outputs from the forward and backward passes of the RNN. This concept is fundamental in NLP, as it helps in tasks like language modeling, machine translation, and text classification. [IDE]



Bidirectional RNN
source: Jurafsky & Martin ‚ÄûSpeech and Language Processing‚Äú, Chapter 9, page 11
Reading left to right
Reading right to left

[IDS] The image illustrates a Bidirectional RNN, which is a type of recurrent neural network used in natural language processing (NLP). In NLP, RNNs are particularly useful for tasks such as language modeling, machine translation, and text classification. The diagram shows the architecture of a Bidirectional RNN, where outputs from the network are concatenated and used to provide context from both past and future time points. This allows the model to capture information from both directions of the sequence, which is essential for tasks like sentiment analysis or named entity recognition where context from both sides of a word is crucial. [IDE]
[Lecture End]

[Lecture Start]

------------Transformers_Encoder------------

Transformer
ÔÅ¨
Developed at Google in 2017 by Vaswani et.al.
ÔÅ¨
Works on a sequence of tokens (e.g. a sentence, document, etc)
ÔÅ¨
Often used as encoder decoder model
ÔÅ¨
Utilizes self-attention

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the concept of "Transformers_Encoder". It highlights that Transformers were developed at Google in 2017 by Vaswani et.al. The slide also mentions that Transformers work on a sequence of tokens, such as a sentence, document, etc., and are often used as encoder decoder models. Additionally, it points out the utilization of self-attention in this context. The slide includes logos of Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting an academic setting, and the name Tim Metzler, possibly the lecturer or author of the presentation. [IDE]



Transformer
Transformer
RNN
Sequence length
fixed
Infite in theory
Attention
Self Attention
Bahdanau or 
Luong Attention
Parsing the input 
sequence
All at once
One by one

[IDS] The image displays a comparison between two types of neural network architectures used in natural language processing (NLP): Transformer and RNN (Recurrent Neural Network). The Transformer is characterized by its fixed sequence length and self-attention mechanism, which allows it to process input data in parallel. It is described as being infinite in theory, suggesting that it can handle sequences of any length. On the other hand, RNNs are known for their ability to parse input sequences one at a time, with attention mechanisms that focus on specific parts of the input. This table provides a concise overview of the fundamental differences between these two NLP models, which are essential for understanding recent advances in the field. [IDE]



Tokenization
ÔÅ¨
Word2Vec: One token per word (word == token)
ÔÅ¨
FastText: One token per subword. Subword is character N-Gram.
Example: Use 3 and 4-Grams of a word. Word: ‚ÄúSchool‚Äù
‚Äôschool‚Äô ‚Üí ['sch', 'cho', 'hoo', 'ool', 'scho', 'choo', 'hool']
ÔÅ¨
FastText of Word2Vec:
- Word2Vec has fewer tokens
- FastText can represent OOV words
Word2Vec and FastText

[IDS] The image you provided is a slide from an NLP lecture focusing on "Tokenization". It explains the process of tokenization using Word2Vec and FastText, which are techniques for converting text into vector representations. The slide specifically describes how Word2Vec represents words as vectors, with each word represented by a pair of tokens (word = token). It also mentions that FastText is one token per subword, and it uses subwords to represent character N-Grams, like 'choo' or 'hool'. This technique helps in representing OOV (out-of-vocabulary) words. The slide also shows examples of Word2Vec and FastText tokenization for the word'school'. The lecture seems to be detailed and informative, aimed at helping students understand how text data can be processed and converted into numerical data for machine learning models. [IDE]



WordPiece Tokenization
ÔÅ¨
Developed at Google in 2015 by Wu et. al. (https://arxiv.org/pdf/1609.08144.pdf)
ÔÅ¨
Split text into tokens that can be subwords or full words
ÔÅ¨
Algorithm:
Input: 
- Size of vocabulary
- Corpus
1. Start with one token == one character
2. Combine two tokens into a new token. Use the combination that appears most 
often in the corpus.
3. Add this new token to the vocabulary
4. Repeat until #tokens = size of vocabulary

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically about the topic of "Transformers_Encoder". The slide focuses on WordPiece Tokenization, a technique used in NLP to split words into subwords or full words. It provides steps for implementing this algorithm, such as starting with one token, combining tokens into a new token, adding new tokens to the vocabulary, and repeating until reaching a desired size of vocabulary. This process is crucial for models like BERT, which use subword tokenization to improve their understanding of language. [IDE]



WordPiece Tokenization (cont‚Äôd)
ÔÅ¨
Example:
Vocab Size: 4
Corpus: snowboard, snow, snowboarding, surfing, surfboarding, surf
Tokens: 
- snow
- board
- ing
- surf
ÔÅ¨
We only need 4 tokens to represent all words:
- snowboard = snow + ##board
- snowboarding = snow + ##board + ##ing
ÔÅ¨
Can be applied to language such as Chinese or Japanese

[IDS] The image is a slide from an NLP (Natural Language Processing) lecture, focusing on the concept of "Transformers_Encoder". It explains how WordPiece Tokenization is used in transformer models to handle out-of-vocabulary words by breaking them down into subwords. The slide provides an example vocabulary size of 4 and lists tokens such as'snow', 'board', 'ing', and'surf'. It also mentions that only 4 tokens are needed to represent all words, with examples like'snow + ##board = snow + ##board + ##ing'. Additionally, it notes that this tokenization can be applied to languages such as Chinese or Japanese. The slide is attributed to Hochschule Bonn-Rhein-Sieg and Tim Metzler. [IDE]



Preprocessing
ÔÅ¨
Add special tokens to the text
ÔÅ¨
[CLS] ‚Äì Special token at the start of each input sequence. The embedding for this will often be 
used for classification. Learns information about the whole sequence.
ÔÅ¨
[PAD] ‚Äì We always feed a fixed length sequence of text. Usually our input sequence is smaller 
and needs to be padded to have this length. This is done by the padding token.
ÔÅ¨
[SEP] ‚Äì We might feed several sentences or documents to the model. Each of them is 
separated by the separator token.
ÔÅ¨
[MASK] ‚Äì During training we might want to hide tokens and predict them. These are replaced 
with the mask token
ÔÅ¨
Example: Model sequence size = 12. Input: ‚ÄúI like cake. You like cake‚Äù
‚Üí [CLS] I like cake [SEP] You like cake [SEP] [PAD] [PAD] [PAD]

[IDS] The image you see is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "Transformers_Encoder". The slide outlines the preprocessing steps involved in preparing text data for use with a transformer model. It emphasizes the importance of adding special tokens, understanding the concept of [CLS], [PAD], and [MASK] in the context of sequence modeling, and how these elements contribute to the overall learning process within the encoder component of transformer models. The slide serves as an educational tool to explain complex NLP concepts in a structured and easy-to-understand manner. [IDE]



Transformer Architecture
Fig. 1: Transformer Encoder Decoder 
(source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image depicts a Transformer Encoder architecture, which is a key component of Transformer models widely used in Natural Language Processing (NLP). The figure illustrates the structure of the encoder, emphasizing its multi-headed attention mechanism and feed-forward layers. This architecture allows the model to attend to different positions of the input sequence simultaneously, capturing long-range dependencies effectively. In an NLP lecture, this would be an important topic as it explains how such models process and understand language. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual representation of a Transformer Encoder, which is a key component in the architecture of transformer-based models used in natural language processing (NLP). In this context, input is typically a sequence of tokens, often with a dimensionality of 768 and a length of 12*64. The diagram illustrates how multi-head attention and positional encoding are integrated into the encoder to process inputs effectively. This structure is essential for tasks like translation or text generation where understanding the relationships between different parts of the input data is crucial. [IDE]



Transformer Encoder ‚Äì Input (I)
Fig. 3: Token Embeddings (source: own)
Input sentence: I like cake
Fig. 3: Token Embeddings (source: own)

[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a fundamental concept in transformer-based models: the encoder. It serves as an illustration for how input sentences are processed and converted into token embeddings, which are numerical representations used by the model to understand the meaning behind words. The yellow grid indicates the encoding process, where each cell corresponds to a word or token from the sentence "I like cake," and the figure number suggests that this is part of a larger presentation or educational material. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the 
same time. 
We need to store position info using a Positional 
Encoding
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image you see is a visual representation of the Transformer Encoder, which is a key component in the Transformer architecture commonly used in Natural Language Processing (NLP). This encoder is responsible for processing input sequences of tokens, typically words or subwords, and encoding them into a continuous representation. The figure illustrates the multi-headed attention mechanism that allows the model to focus on different parts of the input sequence simultaneously. This process enables the model to capture long-range dependencies between tokens and is crucial for tasks such as language translation, text summarization, and question answering. The accompanying text explains that the input sequence is usually of dimensionality 768 (12*64) and that the model uses all tokens in the input sequence at the same time to store position information using a positional encoding. This is essential for maintaining the order of the tokens in the sequence, as the attention mechanism does not inherently consider the token's position. [IDE]



Transformer Encoder
Fig. 4: Positional Embeddings (source: own)

[IDS] The image displays a visual representation of the Transformer Encoder, which is a key component in the Transformer model commonly used in Natural Language Processing (NLP). The Transformer Encoder is responsible for processing input sequences, such as sentences or paragraphs, and converting them into meaningful representations that can be used for tasks like language translation, sentiment analysis, or question-answering systems. In this specific visualization, we see the concept of positional embeddings being used to give the model information about the order of the input sequence. These embeddings are crucial because they allow the model to understand the context and relationships between words within a sentence, even though the input is typically fed into the model as a flat sequence of tokens without any explicit indication of their original order. [IDE]



Transformer Encoder
Fig. 5: Token + Positional Embeddings = Input Embeddings (source: own)

[IDS] The image you're seeing is a visual representation of the concept of an "Encoder" in the context of Transformers, which is a type of neural network architecture commonly used in Natural Language Processing (NLP). The Encoder is part of the Transformer model that processes input data, such as sentences or sequences of tokens, and generates embeddings. These embeddings are representations of the input data in a vector space where each token is assigned a unique vector. In this specific figure, we can see how different types of padding (PAD) are represented in the embedding space. This visualization helps us understand how the Transformer Encoder handles varying input lengths by adding padding tokens (like [SEP], [PAD]) to ensure that all inputs have the same length before being processed. This is crucial for maintaining the integrity of the sequence information during the encoding process. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the same 
time. 
We need to store position info using a Positional Encoding
ÔÅ¨Perform self-attention
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image depicts a Transformer Encoder, which is a crucial component in the architecture of transformer-based models, particularly those used in Natural Language Processing (NLP). This encoder is responsible for processing input sequences of tokens, typically in the form of embeddings. It takes these embeddings as input and outputs a sequence of tokens at the same time, which is achieved through multi-head attention mechanisms that allow the model to attend to all tokens in the input sequence simultaneously. The figure is a simplified representation of how the Transformer Encoder functions, emphasizing its role in NLP tasks such as language translation or sentiment analysis. [IDE]



Learned weight matrix
Fig. 6: Computation of query, key and value matrices (source: own)

[IDS] In the context of a NLP lecture, this image illustrates the process and components involved in understanding and interpreting text. The "Input Embeddings" section shows how words are initially represented as numerical vectors, which then go through a learned weight matrix to generate "Output Embeddings". This transformation is crucial for capturing the meaning and context of words within a sentence or text.

The "Figure 6: Computation of query, key and value matrices (source: own)" indicates that the lecture might be discussing the inner workings of attention mechanisms in models like transformers. These matrices are fundamental to how transformers encode and process information, enabling the model to focus on different parts of the input sequence when making predictions or generating outputs.

Lastly, the mention of "Hochschule Bonn-Rhein-Sieg" and "Tim Metzler" suggests that this is an educational material from a specific institution, possibly created by or attributed to Tim Metzler. This could imply that the lecture is part of a curriculum or research project at that institution. [IDE]



Query, Key, Value
One row is the embedding of one input token
Fig. 7: Query, Value, Key (source: own)

[IDS] The image is a visual representation of a key concept in the field of Natural Language Processing (NLP), specifically related to the topic of Transformers and their encoders. It illustrates the idea of embedding one input token into a matrix, which can be seen as a fundamental step in processing text data using transformer models. These models are known for their ability to capture complex relationships between words in a sentence or document. The "Query, Key, Value" components shown in the image are essential parts of the attention mechanism in transformers, which allows the model to focus on different parts of the input data when making predictions or generating outputs. The source of this visualization is indicated as "Hochschule Bonn-Rhein-Sieg Fachbereich Informatik," suggesting that this lecture slide is part of an educational resource from this institution's informatics department. [IDE]



Query, Key, Value
ÔÅ¨
Wikipedia search example:
ÔÅ¨
Query: Give me documents about a search term. The term could be ‚ÄúGerman car 
manufacturers‚Äù
ÔÅ¨
Key: The ids of the documents we want to search. Could be the page name like: 
‚ÄúMercedes-Benz‚Äù, ‚ÄúAudi‚Äù, ‚ÄúCars‚Äù, ‚ÄúPotato‚Äù, ‚Ä¶
ÔÅ¨
Value: The content of the document. Could be ‚ÄúMercedes-Benz is a German car 
manufacturer founded in 1926, ...‚Äù
ÔÅ¨
Goal: Make query and relevant keys similar. Encode query and keys as vectors. 
Take dot product. High values indicate high relevance, low values low relevance.

[IDS] The image displays a slide from an NLP lecture, focusing on the topic of "Transformers_Encoder." It explains how to perform a keyword search using Wikipedia as an example. The slide outlines the key components of such a search: the query (e.g., "German car manufacturers"), the key terms or topics of interest (e.g., "Mercedes-Benz," "Audi," "Cars," "Potato"), and the value or relevance of the document (in this case, mentioning Mercedes-Benz's founding year). The goal is to make queries and encode keywords as vectors with high relevance and low values for low relevance. This concept is fundamental in natural language processing, especially in transformer models that use encoders to process and represent text data. [IDE]



Query, Key, Value

[IDS] The image is a visual representation of the structure and function of a Transformer Encoder, which is a crucial component in transformer-based models used in Natural Language Processing (NLP). The encoder takes in input sequences, processes them through self-attention mechanisms, and produces output sequences. The diagram illustrates how queries (Q), keys (K), and values (V) are processed through multiple layers, each composed of attention and feed-forward networks. The [CLS] token at the beginning serves as a special token for classification tasks, and the [PAD] tokens likely represent padding added to ensure that all sequences have the same length. This structure is essential for understanding how transformers encode information from input data. [IDE]



Query, Key, Value

[IDS] The image represents a simplified visualization of the attention mechanism in a Transformer Encoder, which is a key component in transformer-based models used in Natural Language Processing (NLP). The Transformer Encoder utilizes self-attention to weigh the importance of different parts of the input sequence when generating the output. In this illustration, the "Query" (Q), "Key" (K), and "Value" (V) represent the three components of the attention mechanism. The colored blocks symbolize the encoding of these components from the input sequence, which are then processed through multiple layers to produce the final output. This process allows the model to focus on relevant parts of the input data while ignoring irrelevant information, leading to more accurate and contextually appropriate outputs. [IDE]



Attention
Take query vector and all key 
vectors.

[IDS] The image illustrates the process of attention mechanism in a Transformer Encoder, which is a crucial component in many NLP models. It shows how queries (Q), keys (K), and values (V) are used to compute attention scores, allowing the model to focus on different parts of the input sequence when making predictions. The color-coded representation helps in understanding the different roles of Q, K, and V vectors in the attention mechanism. [IDE]



Attention Scores
Take query vector and all key 
vectors.
Build dot product (Q*KT)

[IDS] The image represents a slide from an NLP lecture focused on the Transformer Encoder. It illustrates the concept of Attention Scores, which is a crucial component of the Transformer architecture, particularly in models like BERT, GPT, and XLNet. The attention scores determine how much weight each part of the input sequence should have when the model is generating the output. The slide visually breaks down the process into two main parts: the initial attention scores calculation and the final query vector formation. The initial attention scores are represented as a grid, where each cell corresponds to the interaction between a query (Q) and a key (K). These scores are then used to form a query vector and all key vectors, which are essential for the next steps in the Transformer Encoder's process. This visual aid helps students understand the complex mechanism behind attention mechanisms in NLP models. [IDE]



Attention Scores
Take query vector and all key 
vectors.
Build dot product (Q*KT)
How relevant is the token ‚ÄúI‚Äù to the CLS token?

[IDS] The image represents a visual explanation of how attention scores are used in the Transformer Encoder architecture. The Transformer Encoder is a crucial component of transformer models, which are widely used in Natural Language Processing (NLP) tasks. The attention mechanism allows the model to weigh the importance of different parts of the input sequence when making predictions or generating outputs.

In this particular illustration, we see an example of how attention scores are calculated for a given query vector and key vectors. The scores are then used to build a dot product (Q*K^T), which influences the final output of the transformer model. The lower part of the image shows an example of how the attention scores are applied to a sentence, highlighting the relevance of certain tokens (like "I" and "the") over others.

Understanding the concept of attention scores and their role in the Transformer Encoder is essential for grasping the workings of state-of-the-art NLP models. This knowledge is valuable for both researchers working on improving these models and practitioners applying them in real-world applications. [IDE]



Masking

[IDS] The image illustrates a concept from the field of Natural Language Processing (NLP), specifically related to transformer encoders. The key focus is on the masking technique used in transformer models, particularly during pre-training. This process involves covering parts of the input sequence with special tokens like [CLS] and [PAD], which serve as placeholders for missing information or padding. These masks are essential for training models that can predict missing words or handle sequences of different lengths. The image provides a visual representation of how this technique is applied to the input sequence "I like cake," showing the masked version as "[CLS] K I like V [PAD] [PAD] [PAD]". The lecture likely explains the benefits of this technique, such as allowing the model to learn without relying on the exact word order and improving its ability to generalize to unseen data. [IDE]



Masking
Set to -infinity to mask out

[IDS] The image represents a concept in NLP known as "Masking," specifically related to the Transformer Encoder architecture. In this context, masking is used to create a target sequence for training sequence-to-sequence models like translation or summarization. The goal is to generate a target sequence that corresponds to the input sequence, but with some information removed (masked) to challenge the model and improve its ability to predict the missing information. This process is crucial for training effective models that can handle real-world tasks where not all information is available. The diagram visually demonstrates how certain positions in the input sequence are masked, leading to an incomplete sequence that the model must learn to predict. [IDE]



Masking
-infinity becomes 0. Scores sum up to 1.

[IDS] The image is a visual representation of the masking technique used in Transformer models, specifically within the encoder component. This technique is crucial for preventing the model from seeing the future tokens during training, which would make it impossible to learn any form of sequence modeling. The mask is applied to the attention mechanism of the Transformer, where the masked positions are replaced with a special token (often referred to as the 'pad' token) and the original values are stored elsewhere. During the training phase, the model learns to predict these masks based on the context provided by the surrounding tokens. The softmax function is then applied to ensure that the probabilities sum up to 1. This process is essential for training transformer-based language models like BERT, GPT, and others, which rely on self-attention mechanisms to understand the context of words within sentences. [IDE]



Masking
-infinity becomes 0. Scores sum up to 1.

[IDS] The image illustrates a key concept in the context of NLP, specifically related to the Transformer Encoder architecture. It demonstrates the process of masking in the attention mechanism of the Transformer model. The attention mechanism is crucial for understanding the context of words in a sequence and assigning importance to them when generating translations or performing other tasks. By masking certain positions in the input sequence, the model can learn to focus on relevant information without being influenced by the context provided by future tokens. This is essential for training models that need to predict or generate sequences where the order of tokens matters. In this lecture, you would learn how this masking process helps in preventing the model from looking at future tokens during training, which is a critical step in making the model more robust and capable of handling real-world NLP tasks. [IDE]



Attention: Embeddings
Embedding for token 
[CLS]. Incorporates 
information about all 
other tokens.

[IDS] The image represents a concept in the field of Natural Language Processing (NLP), specifically related to transformer encoders. These encoders are part of the architecture of transformer models, which are widely used in NLP tasks such as language translation, sentiment analysis, and text summarization. The figure illustrates how embeddings for tokens, like 'Hochschule' or 'Bonn-Rhein-Sieg', are processed by a transformer encoder. This process involves stacking layers of attention mechanisms and feed-forward networks to capture the context and meaning of the input tokens. The embeddings are incorporated into the model to provide information about all other tokens, enabling the model to understand the relationships between words and phrases in the input text. [IDE]



Attention: Embeddings
Repeat for all tokens in the input sequence.

[IDS] The image illustrates the concept of embeddings in a transformer encoder, which is a key component in natural language processing (NLP) models. Embeddings are representations of words or tokens as vectors in a high-dimensional space, allowing the model to capture semantic relationships between words. The transformer encoder uses self-attention mechanisms to process these embeddings and generate contextually relevant representations. The repeating pattern of embeddings in the input sequence is crucial for the transformer encoder to understand the meaning of each token within its context. This visualization aids in understanding how embeddings contribute to the performance of transformer-based NLP models. [IDE]



Matrix View

[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP) with the title "Matrix View". The content suggests that the lecture is focused on explaining how transformers, specifically the encoder part of a transformer model, process and understand language. The encoder is a crucial component in transformer models as it encodes input sequences into continuous representations that are used for further processing or to generate outputs. The mention of Hochschule Bonn-Rhein-Sieg indicates that this lecture might be taking place at that institution. The names Tim Metzler and Fachbereich Informatik suggest that Tim Metzler is likely the instructor, and the faculty of computer science is involved in teaching this course. [IDE]



Fig. 8a: Computation of relevancy scores (query * key) (source: own)

[IDS] The image represents a conceptual diagram related to the topic of Transformers and their Encoder architecture, which is a key component in NLP models like BERT. The Encoder is designed to process input sequences, such as words or tokens, and generate continuous representations for each token. In this diagram, the matrix labeled 'K' likely represents the key vectors, which are part of the attention mechanism within the Transformer Encoder. These key vectors are compared with query and value vectors to compute the attention scores, allowing the model to focus on different parts of the input sequence when making predictions. The figure serves as an educational tool to help students understand how the Transformer Encoder processes input data and computes relevance scores, which is essential knowledge in the field of Natural Language Processing (NLP). [IDE]



‚óèEach element tells us 
how relevant each token 
is for the query ‚Äúcake‚Äù.
Fig. 8b: Computation of relevancy scores (query * key) (source: own)

[IDS] The image is a visual representation of the concept of relevance scores in transformer encoders, which are commonly used in natural language processing (NLP). These encoders help in understanding the context and relevance of each token in a query with respect to a specific keyword. The diagram illustrates how a given keyword, "cake," is compared against different tokens in a query, such as "like cake," "SEP like cake," and "PAD like cake." Each token is assigned a relevance score based on its relevance to the keyword. This process is crucial for tasks like information retrieval, where determining the relevance of each document or text snippet to a user's query is essential. In NLP lectures, this would be a fundamental concept taught, as it is a core component of transformer-based models that power many state-of-the-art language models and applications. [IDE]



‚óèEach element tells us 
how relevant each token 
is for the query ‚Äúcake‚Äù.
‚óè[PAD] token should be 
irrelevant
Fig. 8b: Computation of relevancy scores (query * key) (source: own)

[IDS] The image represents a fundamental concept in the field of Natural Language Processing (NLP) known as "Computation of relevance scores" or "query * key" multiplication. This process is crucial for understanding how transformers, specifically their encoder components, work. In the context of a transformer encoder, each token in the input sequence is compared to every other token in the query sequence using attention mechanisms. The resulting relevance scores determine the importance of each token in relation to the others. These scores are then used to weigh the information from the input sequence and contribute to the final output of the transformer model. The figure visually demonstrates the interaction between the input tokens and the query tokens, highlighting the significance of this computation in NLP tasks such as language translation, question-answering systems, and text generation. [IDE]



Fig. 9: Masking out the padding tokens (source: own)

[IDS] The image represents a key concept in the field of NLP, specifically related to the architecture of Transformer models. The diagram illustrates how the padding tokens are masked out during the training process, which is crucial for training models that process sequences of varying lengths. In the context of a Transformer Encoder, this masking technique allows the model to learn to focus on the relevant parts of the input sequence without being influenced by the padding. Understanding this process is essential for building robust NLP models that can effectively handle sequences of different lengths. [IDE]



Fig. 10: Masking out the padding tokens (II) (source: own)

[IDS] The image illustrates a key concept in NLP, specifically related to transformers and their encoders. It shows the process of masking out padding tokens from an input sequence during the encoding process. The Q*KT^T + mask equation represents the final step where the query-key attention matrix is multiplied by its transpose and then added to the masked padding tokens. This technique is crucial for handling variable-length sequences in transformer models, ensuring that the model's attention mechanism is not affected by the padding. The figure is a visual representation of the padding handling process, emphasizing the importance of excluding padding information from the model's processing to maintain accurate and efficient results. [IDE]



softmax (QK T)
‚àödQ
Fig. 11: Attention scores. Each row sums up to 1. (source: own)

[IDS] The image is a visual representation of the attention mechanism in transformer models, specifically focusing on the encoder part. It illustrates how attention scores are calculated and then used to weigh the importance of each input token when generating an output sequence. The figure demonstrates the process of dividing by the square root of the sequence length (d_k) and softmax normalization to ensure that the attention weights sum up to 1. This is a crucial aspect of transformer encoders, as it allows the model to focus on different parts of the input in different ways, depending on their relevance to the task at hand. Understanding this mechanism is vital for grasping how transformer-based models, like BERT or GPT, can process and understand natural language effectively. [IDE]



Fig. 12: Context Embeddings (source: own)

[IDS] The image appears to be a visual representation of attention scores within the context of Natural Language Processing (NLP), specifically related to Transformers and their encoders. In NLP, transformers are a type of neural network architecture that have achieved state-of-the-art results on various tasks. The encoder is a crucial component of the transformer model, responsible for processing input sequences and generating representations.

The attention scores shown in the image are likely part of the self-attention mechanism used by transformers. This mechanism allows the model to weigh the importance of different parts of the input sequence when generating its representations. The scores are represented in a matrix format with different values indicating the level of attention given to each part of the sequence by the model.

The presence of the Hochschule Bonn-Rhein-Sieg logo suggests that this image might be from a lecture or presentation at that institution. The mention of "Context Embeddings" and "Attention Scores" indicates that the topic of the lecture is focused on understanding how these components contribute to the performance of transformer-based models in NLP tasks.

In summary, the image serves as a visual aid to help students or participants in the lecture understand the concept of attention scores and their role in transformer encoders within the context of NLP. [IDE]



Is one set of attention weights enough?

[IDS] The image is likely a slide from a lecture on Natural Language Processing (NLP). It poses the question, "Is one set of attention weights enough?" which could be referring to the mechanism used by transformer models in NLP. Transformers use attention mechanisms to weigh the importance of different parts of the input data when generating outputs. The presence of logos suggests that this lecture might be part of a collaboration between Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik Tim Metzler. [IDE]



In practice we might focus on several relationships.
One view could be ‚Äúnext word‚Äù.
One view could be ‚Äúsubject ‚Üî object‚Äù

[IDS] In the context of a NLP (Natural Language Processing) lecture, the image likely represents a slide or presentation discussing the concept of transformers and encoders in machine learning models. The text on the slide emphasizes the importance of focusing on several relationships between words in natural language processing tasks. It suggests that one view could be the "next word," while another perspective could be seen as "subject" and "object." This is related to how transformer-based models like BERT (Bidirectional Encoder Representations from Transformers) process and understand language by considering different aspects of word relationships. The presence of logos from Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik indicates that this lecture might be part of a computer science or information technology program at that institution. [IDE]



[CLS] I like cake [PAD] [PAD] [PAD]
For ‚ÄúI‚Äù we could give a lot of weight to ‚Äúlike‚Äù because it is the 
next word.
We could also give a lot of weight to ‚Äúcake‚Äù since it is the 
object.

[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a discussion about the use of the word "like" in English grammar. It seems to be emphasizing the importance of understanding how prepositions such as "like" can influence the meaning of a sentence. In NLP, this would be relevant to how language models process and understand the nuances of human communication. The lecture might also touch upon the subject of transformers and encoders, which are key components in NLP models that help in processing and generating text. [IDE]



Solution: Have more ‚Äúattention heads‚Äù to capture different 
relationships.
Final embedding is concatenation of all ‚Äúattention heads‚Äù

[IDS] In the image, you can see a slide from a lecture on Natural Language Processing (NLP). The topic of this particular slide is about "Transformers_Encoder," which is a component of transformer models used in NLP tasks. The slide emphasizes the importance of having multiple "attention heads" to capture different relationships within the data. It also mentions that final embedding is achieved by concatenating all these attention heads. The slide includes logos and names of the institutions involved, such as Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, along with the name Tim Metzler, who may be the presenter or author of the lecture content. [IDE]



Fig. 13: Multi-Head Attention (source: own)

[IDS] The image you're seeing is a visualization of the Multi-Head Attention mechanism in a Transformer Encoder, which is a fundamental component in modern natural language processing (NLP) models. The Transformer Encoder uses self-attention to allow each position in the sequence to attend to all positions, weighing them differently to compute a representation of the sequence suitable for sequential data like text or speech. The heads in the figure represent different attention mechanisms working in parallel to capture various aspects of the input data. Understanding this concept is crucial for grasping how NLP models process and interpret human language. [IDE]



Fig. 14: Context Embeddings. Concatenate for each head (source: own)

[IDS] In the context of NLP (Natural Language Processing), the image represents a concept from the field of transformer models, specifically the encoder part. The encoder is a key component in transformer-based architectures, like BERT or GPT, which are widely used for various NLP tasks such as language translation, sentiment analysis, and text classification. It's designed to process input sequences and produce context-dependent embeddings that capture the meaning and relationships within the text. The visualization helps illustrate how different parts of an input sequence can influence each other through attention mechanisms, allowing the model to focus on relevant parts of the context when generating embeddings. This understanding is crucial for building effective NLP systems that can handle complex language tasks. [IDE]



Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)

[IDS] The image represents a visualization of the attention mechanism in a Transformer encoder, which is a core component of models like BERT and GPT. The Transformer encoder processes sequences of tokens (like cake) and assigns weights to their importance during the encoding process. The blue bars signify the attention weights, indicating how much each token is paying attention to others in the sequence. This is crucial for understanding context in natural language processing tasks. [IDE]



Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)

[IDS] The image represents a visualization of attention mechanisms in transformer models, specifically focusing on the encoder part. It shows how different tokens in a sequence, like "I like cake" and "SEP," interact with each other through their attention weights. The attention mechanism is crucial for understanding the context in which words are used in language processing tasks. In this diagram, the tokens are connected by lines that represent the strength of their attention to one another, with darker lines indicating stronger connections. This is an essential concept in NLP lectures, as it helps in understanding how transformers can capture dependencies between words in sentences or sequences. [IDE]



Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)

[IDS] The image illustrates the attention mechanism in a Transformer Encoder, which is a key component of transformer-based models used in NLP. The diagram shows how the model processes sequential data, such as words in a sentence, and assigns importance to each word when making predictions or understanding the context. The attention scores, represented by the lines connecting the words, indicate how much each word influences the representation of the next word in the sequence. This is crucial for capturing long-range dependencies in text data, allowing the model to better understand the meaning and context of the input. In our NLP lecture, this would be an important topic as it's a fundamental concept behind many state-of-the-art NLP models. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the same time. 
We need to store position info using a Positional Encoding
ÔÅ¨Perform self-attention
ÔÅ¨Add the output to the context embeddings.
Normalize to make sure the numbers in the embeddings don‚Äôt grow 
too much.
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual representation of the Transformer Encoder, which is a crucial component in the Transformer architecture commonly used in Natural Language Processing (NLP). The Transformer Encoder is designed to process sequences of tokens, such as words or characters, and it plays a key role in understanding the context and meaning within these sequences. In NLP lectures, this figure would likely be used to explain how the Transformer Encoder works by breaking down input into smaller chunks, encoding each chunk, and then combining them to form a contextualized representation. This is essential for tasks like language translation, question answering, and text summarization. [IDE]



+
Normed Sum of Embeddings
Fig. 15: Add and normalize (source: own)

[IDS] The image illustrates the concept of embeddings in a Transformer Encoder, which is a crucial component in NLP models. It demonstrates how input embeddings are combined and normalized to produce a 'Normed Sum of Embeddings'. This process is essential for capturing the semantic meaning of words and phrases, enabling the model to understand and generate human-like text. The visual representation aids in understanding how these embeddings are transformed and processed within the encoder, highlighting the complexity and beauty of NLP techniques. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the same time. 
We need to store position info using a Positional Encoding
ÔÅ¨Perform self-attention
ÔÅ¨Add the output to the context embeddings.
Normalize to make sure the numbers in the embeddings don‚Äôt grow too 
much.
ÔÅ¨Feed to a feed forward layer and add and normalize again
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In this NLP lecture, we are discussing the Transformer Encoder, a crucial component of the Transformer architecture used in tasks such as machine translation and text generation. The Transformer Encoder is designed to process input sequences of tokens by attending to different positions in the sequence simultaneously. It uses self-attention mechanisms to weigh the importance of each position relative to others. The figure illustrates the structure of the Transformer Encoder with its multi-head attention and positional encoding layers. The goal is to learn representations that capture the dependencies between tokens in the input sequence effectively. [IDE]



Normed Sum of Embeddings
Fig. 16: Encoder Output. Feed forward layer adds non linearity to the network (source: own)

[IDS] The image depicts the Encoder part of a Transformer model, which is a fundamental component in the field of Natural Language Processing (NLP). The Encoder is responsible for processing input sequences, such as sentences or paragraphs, and generating a continuous representation known as the "sum of embeddings." This process involves multi-head self-attention mechanisms that allow the model to capture dependencies between different parts of the input sequence. The resulting embeddings are then passed through feed-forward layers to further transform them. In this diagram, the Encoder is shown to consist of multiple layers, each containing attention and feed-forward components, with the output of the Encoder being fed back into it, indicating the self-attention mechanism. This architecture enables the Transformer model to understand the context and relationships within the input text, which is crucial for tasks such as language translation, question answering, and text summarization. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the same time. 
We need to store position info using a Positional Encoding
ÔÅ¨Perform self-attention
ÔÅ¨Add the output to the context embeddings.
Normalize to make sure the numbers in the embeddings don‚Äôt grow too much.
ÔÅ¨Feed to a feed forward layer and add and normalize again
ÔÅ¨Repeat N times to build deeper representations
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] In the context of a NLP lecture, the image represents a Transformer Encoder, which is a key component in transformer-based models used for natural language processing tasks. The diagram illustrates how input tokens are sequenced and processed through multiple layers to capture positional information and perform self-attention. This process involves encoding the input sequence, adding position embeddings, multi-head attention, and feed-forward layers to generate representations that capture the meaning and context of the text. The figure serves as an educational tool to help students understand the architecture and functionality of transformer encoders in NLP. [IDE]



Transformer Encoder
ÔÅ¨Input is a sequence of token embeddings
Usually of dimensionality 768 (12*64)
For our examples we will use 16
ÔÅ¨Model takes all tokens in the input sequence at the same time. 
We need to store position info using a Positional Encoding
ÔÅ¨Perform self-attention
ÔÅ¨Add the output to the context embeddings.
Normalize to make sure the numbers in the embeddings don‚Äôt grow too much.
ÔÅ¨Feed to a feed forward layer and add and normalize again
ÔÅ¨Repeat N times to build deeper representations
Fig. 2: Transformer Encoder (source: Attention is all you need. 
Vaswani et al. 2017)

[IDS] The image is a visual representation of a Transformer Encoder, which is a crucial component in the architecture of transformer models used in Natural Language Processing (NLP). The Transformer Encoder is designed to process input sequences of tokens and generate a positional encoding to retain the order information. It consists of multiple layers that perform self-attention mechanisms to understand the context of the input. By repeating these layers and adding feed-forward layers, the model can learn complex representations of the input data. This is essential for tasks like language translation, where the order of words is vital to understanding the meaning of sentences. The diagram serves as a fundamental explanation of how transformers encode and process information, which is a key concept in NLP. [IDE]



BERT
Bidirectional Encoder Representations 
from Transformers
ÔÅ¨
Developed at Google in 2018 by Jacob Devlin et.al.
ÔÅ¨
Builds context dependent embeddings for tokens in sentences
ÔÅ¨
Uses the Transformer architecure
ÔÅ¨
Utilizes Self-Attention

[IDS] The image is a slide from an NLP (Natural Language Processing) lecture that focuses on the BERT model and its Bidirectional Encoder Representations from Transformers (BERT-Encoders). It highlights the development of BERT by Jacob Devlin et.al. in 2018 at Google, emphasizing its ability to build context-dependent embeddings for tokens in sentences. The slide also mentions that BERT uses the Transformer architecture and utilizes self-attention. The logos at the bottom indicate that the lecture is part of the Hochschule Bonn-Rhein-Sieg and is presented by Tim Metzler. [IDE]



BERT
Fig. 17: BERT architecure (I) (source: own)

[IDS] The image is a visual representation of the BERT (Bidirectional Encoder Representations from Transformers) model, which is a popular architecture in the field of Natural Language Processing (NLP). BERT stands for Bidirectional Encoder Representations from Transformers and it's a type of transformer-based neural network. It uses an encoder to process sequences of tokens, both left-to-right and right-to-left, allowing it to take into account context from both sides of a word or phrase. The goal of BERT is to improve the performance of NLP tasks by providing better representations of words in context.

In the diagram, we can see the different components of BERT, such as the masked sentence A and B pair, which are used during pre-training to teach the model about the context of words. The "NSP" and "Mask LM" arrows represent two key training tasks: Next Sentence Prediction and Masked Language Modeling, respectively. These tasks help the model understand relationships between sentences and predict missing words in a given context.

The figure caption mentions "BERT architecture (l) (source: own)" indicating that this specific visualization might be based on the creator's interpretation or adaptation of the original BERT architecture. The source "own" suggests that the creator of this image has made some modifications or additions to the original BERT architecture diagram.

Given that we are in a NLP lecture, this image serves as an educational tool to help students understand how BERT works and its role in improving NLP tasks. It provides a clear and concise overview of the BERT model and its components, making it easier for learners to grasp the concept and apply it in their studies or projects. [IDE]



BERT
Fig. 18: BERT architecure (II) (source: https://peltarion.com/blog/data-
science/self-attention-video)

[IDS] The image is a visual representation of the BERT architecture, which is a state-of-the-art language model developed by Google. It's based on a multi-layer bidirectional transformer encoder and has achieved impressive results in various natural language processing tasks. In the context of our NLP lecture, this diagram helps illustrate how BERT processes input data and generates contextualized representations for each token in the input sequence. These representations are then used for downstream NLP tasks such as sentiment analysis or question answering. [IDE]



BERT
Training Objective
ÔÅ¨
Trained on two tasks:
ÔÅ¨
Masked language model
ÔÅ¨
Next Sentence Prediction

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) focusing on the topic of "Transformers_Encoder." The slide highlights the training objective of BERT, which stands for Bidirectional Encoder Representations from Transformers. It emphasizes that BERT is trained on two tasks: masked language model and next sentence prediction. This indicates that BERT has been designed to learn representations of text by predicting missing words in sentences and understanding the relationship between sentences. The presence of logos suggests that this lecture might be part of a course or program offered by the Hochschule Bonn-Rhein-Sieg and involves instructors such as Tim Metzler. [IDE]



BERT
Masked Language Model
ÔÅ¨
Take the final context embedding for each masked output [MASK]
ÔÅ¨
Predict by feeding this to a simple classifier that predicts the token that was masked out
ÔÅ¨
Next Sentence Prediction: Given two sentences, predict if they are in the correct order. 
Add a segment embedding to the sentences, one indicating sentence one, one 
sentences two.
Predict by feeding the output of the [CLS] token to a simple binary classifier (0 ‚Üí 
sentences in order, 1 ‚Üí sentences out of order)

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) focusing on the BERT masked language model. The slide explains how BERT works by first taking the final context embedding for each masked output, then predicting the masked token using a simple classifier, and finally predicting the next sentence based on the given two sentences. This process involves feeding the output of the classifier to a simple binary classifier to determine if the predicted sentences are in the correct order. [IDE]



BERT
Next Sentence Prediction
ÔÅ¨
Given two sentences, predict if they are in the correct order. 
ÔÅ¨
Add a segment embedding to the sentences, one indicating sentence one, one 
sentences two.
ÔÅ¨
Input Embeddings are now:
token embedding + position embedding + segment embedding
ÔÅ¨
Predict by feeding the output of the [CLS] token to a simple binary classifier (0 ‚Üí 
sentences in order, 1 ‚Üí sentences out of order)

[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of "BERT Next Sentence Prediction." BERT stands for Bidirectional Encoder Representations from Transformers, which is an AI model developed to understand the context of words in a sentence. The slide explains that given two sentences, the task is to predict if they are in the correct order by adding a segment embedding to one sentence and indicating which sentence is the first. It also mentions that input embeddings for tokens are now combined with position embeddings, and that the output of the [CLS] token is used as a simple binary classifier to determine the order of sentences. This process involves feeding the first sentence into the model, followed by the second sentence, and then the model predicts if the order is correct based on the [CLS] token's output. [IDE]
[Lecture End]

