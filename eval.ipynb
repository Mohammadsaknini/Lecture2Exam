{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinas/Documents/notes/s2/nlp/nlp_project/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction_into_nlp\n",
      "['[Question Start]What are the common challenges in NLP, and how do researchers attempt to overcome these issues?[Question End]\\n', '\\n', '[Question Start]Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident? [Question End]\\n', '\\n', '[Question Start]How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?[Question End]\\n', '\\n', '[Question Start]Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\\n', '\\n', 'A) Text classification for spam filtering\\n', 'B) Generating coherent text based on a given context\\n', 'C) Turning sound into written text (Speech-to-text)\\n', 'D) The automatic creation of visual art from textual prompts\\n', '(Note: While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content is overstated and remains in the realm of science fiction for now.)[Question End]\\n', '\\n', '[Question Start]Given the following code snippet from StopWordKeywordExtractor class:\\n', '\\n', '```python\\n', 'class StopWordKeywordExtractor:\\n', '    # ...\\n', '    def is_proper_word(self, token:str) -> bool:\\n', \"        ''' \\n\", '        Checks if the word is a proper word by our definition \\n', '        Arguments: \\n', '            token     -- The token as a string \\n', '        Return: \\n', '            is_proper -- True / False\\n', \"        '''\\n\", \"        match = re.search(r'\\\\b(\\\\w{2,})\\\\b', token)\\n\", '        return match and token == match[0]\\n', '```\\n', '\\n', \"Question: What is the purpose of the regular expression `r'\\\\b(\\\\w{2,})\\\\b'` in the is_proper_word function?[Question End]\"]\n",
      "Text_processing\n",
      "['[Question Start]In the context of BPE tokenization, what is the significance of merging less frequent pairs before more frequent ones during the training phase?[Question End]\\n', '\\n', '[Question Start]What role do morphemes play in subword tokenization methods like BPE and why are they often represented as part of a token in text processed by such algorithms? [Question End]\\n', '\\n', '[Question Start]In the context of NLP, how might sentence segmentation impact the effectiveness of a byte pair encoding token learner and what precautions can be taken to avoid any negative impact of mis-segmented sentences on the learning process?[Question End]\\n', '\\n', '[Question Start]What is the role of the Byte Pair Encoding (BPE) token learner algorithm in text preprocessing for NLP?\\n', 'A) It separates punctuation from words.\\n', 'B) It normalizes case folding, making everything lowercase or uppercase as needed.\\n', 'C) It learns subword tokens that can represent frequent word subparts and are often morphemes\\n', 'D) It performs full stemming of the words in a corpus.[Question End]\\n', '\\n', '[Question Start]Based on the function `get_splits`, what does the dictionary created by this function represent? Can you provide an example of a key and its associated value in the output of this function for the input `pretokenize([[\"apple\", \"banana\", \"apple\"], [\"apple\"]])`?\\n', '\\n', 'Code Snippet:\\n', '```python\\n', 'from collections import Counter\\n', 'from typing import Dict, Tuple\\n', '\\n', 'def get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\\n', '    \"\"\"Get subword splits of tokens in a corpus.\\n', '    \\n', '    Args:\\n', '        corpus (List[List[str]]): A list of sentences where each sentence is represented\\n', '            as a list of tokens.\\n', '\\n', '    Returns:\\n', '        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\\n', '            and values are the counts of occurrences of those splits in the corpus.\\n', '    \\n', '    Example: \\n', '        >>> corpus = [[\"apple\", \"banana\", \"apple\"], [\"apple\"]]\\n', '        >>> get_splits(corpus)\\n', \"        {('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\\n\", '```[Question End]']\n",
      "Transformer_decoder_and_Large_Langauge_Models\n",
      "[\"[Question Start]In the context of large language models, what is the purpose of fine-tuning with human feedback and how does it improve a model's performance?[Question End]\\n\", '\\n', '[Question Start]What are the potential challenges associated with creating high-quality datasets for training transformer decoders, as depicted in the process by OpenAI in 2021?[Question End]\\n', '\\n', '[Question Start]How can multi-task learning be harnessed when training large language models to generate more coherent and human-like text?[Question End]\\n', '\\n', '[Question Start]What was the foundational paper in the field of transformers that sparked research into their applications within natural language processing?[Question End]\\n', '\\n', '[Question Start]Which of the following statements is true regarding the process discussed in \"Learning to summarize from human feedback\" by Stiennon et al (OpenAI), 2022?\\n', 'A) The researchers only used non-expert judges for evaluation.\\n', 'B) The summaries were not posted on a public platform for evaluation but assessed solely in-house.\\n', 'C) Both expert and non-expert judges evaluated the summaries, giving them rewards based on quality as perceived by their respective levels of expertise. \\n', 'D) The model was only fine-tuned once after collecting the initial batch of human feedback.[Question End]']\n",
      "FineTuning_of_LLMs\n",
      "['[Question Start]What is the potential issue with a low-rank assumption in LoRa, and how might it impact model performance?[Question End]\\n', '\\n', \"[Question Start]In few-shot learning, what's the difference between zero-shot and few-shot prompting, and why might one be more effective than the other for certain tasks? [Question End]\\n\", '\\n', '[Question Start]How does prefix tuning differ from parameter-efficient fine-tuning methods like LoRa, in terms of their approach to updating parameters while maintaining model efficiency?[Question End]\\n', '\\n', '[Question Start]In an NLP context, what are some potential advantages and disadvantages of fine-tuning a large pre-trained LLM on a specific task rather than starting with a model trained directly for the target task from scratch?[Question End]\\n', '\\n', '[Question Start]What is the low-rank assumption in LoRa (Low-Rank Adaptation of Large Language Models) and how does it help with fine-tuning these models?\\n', 'A) It assumes that only a few new parameters are needed to adapt the model, reducing the computational resources required during training.\\n', 'B: It assumes the update weight matrix contains just as much information as the original weights, requiring no reduction in dimensionality or computation.\\n', 'C: It assumes the update weight matrix is of higher rank than the original weights, actually increasing the complexity and parameters of the fine-tuned model. \\n', \"D: It makes no assumptions about the weight matrices' ranks, making the approach computationally intensive like a full parameter update.[Question End]\"]\n",
      "Retrival_augemented_generation\n",
      "['[Question Start]What are the key differences between a retrieval-based approach and a keyword search in an NLP context, especially in relation to the RAG system? [Question End]\\n', '\\n', '[Question Start]How can multi-armed bandit theory be leveraged in hybrid document retrieval for a RAG system, given user feedback on relevancy of retrieved documents?[Question End]\\n', '\\n', '[Question Start]In a corporate environment, what are some potential challenges or special considerations when implementing an NLP RAG system and how might these require adaptations to the standard approach? [Question End]\\n', '\\n', '[Question Start]How can intermediate queries improve the effectiveness of the interaction between users, document retrieval, and LLMs in a RAG system?[Question End]\\n', '\\n', '[Question Start]What is the primary motivation behind using a Retrieval-Augmented Generator (RAG) in natural language processing, and what key components does it consist of?\\n', 'A) To reduce computation time and resources; vector search and document retrieval\\n', 'B) To increase context length and handle larger datasets; LLMs and multi-head attention\\n', 'C) To improve the understanding of visual content and tables in documents; computer vision models and table parsing algorithms\\n', 'D) To personalize text generation for different users; user modeling and reinforcement learning[Question End]']\n",
      "Statistical_language_models\n",
      "['[Question Start]What is the intuition behind the smoothing techniques in statistical language modeling, and how do they help with the sparsity issue of n-gram models?[Question End]\\n', '\\n', '[Question Start]How does the use of caching models impact speech recognition tasks as opposed to text-based applications? [Question End]\\n', '\\n', '[Question Start]How can N-gram probabilities be combined through interpolation, and why might simple linear interpolation not always provide the best results in practice?[Question End]\\n', '\\n', '[Question Start]What is the primary purpose of smoothing techniques in statistical language models? \\n', 'A) To improve the probability assigned to unseen n-grams during evaluation on test sets\\n', 'B) To reduce the complexity of the model for more efficient processing\\n', \"C) To increase the likelihood that all words are part of the model's vocabulary\\n\", 'D) To specifically weight more recently used words as more likely to appear next in a text[IDE][Question End]\\n', '\\n', '[Question Start]Question:In the lecture content, a bigram model is built for text from the movie *Spider-Man Homecoming*. The `bigram_probability` method in the BigramModel class calculates the probability of one word given another. Can you explain, step-by-step, how the probabilities are calculated and what data structures are used to enable quick access to these probabilities?[Question End]']\n",
      "Static_embedding_OneHot_encoding\n",
      "[\"[Question Start]What is the main purpose of converting words into vector representations in natural language processing, and why can't machine learning models work directly with text data? [Question End]\\n\", '\\n', '[Question Start]Why might one-hot encoding not always be an ideal representation for word embeddings, and what are some alternative methods to capture semantic meaning in vectors?[Question End]\\n', '\\n', '[Question Start]In the context of NLP, why is it important for a model to understand the similarity between words, and how does representing words as vectors help with this task?[Question End]\\n', '\\n', '[Question Start]What is the primary purpose of one-hot encoding in NLP?\\n', 'A) To reduce dimensionality of word embeddings\\n', 'B) To convert categorical data into a usable format for machine learning algorithms\\n', 'C) To quantify the semantic similarity between words in vector space\\n', 'D) To create a sparse representation of text documents for improved computational efficiency[Question End]\\n', '\\n', '[Question Start]**Question:**\\n', 'In the `TfIdfModel` class, the method `build_index` is implemented to create a dictionary that maps each word type in the documents to an index. The same functionality was already implemented in the `OneHotModel` class and can be reused there. Should you copy that implementation over or write your own? What are the trade-offs between these two decisions, considering the need for maintaining the code DRY while also avoiding unnecessary complexity due to shared code with a different model?[Question End]']\n",
      "Static_embedding_TFIDF\n",
      "['[Question Start]What is the purpose of normalizing term frequencies in the TF-IDF method?[Question End]\\n', '\\n', '[Question Start]In the given example, what are the calculated TF-IDF values for \"a\" and \"linux\", respectively? [Question End]\\n', '\\n', '[Question Start]Why would a term with an inverse document frequency of 0 not be useful for distinguishing between documents in this context?[Question End]\\n', '\\n', \"[Question Start]What is the effect of a high term frequency and low document frequency on a word's TF-IDF value in a text corpus?\\n\", 'A) The value will be low, as high tf and low df could indicate the word is common within a single document but not very important across the entire corpus.\\n', 'B) The value will be moderate, as the high tf somewhat offsets the low df.\\n', 'C) The value will be high, as a high tf and a low df may suggest the word is particularly significant to one specific document in the corpus.\\n', \"D) It's impossible to say without more information on the inverse document frequency values.[Question End]\\n\", '\\n', '[Question Start]Based on the given excerpt, a potential question could be: \\n', '\\n', \"In the 'TfIdfModel' class, what is the purpose of the 'df_vector' and how is it computed from the 'term_doc_matrix'? \\n\", 'Code snippet for reference:\\n', '```python\\n', 'df_vector = np.zeros(len(self.index))\\n', '# Fill the matrix and compute the tfidf matrix\\n', 'raise NotImplementedError()\\n', '```[Question End]']\n",
      "Static_embedding_word2Vec\n",
      "['[Question Start]What is the main difference between the traditional approach to word embeddings and the Negative Sampling Skip Gram Model in terms of computational time? [Question End]\\n', '\\n', '[Question Start]In the context of Word2Vec, why is it crucial for the model to incorporate negative examples along with positive examples during training?[Question End]\\n', '\\n', \"[Question Start]How does the frequency-based negative sampling method help improve the Word2Vec model's understanding of word relationships and context within a text corpus?[Question End]\\n\", '\\n', '[Question Start]Question: In the context of Word2Vec and negative sampling, what is one primary reason for creating synthetic negative examples during training?\\n', 'A) To outweigh the number of positive examples in a dataset\\n', 'B) To ensure that the model isn\\'t \"tricked\" by only learning from positive examples \\n', 'C) To make the most of sparse data where there are limited negative examples available\\n', 'D) To artificially boost the performance of the classifier on test data[IDE][Question End]\\n', '\\n', '[Question Start]### Question:\\n', \"In the given lecture content, there's a function called `bagOfWords` that is used to create document embeddings. It takes in a model and a document as input and returns the document embedding by summing up word vectors. Can you explain why taking the mean of the word vectors would not work here?[Question End]\"]\n",
      "Static_embeding_Vector_semeantic_embeddings\n",
      "['[Question Start]How does the parallelogram method work in solving analogies, and why is it not effective for all types of analogical relations?[Question End]\\n', '\\n', '[Question Start]What role can historical diachronic embeddings play as a tool to study semantic shifts and cultural biases over time? Can you provide an example from your text above? [Question End]\\n', '\\n', '[Question Start]How are gender and ethnic bias quantified in word embeddings, and what was the trend observed by Garg et al for competence adjectives during 1960-1990?[Question End]\\n', '\\n', '[Question Start]What analogy-solving method is related to the parallelogram model of analogical reasoning and was shown by Mikolov et al. (2013b) to be viable with word2vec embeddings? \\n', 'A) Vector addition only in a single dimension \\n', 'B) Vector subtraction only in a single dimension \\n', 'C) Parallelogram method\\n', 'D) The cosine between the vectors of the two pairs is maximized for the second pair relative to the first.[Question End]\\n', '\\n', '[Question Start]Based on the given code snippet and content, a good question could be: How does the function `bagOfWords` create document embeddings, and how do these differ from the input token-level embeddings in the dictionaries?[Question End]']\n",
      "Feedforward_neural_anguage_models\n",
      "['[Question Start]What is the main reason why a simple feedforward neural language model often performs almost as well as more complex models in predicting the next word in a sequence? \\n', 'A) The use of embeddings for words allows the network to find semantic similarities.\\n', 'B) It only considers a fixed number of prior words through sliding windows, limiting complexity. \\n', 'C) It is easier and faster to train compared to more complex architectures. \\n', 'D) It does not suffer from the vanishing gradient problem that can affect recurrent neural networks.[Question End]\\n', '\\n', \"[Question Start]Based on the implementation of the `OneHotEncoder` class and its `encode` method in the lecture content, what would be a proper way to call the `encode` function for the token with index 5 (assuming our tokenizer's vocabulary size is 20), and what should be the shape of the output tensor?```python\\n\", \"# Assuming that we've created an instance of OneHotEncoder:\\n\", 'encoder = OneHotEncoder(vocab_size=20)\\n', '# To get a one-hot encoded vector for the token at index 5, we would call our method as follows:\\n', 'encoding = encoder.encode(5)\\n', '# And the shape of this output tensor should be (20,) which indicates that there are 20 elements in the vector, matching the size of our assumed vocabulary.\\n', '```[Question End]']\n",
      "recurrent_neural_networks_for_nlp\n",
      "['[Question Start]What is the main advantage of a Bidirectional RNN as compared to a unidirectional one for NLP tasks?\\n', 'A) It can only process data in one direction, which may limit its understanding of context.\\n', \"B) It's less computationally expensive than a bidirectional model.\\n\", 'C) It captures context from both past and future time points, improving the performance on many NLP tasks\\n', 'D) It is easier to train as it has fewer connections to learn. [IDE][Question End]']\n",
      "Transformers_Encoder\n",
      "['[Question Start]What is the role of self-attention in the Transformer Encoder, and how does it enable the model to understand context within a sequence of tokens? [Question End]\\n', '\\n', \"[Question Start]In BERT's masked language modeling task, what do we feed into the simple classifier for predicting the masked token, given that we have the final context embeddings from the encoder?[Question End]\\n\", '\\n', '[Question Start]How are [SEP] and [CLS] tokens utilized in BERT to tackle the next sentence prediction task and how does this improve its understanding of the order of sentences? [Question End]\\n', '\\n', \"[Question Start]What is the purpose of positional encoding in Transformer models, and why can't we simply use a flat word embedding for each token without any information about their positions within the sequence?[Question End]\\n\", '\\n', '[Question Start]What is the main purpose of the self-attention mechanism in a Transformer Encoder?\\n', '\\n', 'A) To generate individual token embeddings from positional information only\\n', 'B) To weigh the importance of different input tokens when generating output representations\\n', 'C) To learn a fixed, one-size-fits-all representation for all inputs\\n', 'D) To rearrange the order of the input sequence based on attention weights [IDE]B) To weigh the importance of different input tokens when generating output representations[Question End]']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataset import Lecture\n",
    "\n",
    "lectures: dict[int, Lecture] = pickle.load(open(\"dataset.pkl\", \"rb\"))\n",
    "\n",
    "for lecture_index in lectures:\n",
    "    print(lectures[lecture_index].topic)\n",
    "    with open(f\"questions/{lectures[lecture_index].topic}.txt\", \"r\") as question_file:\n",
    "        print(question_file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def is_question(text: str):\n",
    "    pattern = r'\\[Question Start\\](.*?)\\[Question End\\]'\n",
    "    regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "    matches = regex.findall(text.strip())\n",
    "\n",
    "    return matches\n",
    "\n",
    "def is_valid(text: str):\n",
    "    pattern = r'^[a-zA-Z0-9].*'\n",
    "\n",
    "    # Compile the regex pattern\n",
    "    regex = re.compile(pattern, re.MULTILINE)\n",
    "\n",
    "    # Find all matching sentences\n",
    "    matches = regex.findall(text)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LectureQuestions():\n",
    "    def __init__(self, \n",
    "                 topic: str, \n",
    "                 questions: list = [],\n",
    "                 evaluations: list = [],\n",
    "                 overall_evaluation: list = [],\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.topic: str = topic\n",
    "        self.questions: list = questions\n",
    "        self.evaluations: list = evaluations\n",
    "        self.overall_evaluation: list = overall_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction_into_nlp\n",
      "['What are the common challenges in NLP, and how do researchers attempt to overcome these issues?', 'Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident? ', 'How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?', 'Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\\n\\n\\n\\nA) Text classification for spam filtering\\n\\nB) Generating coherent text based on a given context\\n\\nC) Turning sound into written text (Speech-to-text)\\n\\nD) The automatic creation of visual art from textual prompts\\n\\n(Note: While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content is overstated and remains in the realm of science fiction for now.)', \"Given the following code snippet from StopWordKeywordExtractor class:\\n\\n\\n\\n```python\\n\\nclass StopWordKeywordExtractor:\\n\\n    # ...\\n\\n    def is_proper_word(self, token:str) -> bool:\\n\\n        ''' \\n\\n        Checks if the word is a proper word by our definition \\n\\n        Arguments: \\n\\n            token     -- The token as a string \\n\\n        Return: \\n\\n            is_proper -- True / False\\n\\n        '''\\n\\n        match = re.search(r'\\\\b(\\\\w{2,})\\\\b', token)\\n\\n        return match and token == match[0]\\n\\n```\\n\\n\\n\\nQuestion: What is the purpose of the regular expression `r'\\\\b(\\\\w{2,})\\\\b'` in the is_proper_word function?\"]\n",
      " \n",
      "Text_processing\n",
      "['In the context of BPE tokenization, what is the significance of merging less frequent pairs before more frequent ones during the training phase?', 'What role do morphemes play in subword tokenization methods like BPE and why are they often represented as part of a token in text processed by such algorithms? ', 'In the context of NLP, how might sentence segmentation impact the effectiveness of a byte pair encoding token learner and what precautions can be taken to avoid any negative impact of mis-segmented sentences on the learning process?', 'What is the role of the Byte Pair Encoding (BPE) token learner algorithm in text preprocessing for NLP?\\n\\nA) It separates punctuation from words.\\n\\nB) It normalizes case folding, making everything lowercase or uppercase as needed.\\n\\nC) It learns subword tokens that can represent frequent word subparts and are often morphemes\\n\\nD) It performs full stemming of the words in a corpus.', 'Based on the function `get_splits`, what does the dictionary created by this function represent? Can you provide an example of a key and its associated value in the output of this function for the input `pretokenize([[\"apple\", \"banana\", \"apple\"], [\"apple\"]])`?\\n\\n\\n\\nCode Snippet:\\n\\n```python\\n\\nfrom collections import Counter\\n\\nfrom typing import Dict, Tuple\\n\\n\\n\\ndef get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\\n\\n    \"\"\"Get subword splits of tokens in a corpus.\\n\\n    \\n\\n    Args:\\n\\n        corpus (List[List[str]]): A list of sentences where each sentence is represented\\n\\n            as a list of tokens.\\n\\n\\n\\n    Returns:\\n\\n        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\\n\\n            and values are the counts of occurrences of those splits in the corpus.\\n\\n    \\n\\n    Example: \\n\\n        >>> corpus = [[\"apple\", \"banana\", \"apple\"], [\"apple\"]]\\n\\n        >>> get_splits(corpus)\\n\\n        {(\\'a\\', \\'p\\', \\'p\\', \\'l\\', \\'e\\', \\'</w>\\'): 3, (\\'b\\', \\'a\\', \\'n\\', \\'a\\', \\'n\\', \\'a\\', \\'</w>\\'): 1}\\n\\n```']\n",
      " \n",
      "Transformer_decoder_and_Large_Langauge_Models\n",
      "[\"In the context of large language models, what is the purpose of fine-tuning with human feedback and how does it improve a model's performance?\", 'What are the potential challenges associated with creating high-quality datasets for training transformer decoders, as depicted in the process by OpenAI in 2021?', 'How can multi-task learning be harnessed when training large language models to generate more coherent and human-like text?', 'What was the foundational paper in the field of transformers that sparked research into their applications within natural language processing?', 'Which of the following statements is true regarding the process discussed in \"Learning to summarize from human feedback\" by Stiennon et al (OpenAI), 2022?\\n\\nA) The researchers only used non-expert judges for evaluation.\\n\\nB) The summaries were not posted on a public platform for evaluation but assessed solely in-house.\\n\\nC) Both expert and non-expert judges evaluated the summaries, giving them rewards based on quality as perceived by their respective levels of expertise. \\n\\nD) The model was only fine-tuned once after collecting the initial batch of human feedback.']\n",
      " \n",
      "FineTuning_of_LLMs\n",
      "['What is the potential issue with a low-rank assumption in LoRa, and how might it impact model performance?', \"In few-shot learning, what's the difference between zero-shot and few-shot prompting, and why might one be more effective than the other for certain tasks? \", 'How does prefix tuning differ from parameter-efficient fine-tuning methods like LoRa, in terms of their approach to updating parameters while maintaining model efficiency?', 'In an NLP context, what are some potential advantages and disadvantages of fine-tuning a large pre-trained LLM on a specific task rather than starting with a model trained directly for the target task from scratch?', \"What is the low-rank assumption in LoRa (Low-Rank Adaptation of Large Language Models) and how does it help with fine-tuning these models?\\n\\nA) It assumes that only a few new parameters are needed to adapt the model, reducing the computational resources required during training.\\n\\nB: It assumes the update weight matrix contains just as much information as the original weights, requiring no reduction in dimensionality or computation.\\n\\nC: It assumes the update weight matrix is of higher rank than the original weights, actually increasing the complexity and parameters of the fine-tuned model. \\n\\nD: It makes no assumptions about the weight matrices' ranks, making the approach computationally intensive like a full parameter update.\"]\n",
      " \n",
      "Retrival_augemented_generation\n",
      "['What are the key differences between a retrieval-based approach and a keyword search in an NLP context, especially in relation to the RAG system? ', 'How can multi-armed bandit theory be leveraged in hybrid document retrieval for a RAG system, given user feedback on relevancy of retrieved documents?', 'In a corporate environment, what are some potential challenges or special considerations when implementing an NLP RAG system and how might these require adaptations to the standard approach? ', 'How can intermediate queries improve the effectiveness of the interaction between users, document retrieval, and LLMs in a RAG system?', 'What is the primary motivation behind using a Retrieval-Augmented Generator (RAG) in natural language processing, and what key components does it consist of?\\n\\nA) To reduce computation time and resources; vector search and document retrieval\\n\\nB) To increase context length and handle larger datasets; LLMs and multi-head attention\\n\\nC) To improve the understanding of visual content and tables in documents; computer vision models and table parsing algorithms\\n\\nD) To personalize text generation for different users; user modeling and reinforcement learning']\n",
      " \n",
      "Statistical_language_models\n",
      "['What is the intuition behind the smoothing techniques in statistical language modeling, and how do they help with the sparsity issue of n-gram models?', 'How does the use of caching models impact speech recognition tasks as opposed to text-based applications? ', 'How can N-gram probabilities be combined through interpolation, and why might simple linear interpolation not always provide the best results in practice?', \"What is the primary purpose of smoothing techniques in statistical language models? \\n\\nA) To improve the probability assigned to unseen n-grams during evaluation on test sets\\n\\nB) To reduce the complexity of the model for more efficient processing\\n\\nC) To increase the likelihood that all words are part of the model's vocabulary\\n\\nD) To specifically weight more recently used words as more likely to appear next in a text[IDE]\", 'Question:In the lecture content, a bigram model is built for text from the movie *Spider-Man Homecoming*. The `bigram_probability` method in the BigramModel class calculates the probability of one word given another. Can you explain, step-by-step, how the probabilities are calculated and what data structures are used to enable quick access to these probabilities?']\n",
      " \n",
      "Static_embedding_OneHot_encoding\n",
      "[\"What is the main purpose of converting words into vector representations in natural language processing, and why can't machine learning models work directly with text data? \", 'Why might one-hot encoding not always be an ideal representation for word embeddings, and what are some alternative methods to capture semantic meaning in vectors?', 'In the context of NLP, why is it important for a model to understand the similarity between words, and how does representing words as vectors help with this task?', 'What is the primary purpose of one-hot encoding in NLP?\\n\\nA) To reduce dimensionality of word embeddings\\n\\nB) To convert categorical data into a usable format for machine learning algorithms\\n\\nC) To quantify the semantic similarity between words in vector space\\n\\nD) To create a sparse representation of text documents for improved computational efficiency', '**Question:**\\n\\nIn the `TfIdfModel` class, the method `build_index` is implemented to create a dictionary that maps each word type in the documents to an index. The same functionality was already implemented in the `OneHotModel` class and can be reused there. Should you copy that implementation over or write your own? What are the trade-offs between these two decisions, considering the need for maintaining the code DRY while also avoiding unnecessary complexity due to shared code with a different model?']\n",
      " \n",
      "Static_embedding_TFIDF\n",
      "['What is the purpose of normalizing term frequencies in the TF-IDF method?', 'In the given example, what are the calculated TF-IDF values for \"a\" and \"linux\", respectively? ', 'Why would a term with an inverse document frequency of 0 not be useful for distinguishing between documents in this context?', \"What is the effect of a high term frequency and low document frequency on a word's TF-IDF value in a text corpus?\\n\\nA) The value will be low, as high tf and low df could indicate the word is common within a single document but not very important across the entire corpus.\\n\\nB) The value will be moderate, as the high tf somewhat offsets the low df.\\n\\nC) The value will be high, as a high tf and a low df may suggest the word is particularly significant to one specific document in the corpus.\\n\\nD) It's impossible to say without more information on the inverse document frequency values.\", \"Based on the given excerpt, a potential question could be: \\n\\n\\n\\nIn the 'TfIdfModel' class, what is the purpose of the 'df_vector' and how is it computed from the 'term_doc_matrix'? \\n\\nCode snippet for reference:\\n\\n```python\\n\\ndf_vector = np.zeros(len(self.index))\\n\\n# Fill the matrix and compute the tfidf matrix\\n\\nraise NotImplementedError()\\n\\n```\"]\n",
      " \n",
      "Static_embedding_word2Vec\n",
      "['What is the main difference between the traditional approach to word embeddings and the Negative Sampling Skip Gram Model in terms of computational time? ', 'In the context of Word2Vec, why is it crucial for the model to incorporate negative examples along with positive examples during training?', \"How does the frequency-based negative sampling method help improve the Word2Vec model's understanding of word relationships and context within a text corpus?\", 'Question: In the context of Word2Vec and negative sampling, what is one primary reason for creating synthetic negative examples during training?\\n\\nA) To outweigh the number of positive examples in a dataset\\n\\nB) To ensure that the model isn\\'t \"tricked\" by only learning from positive examples \\n\\nC) To make the most of sparse data where there are limited negative examples available\\n\\nD) To artificially boost the performance of the classifier on test data[IDE]', \"### Question:\\n\\nIn the given lecture content, there's a function called `bagOfWords` that is used to create document embeddings. It takes in a model and a document as input and returns the document embedding by summing up word vectors. Can you explain why taking the mean of the word vectors would not work here?\"]\n",
      " \n",
      "Static_embeding_Vector_semeantic_embeddings\n",
      "['How does the parallelogram method work in solving analogies, and why is it not effective for all types of analogical relations?', 'What role can historical diachronic embeddings play as a tool to study semantic shifts and cultural biases over time? Can you provide an example from your text above? ', 'How are gender and ethnic bias quantified in word embeddings, and what was the trend observed by Garg et al for competence adjectives during 1960-1990?', 'What analogy-solving method is related to the parallelogram model of analogical reasoning and was shown by Mikolov et al. (2013b) to be viable with word2vec embeddings? \\n\\nA) Vector addition only in a single dimension \\n\\nB) Vector subtraction only in a single dimension \\n\\nC) Parallelogram method\\n\\nD) The cosine between the vectors of the two pairs is maximized for the second pair relative to the first.', 'Based on the given code snippet and content, a good question could be: How does the function `bagOfWords` create document embeddings, and how do these differ from the input token-level embeddings in the dictionaries?']\n",
      " \n",
      "Feedforward_neural_anguage_models\n",
      "['What is the main reason why a simple feedforward neural language model often performs almost as well as more complex models in predicting the next word in a sequence? \\n\\nA) The use of embeddings for words allows the network to find semantic similarities.\\n\\nB) It only considers a fixed number of prior words through sliding windows, limiting complexity. \\n\\nC) It is easier and faster to train compared to more complex architectures. \\n\\nD) It does not suffer from the vanishing gradient problem that can affect recurrent neural networks.', \"Based on the implementation of the `OneHotEncoder` class and its `encode` method in the lecture content, what would be a proper way to call the `encode` function for the token with index 5 (assuming our tokenizer's vocabulary size is 20), and what should be the shape of the output tensor?```python\\n\\n# Assuming that we've created an instance of OneHotEncoder:\\n\\nencoder = OneHotEncoder(vocab_size=20)\\n\\n# To get a one-hot encoded vector for the token at index 5, we would call our method as follows:\\n\\nencoding = encoder.encode(5)\\n\\n# And the shape of this output tensor should be (20,) which indicates that there are 20 elements in the vector, matching the size of our assumed vocabulary.\\n\\n```\"]\n",
      " \n",
      "recurrent_neural_networks_for_nlp\n",
      "[\"What is the main advantage of a Bidirectional RNN as compared to a unidirectional one for NLP tasks?\\n\\nA) It can only process data in one direction, which may limit its understanding of context.\\n\\nB) It's less computationally expensive than a bidirectional model.\\n\\nC) It captures context from both past and future time points, improving the performance on many NLP tasks\\n\\nD) It is easier to train as it has fewer connections to learn. [IDE]\"]\n",
      " \n",
      "Transformers_Encoder\n",
      "['What is the role of self-attention in the Transformer Encoder, and how does it enable the model to understand context within a sequence of tokens? ', \"In BERT's masked language modeling task, what do we feed into the simple classifier for predicting the masked token, given that we have the final context embeddings from the encoder?\", 'How are [SEP] and [CLS] tokens utilized in BERT to tackle the next sentence prediction task and how does this improve its understanding of the order of sentences? ', \"What is the purpose of positional encoding in Transformer models, and why can't we simply use a flat word embedding for each token without any information about their positions within the sequence?\", 'What is the main purpose of the self-attention mechanism in a Transformer Encoder?\\n\\n\\n\\nA) To generate individual token embeddings from positional information only\\n\\nB) To weigh the importance of different input tokens when generating output representations\\n\\nC) To learn a fixed, one-size-fits-all representation for all inputs\\n\\nD) To rearrange the order of the input sequence based on attention weights [IDE]B) To weigh the importance of different input tokens when generating output representations']\n",
      " \n",
      "defaultdict(None, {0: <__main__.LectureQuestions object at 0x7b349c090890>, 1: <__main__.LectureQuestions object at 0x7b3498778cd0>, 10: <__main__.LectureQuestions object at 0x7b3589bf6590>, 11: <__main__.LectureQuestions object at 0x7b34987e0150>, 12: <__main__.LectureQuestions object at 0x7b34987de750>, 2: <__main__.LectureQuestions object at 0x7b34987e1490>, 3: <__main__.LectureQuestions object at 0x7b34987e1550>, 4: <__main__.LectureQuestions object at 0x7b34987e15d0>, 5: <__main__.LectureQuestions object at 0x7b34987e1710>, 6: <__main__.LectureQuestions object at 0x7b34987e1790>, 7: <__main__.LectureQuestions object at 0x7b34987e1810>, 8: <__main__.LectureQuestions object at 0x7b34987e1750>, 9: <__main__.LectureQuestions object at 0x7b34987e17d0>})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "questions = defaultdict()\n",
    "\n",
    "for lecture_index in lectures:\n",
    "    print(f\"{lectures[lecture_index].topic}\")\n",
    "\n",
    "    lecture_questions = LectureQuestions(lectures[lecture_index].topic)\n",
    "\n",
    "    with open(f\"questions/{lectures[lecture_index].topic}.txt\", \"r\") as question_file:\n",
    "        text = \"\\n\".join(question_file.readlines())\n",
    "        question_text = is_question(text)\n",
    "\n",
    "        print(question_text)\n",
    "\n",
    "    lecture_questions.questions = question_text\n",
    "    questions[lecture_index] = lecture_questions\n",
    "\n",
    "    print(f\" \")\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lecture_content: What is NLP?\n",
      "\n",
      "\n",
      "What is NLP?\n",
      "Natural language processing (NLP) is an interdisciplinary subfield of \n",
      "computer science and information retrieval. It is primarily concerned with \n",
      "giving computers the ability to support and manipulate human language. It \n",
      "involves processing natural language datasets, such as text corpora or \n",
      "speech corpora, using either rule-based or probabilistic (i.e. statistical and, \n",
      "most recently, neural network-based) machine learning approaches. The \n",
      "goal is a computer capable of \"understanding\" the contents of documents, \n",
      "including the contextual nuances of the language within them. To this end, \n",
      "natural language processing often borrows ideas from theoretical \n",
      "linguistics. The technology can then accurately extract information and \n",
      "insights contained in the documents as well as categorize and organize the \n",
      "documents themselves.\n",
      "“\n",
      "”\n",
      "Source: https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1215529997\n",
      "focus in this lecture\n",
      "\n",
      "\n",
      "What are common NLP tasks?\n",
      "• Text Classification / Sentiment Analysis / Moderation Systems\n",
      "• Summarization\n",
      "• Text Generation / Autocomplete / Recommendation\n",
      "• Assistant systems\n",
      "• Translation\n",
      "• Search / Retrieval / QA\n",
      "• Speech to Text\n",
      "• Entity Recognition (linking to Knowledge Bases)\n",
      "\n",
      "\n",
      "What are common challenges in NLP?\n",
      "• Ambiguities / Homonyms\n",
      "• Computation\n",
      "• Speech 2 Text losses: Informal Speech / Filler Words, Utterances\n",
      "• Vectorization / Representation (flexible input lengths, vocab sizes…)\n",
      "• Typos\n",
      "• Dataset sizes\n",
      "• Languages, Character Sets, Writing styles, Accents\n",
      "• Hallucinations\n",
      "• Explainability\n",
      "• Biases in datasets / need for diverse datasets\n",
      "\n",
      "\n",
      "Application Areas, Tasks & \n",
      "Examples\n",
      "\n",
      "\n",
      "Text classification\n",
      "• Spam classification\n",
      "• Text Sentiment Analysis\n",
      "Spam Classifier\n",
      "Spam\n",
      "Ham\n",
      "Classifier\n",
      "Negative\n",
      "Positive\n",
      "Often also as\n",
      "scoring /\n",
      "regression variant\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents the fundamental concepts of text classification and sentiment analysis. Text classification refers to the process of categorizing text data into predefined categories, such as spam or not spam. In this case, the diagram illustrates how a spam classifier is used to determine whether an email is spam or ham (not spam). The presence of an often also as scoring/regression variant suggests that these classification methods can be extended to include numerical scores or regression analysis.\n",
      "\n",
      "On the other hand, sentiment analysis focuses on determining the emotional tone or attitude conveyed in a piece of text. The diagram shows a text sentiment analysis with a classifier that categorizes text into positive, negative, or neutral sentiments. This process is essential for understanding public opinion, customer feedback, and overall sentiment towards a product or service. As we delve into NLP, these concepts form the building blocks for more advanced techniques in understanding and interacting with human language. [IDE]\n",
      "\n",
      "\n",
      "Text classification: Sentiment Analysis\n",
      "Source: https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
      "\n",
      "[IDS] In this NLP lecture, we are presented with a demonstration of text classification using sentiment analysis. The slide showcases the hosted inference API from Hugging Face, a popular platform for natural language processing. The interface allows users to input text, and it provides a classification result based on the sentiment, which in this case is labeled as \"Examples\". The code snippet provided is written in JSON format, indicating the structured data used for training the model. The output displays the computation time and the sentiment score, which can be used to determine the polarity of the input text. This tool is useful for understanding public opinion or user feedback in various applications, such as social media monitoring or customer service. [IDE]\n",
      "\n",
      "\n",
      "Machine Translation (MT)\n",
      "• Google Translate (translate.google.com)\n",
      "• DeepL (www.deepl.com) \n",
      "\n",
      "[IDS] We are in a lecture about NLP, which stands for Natural Language Processing. The image shows a presentation slide with examples of machine translation from English to German using different services like Google Translate and DeepL. This demonstrates how NLP technology is used to translate text between languages, which is a fundamental aspect of understanding human language and communication. [IDE]\n",
      "\n",
      "\n",
      "Keyword Extraction\n",
      "• Extract the most important \n",
      "phrases (keywords, key \n",
      "phrases) from a document\n",
      "• Token classification \n",
      "Source: https://huggingface.co/jasminejwebb/KeywordIdentifier \n",
      "\n",
      "[IDS] In the image, we are presented with a slide from an NLP lecture titled \"Keyword Extraction\". The slide emphasizes the importance of extracting key phrases (keywords) from a document, which is a fundamental process in Natural Language Processing (NLP). It explains that these keywords are identified through artificial intelligence and can be used to understand the content of a large amount of natural language data. Additionally, the slide introduces the concept of token classification, which involves computing the time it takes for various NLP tasks on a CPU, using examples such as natural language processing, logistics, computer science, and others. This slide serves as an introductory overview of keyword extraction and token classification within the field of NLP. [IDE]\n",
      "\n",
      "\n",
      "Text Summarization\n",
      "• Produce a shorter version\n",
      "• Preserve important info\n",
      "Source: https://huggingface.co/tasks/summarization\n",
      "\n",
      "[IDS] The image you're seeing is a slide from an NLP (Natural Language Processing) lecture, specifically titled \"Introduction into nlp\". The slide focuses on the concept of text summarization, which is a key topic in NLP. It emphasizes the importance of producing shorter versions of text while preserving important information. The slide uses a simple diagram to visually represent the process of text summarization, where inputs are transformed into outputs. This process involves reducing the length of text while retaining its essence, which is a fundamental challenge and area of research in NLP. [IDE]\n",
      "\n",
      "\n",
      "Image Captioning\n",
      "• Describe contents of an image\n",
      "• Generate a title for an image\n",
      "Source: https://huggingface.co/tasks/image-to-text\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents the process of image captioning, which is a subfield of NLP. The image showcases how an input image, such as the giraffe and zebra, is processed to generate a detailed description and a title for the image. This involves using NLP techniques to analyze the visual content and produce textual descriptions that convey the essence of the image. The slide serves as an example of how NLP can bridge the gap between visual and textual data, enabling machines to understand and describe images in a human-like manner. [IDE]\n",
      "\n",
      "\n",
      "Text Generation\n",
      "• Text continuations\n",
      "• Can also be conditioned\n",
      "• Context\n",
      "• Topic\n",
      "• Contents\n",
      "• Questions\n",
      "• Language\n",
      "• …\n",
      "Source: https://huggingface.co/tasks/text-generation\n",
      "\n",
      "[IDS] In this NLP lecture, we're discussing the fundamentals of text generation, a key aspect of Natural Language Processing. The slide outlines the core components involved in generating text, such as inputs, context, topic, contents, questions, and language. It also emphasizes the importance of being able to condition these inputs and mentions the Text Generation Model as a significant part of our exploration. This model draws inspiration from historical figures like Alexander the Great and Chaucer, showcasing how NLP can bring ancient explorers and poets to life through modern technology. The source for this information is provided as https://huggingface.co/tasks/text-generation, indicating that this is a resource from Hugging Face, a well-known entity in the NLP community. [IDE]\n",
      "\n",
      "\n",
      "Image Generation from Text (T2I)\n",
      "• Text to Image\n",
      "• Also interactive\n",
      "Source: https://huggingface.co/tasks/text-to-image\n",
      "\n",
      "[IDS] In this NLP lecture, we are exploring the concept of \"Image Generation from Text\" or T2I. This process involves using natural language processing to generate images from textual inputs. The slide presents a visual representation of the T2I model, where inputs like text descriptions and styles are processed to produce an output image. The example given is a cityscape described in a Victorian style, which is then transformed into a colorful and fantastical image. The source for this information is provided as a link to a GitHub repository containing tasks related to text-to-image generation. As we delve deeper into the topic, we learn about the intricacies of how language is translated into visual representations, highlighting the intersection of NLP and computer vision. [IDE]\n",
      "\n",
      "\n",
      "Question Answering\n",
      "• Ask questions about text\n",
      "• Get answers\n",
      "• Variants:\n",
      "• Relevant passage given\n",
      "• Corpus based\n",
      "• General purpose model\n",
      "Source: https://huggingface.co/tasks/question-answering\n",
      "\n",
      "[IDS] The image depicts a slide from an NLP lecture, specifically addressing the concept of \"Question Answering\" within the field. It highlights various aspects such as asking questions about text, getting answers, and using different models like variants (relevant passage given, corpus-based, general purpose), context (Amazon rainforest, also known in English as Amazonia or the Amazon Jungle), and inputs (question and question answering model). The source is mentioned as a website with tasks and questions related to this topic. [IDE]\n",
      "\n",
      "\n",
      "“Chat Bots”\n",
      "• IRC / Discord\n",
      "• Bot Users / Chat integration\n",
      "• Often keyword / rule based\n",
      "• Intent Recognition Systems:\n",
      "• Customer support (the annoying things on websites / phone hotlines)\n",
      "• (Air Canada Incident! https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know )\n",
      "• (To some degree after Speech 2 Text) Alexa, Cortana, Google Assistant, Siri \n",
      "• Chat assistant / conversational AI systems\n",
      "• ChatGPT, Gemini (Bard), Copilot, Claude, …\n",
      "• General Task Interfaces\n",
      "\n",
      "lecture_questions:\n",
      "What are the common challenges in NLP, and how do researchers attempt to overcome these issues?\n",
      "Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident? \n",
      "How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?\n",
      "Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\n",
      "\n",
      "\n",
      "\n",
      "A) Text classification for spam filtering\n",
      "\n",
      "B) Generating coherent text based on a given context\n",
      "\n",
      "C) Turning sound into written text (Speech-to-text)\n",
      "\n",
      "D) The automatic creation of visual art from textual prompts\n",
      "\n",
      "(Note: While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content is overstated and remains in the realm of science fiction for now.)\n",
      "Given the following code snippet from StopWordKeywordExtractor class:\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "class StopWordKeywordExtractor:\n",
      "\n",
      "    # ...\n",
      "\n",
      "    def is_proper_word(self, token:str) -> bool:\n",
      "\n",
      "        ''' \n",
      "\n",
      "        Checks if the word is a proper word by our definition \n",
      "\n",
      "        Arguments: \n",
      "\n",
      "            token     -- The token as a string \n",
      "\n",
      "        Return: \n",
      "\n",
      "            is_proper -- True / False\n",
      "\n",
      "        '''\n",
      "\n",
      "        match = re.search(r'\\b(\\w{2,})\\b', token)\n",
      "\n",
      "        return match and token == match[0]\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Question: What is the purpose of the regular expression `r'\\b(\\w{2,})\\b'` in the is_proper_word function?\n",
      "lecture_content: Regular expressions are used everywhere\n",
      "◦Part of every text processing task\n",
      "◦Not a general NLP solution (for that we use large NLP \n",
      "systems we will see in later lectures)\n",
      "◦But very useful as part of those systems (e.g., for pre-\n",
      "processing or text formatting)\n",
      "◦Necessary for data analysis of text data\n",
      "◦A widely used tool in industry and academics\n",
      "2\n",
      "\n",
      "\n",
      "Regular expressions\n",
      "A formal language for specifying text strings\n",
      "How can we search for mentions of these cute animals in text?\n",
      "◦woodchuck\n",
      "◦woodchucks\n",
      "◦Woodchuck\n",
      "◦Woodchucks\n",
      "◦Groundhog\n",
      "◦groundhogs\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a visual aid used to explain how to search for specific patterns or expressions within text data. The beaver, as a symbol of hard work and diligence, may metaphorically represent the meticulous process of searching through text strings using regular expressions. This technique is crucial in NLP for tasks such as data preprocessing, pattern matching, and text analysis. [IDE]\n",
      "\n",
      "\n",
      "Regular Expressions: Disjunctions\n",
      "Letters inside square brackets []\n",
      "Ranges using the dash [A-Z]\n",
      " \n",
      " \n",
      "Pattern\n",
      "Matches\n",
      "[wW]oodchuck\n",
      "Woodchuck, woodchuck\n",
      "[1234567890] \n",
      "Any one digit\n",
      "Pattern\n",
      "Matches\n",
      "[A-Z]\n",
      "An upper case letter\n",
      "Drenched Blossoms\n",
      "[a-z]\n",
      "A lower case letter\n",
      "my beans were impatient\n",
      "[0-9]\n",
      "A single digit\n",
      "Chapter 1: Down the Rabbit Hole\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>[wW]oodchuck</td>\n",
      "      <td>Woodchuck, woodchuck</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>[1234567890]</td>\n",
      "      <td>Any one digit</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table><table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>[A-Z]</td>\n",
      "      <td>An upper case letter</td>\n",
      "      <td>Drenched Blossoms</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>[a-z]</td>\n",
      "      <td>A lower case letter</td>\n",
      "      <td>my beans were impatient</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>[0-9]</td>\n",
      "      <td>A single digit</td>\n",
      "      <td>Chapter 1: Down the Rabbit Hole</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Regular Expressions: Negation in Disjunction\n",
      "Carat as first character in [] negates the list\n",
      "◦Note: Carat means negation only when it's first in []\n",
      "◦Special characters (., *, +, ?) lose their special meaning inside []\n",
      "Pattern\n",
      "Matches\n",
      "Examples\n",
      "[^A-Z]\n",
      "Not an upper case letter\n",
      "Oyfn pripetchik\n",
      "[^Ss]\n",
      "Neither ‘S’ nor ‘s’\n",
      "I have no exquisite reason”\n",
      "[^.]\n",
      "Not a period\n",
      "Our resident Djinn\n",
      "[e^]\n",
      "Either e or ^\n",
      "Look up ^ now\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "      <th>Examples</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>[^A-Z]</td>\n",
      "      <td>Not an upper case letter</td>\n",
      "      <td>Oyfn pripetchik</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>[^Ss]</td>\n",
      "      <td>Neither ‘S’ nor ‘s’</td>\n",
      "      <td>I have no exquisite reason”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>[^.]</td>\n",
      "      <td>Not a period</td>\n",
      "      <td>Our resident Djinn</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>[e^]</td>\n",
      "      <td>Either e or ^</td>\n",
      "      <td>Look up ^ now</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Regular Expressions: Convenient aliases\n",
      "Pattern\n",
      "Expansion\n",
      "Matches\n",
      "Examples\n",
      "\\d\n",
      "[0-9]\n",
      "Any digit\n",
      "Fahreneit 451\n",
      "\\D \n",
      "[^0-9]\n",
      "Any non-digit\n",
      "Blue Moon\n",
      "\\w\n",
      "[a-ZA-Z0-9_]\n",
      "Any alphanumeric or _\n",
      "Daiyu\n",
      "\\W\n",
      "[^\\w]\n",
      "Not alphanumeric or _\n",
      "Look!\n",
      "\\s\n",
      "[ \\r\\t\\n\\f]\n",
      "Whitespace (space, tab)\n",
      "Look␣up\n",
      "\\S\n",
      "[^\\s]\n",
      "Not whitespace\n",
      "Look up\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Expansion</th>\n",
      "      <th>Matches</th>\n",
      "      <th>Examples</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>\\d</td>\n",
      "      <td>[0-9]</td>\n",
      "      <td>Any digit</td>\n",
      "      <td>Fahreneit 451</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>\\D</td>\n",
      "      <td>[^0-9]</td>\n",
      "      <td>Any non-digit</td>\n",
      "      <td>Blue Moon</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>\\w</td>\n",
      "      <td>[a-ZA-Z0-9_]</td>\n",
      "      <td>Any alphanumeric or _</td>\n",
      "      <td>Daiyu</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>\\W</td>\n",
      "      <td>[^\\w]</td>\n",
      "      <td>Not alphanumeric or _</td>\n",
      "      <td>Look!</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>\\s</td>\n",
      "      <td>[ \\r\\t\\n\\f]</td>\n",
      "      <td>Whitespace (space, tab)</td>\n",
      "      <td>Look␣up</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>\\S</td>\n",
      "      <td>[^\\s]</td>\n",
      "      <td>Not whitespace</td>\n",
      "      <td>Look up</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Regular Expressions: More Disjunction\n",
      "Groundhog is another name for woodchuck!\n",
      "The pipe symbol | for disjunction\n",
      "Pattern\n",
      "Matches\n",
      "groundhog|woodchuck\n",
      "woodchuck\n",
      "yours|mine\n",
      "yours\n",
      "a|b|c\n",
      "= [abc]\n",
      "[gG]roundhog|[Ww]oodchuck\n",
      "Woodchuck\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>groundhog|woodchuck</td>\n",
      "      <td>woodchuck</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>yours|mine</td>\n",
      "      <td>yours</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>a|b|c</td>\n",
      "      <td>= [abc]</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>[gG]roundhog|[Ww]oodchuck</td>\n",
      "      <td>Woodchuck</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "[IDS] The image is a visual aid for a lecture on Natural Language Processing (NLP), specifically focusing on text processing. It illustrates the concept of \"Regular Expressions\" as a tool for disjunction, which is a way to match patterns in text data. The example given is the word \"Groundhog\" and its alternative name \"Woodchuck.\" The slide provides a pattern and matches section, showing how regular expressions can be used to find occurrences of these names in text. This is relevant to NLP as it demonstrates how computational methods can be applied to process and analyze natural language texts. [IDE]\n",
      "\n",
      "\n",
      "Wildcards, optionality, repetition: . ? * +\n",
      "Stephen C Kleene\n",
      "Pattern\n",
      "Matches\n",
      "Examples\n",
      "beg.n\n",
      "Any char\n",
      "begin\n",
      "begun \n",
      "beg3n  beg n\n",
      "woodchucks?\n",
      "Optional s\n",
      "woodchuck \n",
      "woodchucks\n",
      "to*\n",
      "0 or more of \n",
      "previous char\n",
      "t to too tooo\n",
      "to+\n",
      "1 or more of \n",
      "previous char\n",
      "to too tooo \n",
      "toooo\n",
      "Kleene *,   Kleene +  \n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "      <th>Examples</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>beg.n</td>\n",
      "      <td>Any char</td>\n",
      "      <td>begin begun\\nbeg3n beg n</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>woodchucks?</td>\n",
      "      <td>Optional s</td>\n",
      "      <td>woodchuck\\nwoodchucks</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>to*</td>\n",
      "      <td>0 or more of\\nprevious char</td>\n",
      "      <td>t to too tooo</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>to+</td>\n",
      "      <td>1 or more of\\nprevious char</td>\n",
      "      <td>to too tooo\\ntoooo</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on text processing techniques. It introduces the concept of wildcards and optionality in regular expressions, which are crucial for pattern matching in text data. The slide outlines how to use these concepts to match different patterns, such as any character, optional sequences, zero or more occurrences of a previous character, and one or more occurrences of a previous character. These patterns are essential in NLP for tasks like tokenization, parsing, and search algorithms. The examples provided demonstrate how to apply these patterns in regular expressions. The presence of the name \"Stephen C Kleene\" suggests that the lecture might be discussing the contributions of Stephen Kleene, a mathematician known for his work on regular expressions and formal languages. [IDE]\n",
      "\n",
      "\n",
      "Regular Expressions: Anchors  ^   $\n",
      "Pattern\n",
      "Matches\n",
      "^[A-Z] \n",
      "Palo Alto\n",
      "^[^A-Za-z] \n",
      "1    “Hello”\n",
      "\\.$ \n",
      "The end.\n",
      ".$ \n",
      "The end?  The end!\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Pattern</th>\n",
      "      <th>Matches</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>^[A-Z]</td>\n",
      "      <td>Palo Alto</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>^[^A-Za-z]</td>\n",
      "      <td>1 “Hello”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>\\.$</td>\n",
      "      <td>The end.</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>.$</td>\n",
      "      <td>The end? The end!</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "A note about Python regular expressions\n",
      "◦Regex and Python both use backslash \"\\\" for \n",
      "special characters. You must type extra backslashes!\n",
      "◦\"\\\\d+\"  to search for 1 or more digits\n",
      "◦\"\\n\" in Python means the \"newline\" character, not a \n",
      "\"slash\" followed by an \"n\". Need \"\\\\n\" for two characters.\n",
      "◦Instead: use Python's raw string notation for regex:\n",
      "◦r\"[tT]he\"\n",
      "◦r\"\\d+\" matches one or more digits\n",
      "◦instead of \"\\\\d+\"\n",
      "10\n",
      "\n",
      "\n",
      "The iterative process of writing regex's\n",
      "Find me all instances of the word “the” in a text.\n",
      "the\n",
      "Misses capitalized examples\n",
      "[tT]he\n",
      "Incorrectly returns other or Theology\n",
      "\\W[tT]he\\W\n",
      "\n",
      "\n",
      "False positives and false negatives\n",
      "The process we just went through was based on \n",
      "fixing two kinds of errors:\n",
      "1. Not matching things that we should have matched \n",
      "(The)\n",
      "False negatives\n",
      "2. Matching strings that we should not have matched \n",
      "(there, then, other)\n",
      "False positives\n",
      "\n",
      "\n",
      "Characterizing work on NLP\n",
      "In NLP we are always dealing with these kinds of errors.\n",
      "Reducing the error rate for an application often \n",
      "involves two antagonistic efforts: \n",
      "◦Increasing coverage (or recall) (minimizing false negatives).\n",
      "◦Increasing accuracy (or precision) (minimizing false positives)\n",
      "\n",
      "\n",
      "Regular expressions play a surprisingly large role\n",
      "Widely used in both academics and industry\n",
      "1. Part of most text processing tasks, even for big \n",
      "neural language model pipelines\n",
      "◦including text formatting and pre-processing\n",
      "2. Very useful for data analysis of any text data\n",
      "14\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Regular Expressions\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "More Regular Expressions: \n",
      "Substitutions and ELIZA\n",
      "\n",
      "\n",
      "Substitutions\n",
      "Substitution in Python and UNIX commands:\n",
      "s/regexp1/pattern/ \n",
      "e.g.:\n",
      "s/colour/color/ \n",
      "\n",
      "\n",
      "Capture Groups\n",
      "• Say we want to put angles around all numbers:\n",
      "the 35 boxes à the <35> boxes \n",
      "• Use parens () to \"capture\" a pattern into a \n",
      "numbered register (1, 2, 3…)\n",
      "• Use \\1  to refer to the contents of the register\n",
      "s/([0-9]+)/<\\1>/ \n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>the</th>\n",
      "      <th>&lt;</th>\n",
      "      <th>35</th>\n",
      "      <th>&gt;</th>\n",
      "      <th>boxes</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Capture groups: multiple registers\n",
      "/the (.*)er they (.*), the \\1er we \\2/ \n",
      "Matches\n",
      "      the faster they ran, the faster we ran \n",
      "But not\n",
      "      the faster they ran, the faster we ate \n",
      "\n",
      "\n",
      "But suppose we don't want to capture?\n",
      "Parentheses have a double function: grouping terms, and \n",
      "capturing\n",
      "Non-capturing groups: add a ?: after paren:\n",
      "/(?:some|a few) (people|cats) like some \\1/ \n",
      "matches \n",
      "◦some cats like some cats \n",
      "but not \n",
      "◦some cats like some some\n",
      "\n",
      "\n",
      "Lookahead assertions\n",
      "(?= pattern) is true if pattern matches, but is \n",
      "zero-width; doesn't advance character pointer\n",
      "(?! pattern) true if a pattern does not match \n",
      "How to match, at the beginning of a line, any single \n",
      "word that doesn’t start with “Volcano”: \n",
      "/ˆ(?!Volcano)[A-Za-z]+/ \n",
      "\n",
      "\n",
      "Simple Application: ELIZA\n",
      "Early NLP system that imitated a Rogerian \n",
      "psychotherapist \n",
      "◦Joseph Weizenbaum, 1966. \n",
      "Uses pattern matching to match, e.g.,:\n",
      "◦“I need X” \n",
      "and translates them into, e.g.\n",
      "◦“What would it mean to you if you got X? \n",
      "\n",
      "\n",
      "Simple Application: ELIZA\n",
      "Men are all alike.\n",
      "IN WHAT WAY\n",
      "They're always bugging us about something or other. \n",
      "CAN YOU THINK OF A SPECIFIC EXAMPLE \n",
      "Well, my boyfriend made me come here.\n",
      "YOUR BOYFRIEND MADE YOU COME HERE \n",
      "He says I'm depressed much of the time.\n",
      "I AM SORRY TO HEAR YOU ARE DEPRESSED \n",
      "\n",
      "\n",
      "How ELIZA works\n",
      "s/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/ \n",
      "s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\n",
      "s/.* all .*/IN WHAT WAY?/ \n",
      "s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE?/ \n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "More Regular Expressions: \n",
      "Substitutions and ELIZA\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Words and Corpora\n",
      "\n",
      "\n",
      "How many words in a sentence?\n",
      "\"I do uh main- mainly business data processing\"\n",
      "◦Fragments, filled pauses\n",
      "\"Seuss’s cat in the hat is different from other cats!\" \n",
      "◦Lemma: same stem, part of speech, rough word sense\n",
      "◦cat and cats = same lemma\n",
      "◦Wordform: the full inflected surface form\n",
      "◦cat and cats = different wordforms\n",
      "\n",
      "\n",
      "How many words in a sentence?\n",
      "they lay back on the San Francisco grass and looked at the stars \n",
      "and their\n",
      "Type: an element of the vocabulary.\n",
      "Token: an instance of that type in running text.\n",
      "How many?\n",
      "◦15 tokens (or 14)\n",
      "◦13 types (or 12) (or 11?)\n",
      "\n",
      "\n",
      "How many words in a corpus?\n",
      "N = number of tokens\n",
      "V = vocabulary = set of types, |V| is size of vocabulary\n",
      "Heaps Law = Herdan's Law =                                 where often .67 < β < .75\n",
      "i.e., vocabulary size grows with > square root of the number of word tokens\n",
      "Tokens = N\n",
      "Types = |V|\n",
      "Switchboard phone conversations\n",
      "2.4 million\n",
      "20 thousand\n",
      "Shakespeare\n",
      "884,000\n",
      "31 thousand\n",
      "COCA\n",
      "440 million\n",
      "2 million\n",
      "Google N-grams\n",
      "1 trillion\n",
      "13+ million\n",
      "and in fact this relationship between the number of types |V| and nu\n",
      " N is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1\n",
      "iscoverers (in linguistics and information retrieval respectively). It is sh\n",
      "1, where k and b are positive constants, and 0 < b < 1.\n",
      "|V| = kNb\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>Tokens = N</th>\n",
      "      <th>Types = |V|</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Switchboard phone conversations</td>\n",
      "      <td>2.4 million</td>\n",
      "      <td>20 thousand</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>Shakespeare</td>\n",
      "      <td>884,000</td>\n",
      "      <td>31 thousand</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>COCA</td>\n",
      "      <td>440 million</td>\n",
      "      <td>2 million</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>Google N-grams</td>\n",
      "      <td>1 trillion</td>\n",
      "      <td>13+ million</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Corpora\n",
      "Words don't appear out of nowhere! \n",
      "A text is produced by \n",
      "• a specific writer(s), \n",
      "• at a specific time, \n",
      "• in a specific variety,\n",
      "• of a specific language, \n",
      "• for a specific function.\n",
      "\n",
      "\n",
      "Corpora vary along dimension like\n",
      "◦Language: 7097 languages in the world\n",
      "◦Variety, like African American Language varieties.\n",
      "◦AAE Twitter posts might include forms like \"iont\" (I don't)\n",
      "◦Code switching, e.g., Spanish/English, Hindi/English:\n",
      " \n",
      "S/E: Por primera vez veo a @username actually being hateful! It was beautiful:) \n",
      " \n",
      "   [For the first time I get to see @username actually being hateful! it was beautiful:) ] \n",
      " \n",
      "H/E: dost tha or ra- hega ... dont wory ... but dherya rakhe \n",
      " \n",
      "   [“he was and will remain a friend ... don’t worry ... but have faith”] \n",
      "◦Genre: newswire, fiction, scientific articles, Wikipedia\n",
      "◦Author Demographics: writer's age, gender, ethnicity, SES \n",
      "\n",
      "\n",
      "Corpus datasheets\n",
      "Motivation: \n",
      "• Why was the corpus collected?\n",
      "• By whom? \n",
      "• Who funded it? \n",
      "Situation: In what situation was the text written?\n",
      "Collection process: If it is a subsample how was it sampled? Was \n",
      "there consent? Pre-processing?\n",
      "   +Annotation process, language variety, demographics, etc.\n",
      "Gebru et al (2020), Bender and Friedman (2018)\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Words and Corpora\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Word tokenization\n",
      "\n",
      "\n",
      "Text Normalization\n",
      "Every NLP task requires text normalization: \n",
      "1. Tokenizing (segmenting) words\n",
      "2. Normalizing word formats\n",
      "3. Segmenting sentences\n",
      "\n",
      "\n",
      "Space-based tokenization\n",
      "A very simple way to tokenize\n",
      "◦For languages that use space characters between words\n",
      "◦Arabic, Cyrillic, Greek, Latin, etc., based writing systems\n",
      "◦Segment off a token between instances of spaces\n",
      "Unix tools for space-based tokenization\n",
      "◦The \"tr\" command\n",
      "◦Inspired by Ken Church's UNIX for Poets\n",
      "◦Given a text file, output the word tokens and their frequencies\n",
      "\n",
      "\n",
      "Simple Tokenization in UNIX\n",
      "(Inspired by Ken Church’s UNIX for Poets.)\n",
      "Given a text file, output the word tokens and their frequencies\n",
      "tr -sc ’A-Za-z’ ’\\n’ < shakes.txt\n",
      "| sort \n",
      "     | uniq –c \n",
      "1945 A\n",
      "  72 AARON\n",
      "  19 ABBESS\n",
      "   5 ABBOT\n",
      " ... ...\n",
      "25 Aaron\n",
      "6 Abate\n",
      "1 Abates\n",
      "5 Abbess\n",
      "6 Abbey\n",
      "3 Abbot\n",
      "....   …\n",
      "Change all non-alpha to newlines\n",
      "Sort in alphabetical order\n",
      "Merge and count each type\n",
      "\n",
      "\n",
      "The first step: tokenizing\n",
      "tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | head\n",
      "THE\n",
      "SONNETS\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "From\n",
      "fairest\n",
      "creatures\n",
      "We\n",
      "...\n",
      "\n",
      "\n",
      "The second step: sorting\n",
      "tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | sort | head\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "...   \n",
      "\n",
      "\n",
      "More counting\n",
      "Merging upper and lower case\n",
      "tr ‘A-Z’ ‘a-z’ < shakes.txt | tr –sc ‘A-Za-z’ ‘\\n’ | sort | uniq –c \n",
      "Sorting the counts\n",
      "tr ‘A-Z’ ‘a-z’ < shakes.txt | tr –sc ‘A-Za-z’ ‘\\n’ | sort | uniq –c | sort –n –r\n",
      "23243 the\n",
      "22225 i\n",
      "18618 and\n",
      "16339 to\n",
      "15687 of\n",
      "12780 a\n",
      "12163 you\n",
      "10839 my\n",
      "10005 in\n",
      "8954  d\n",
      "What happened here?\n",
      "\n",
      "\n",
      "Issues in Tokenization\n",
      "Can't just blindly remove punctuation:\n",
      "◦m.p.h., Ph.D., AT&T, cap’n\n",
      "◦prices ($45.55)\n",
      "◦dates (01/02/06)\n",
      "◦URLs (http://www.stanford.edu)\n",
      "◦hashtags (#nlproc)\n",
      "◦email addresses (someone@cs.colorado.edu)\n",
      "Clitic: a word that doesn't stand on its own\n",
      "◦\"are\" in we're, French \"je\" in j'ai, \"le\" in l'honneur\n",
      "When should multiword expressions (MWE) be words?\n",
      "◦New York, rock ’n’ roll \n",
      "\n",
      "\n",
      "Tokenization in NLTK\n",
      "ficient finite state automata. For example, Fig. 2.12 shows an example of a basic\n",
      "regular expression that can be used to tokenize with the nltk.regexp tokenize\n",
      "function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009;\n",
      "http://www.nltk.org).\n",
      ">>> text = ’That U.S.A. poster-print costs $12.40...’\n",
      ">>> pattern = r’’’(?x)\n",
      "# set flag to allow verbose regexps\n",
      "...\n",
      "([A-Z]\\.)+\n",
      "# abbreviations, e.g. U.S.A.\n",
      "...\n",
      "| \\w+(-\\w+)*\n",
      "# words with optional internal hyphens\n",
      "...\n",
      "| \\$?\\d+(\\.\\d+)?%?\n",
      "# currency and percentages, e.g. $12.40, 82%\n",
      "...\n",
      "| \\.\\.\\.\n",
      "# ellipsis\n",
      "...\n",
      "| [][.,;\"’?():-_‘]\n",
      "# these are separate tokens; includes ], [\n",
      "... ’’’\n",
      ">>> nltk.regexp_tokenize(text, pattern)\n",
      "[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]\n",
      "Figure 2.12\n",
      "A Python trace of regular expression tokenization in the NLTK Python-based\n",
      "natural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)\n",
      "verbose flag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird\n",
      "et al. (2009).\n",
      "Carefully designed deterministic algorithms can deal with the ambiguities that\n",
      "arise, such as the fact that the apostrophe needs to be tokenized differently when used\n",
      "as a genitive marker (as in the book’s cover), a quotative as in ‘The other class’, she\n",
      "Bird, Loper and Klein (2009), Natural Language Processing with Python. O’Reilly\n",
      "\n",
      "\n",
      "Tokenization in languages without spaces \n",
      "Many languages (like Chinese, Japanese, Thai) don't \n",
      "use spaces to separate words!\n",
      "How do we decide where the token boundaries \n",
      "should be?\n",
      "\n",
      "\n",
      "Word tokenization in Chinese\n",
      "Chinese words are composed of characters called \n",
      "\"hanzi\" (or sometimes just \"zi\")\n",
      "Each one represents a meaning unit called a morpheme.\n",
      "Each word has on average 2.4 of them.\n",
      "But deciding what counts as a word is complex and not \n",
      "agreed upon.\n",
      "\n",
      "\n",
      "How to do word tokenization in Chinese?\n",
      "姚明进入总决赛  “Yao Ming reaches the finals”\n",
      "3 words?\n",
      "姚明        进入\n",
      "总决赛\n",
      "YaoMing  reaches  finals \n",
      "5 words?\n",
      "姚      明\n",
      "进入\n",
      "总\n",
      "决赛\n",
      "Yao    Ming    reaches    overall    finals \n",
      "7 characters? (don't use words at all):\n",
      "姚明\n",
      "进\n",
      "入\n",
      "总\n",
      "决\n",
      "赛\n",
      "Yao Ming enter enter overall decision game\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>姚明进入总决赛 “Yao Ming reaches the finals”</th>\n",
      "      <th>Col1</th>\n",
      "      <th>Col2</th>\n",
      "      <th>Col3</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>3 words?\\n姚明 进入 总决赛\\nYaoMing reaches finals</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>5 words?\\n姚 明 进入 总 决赛\\nYao Ming reaches overall finals\\n7 characters? (don't use words at all):\\n姚 明 进 入 总 决 赛\\nYao Ming enter enter overall decision game</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>None</td>\n",
      "      <td>7 characters? (don't use words at all):\\n姚 明 进 入 总 决 赛\\nYao Ming enter enter overall decision game</td>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "How to do word tokenization in Chinese?\n",
      "姚明进入总决赛  “Yao Ming reaches the finals”\n",
      "3 words?\n",
      "姚明        进入\n",
      "总决赛\n",
      "YaoMing  reaches  finals \n",
      "5 words?\n",
      "姚      明\n",
      "进入\n",
      "总\n",
      "决赛\n",
      "Yao    Ming    reaches    overall    finals \n",
      "7 characters? (don't use words at all):\n",
      "姚明\n",
      "进\n",
      "入\n",
      "总\n",
      "决\n",
      "赛\n",
      "Yao Ming enter enter overall decision game\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>3 words? 姚明    进入 总决赛 YaoMing reaches finals</th>\n",
      "      <th>Col1</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>5 words?\\n姚 明 进入 总 决赛\\nYao Ming reaches overall finals\\n7 characters? (don't use words at all):\\n姚 明 进 入 总 决 赛\\nYao Ming enter enter overall decision game</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>7 characters? (don't use words at all):\\n姚 明 进 入 总 决 赛\\nYao Ming enter enter overall decision game</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "How to do word tokenization in Chinese?\n",
      "姚明进入总决赛  “Yao Ming reaches the finals”\n",
      "3 words?\n",
      "姚明        进入\n",
      "总决赛\n",
      "YaoMing  reaches  finals \n",
      "5 words?\n",
      "姚      明\n",
      "进入\n",
      "总\n",
      "决赛\n",
      "Yao    Ming    reaches    overall    finals \n",
      "7 characters? (don't use words at all):\n",
      "姚明\n",
      "进\n",
      "入\n",
      "总\n",
      "决\n",
      "赛\n",
      "Yao Ming enter enter overall decision game\n",
      "\n",
      "\n",
      "How to do word tokenization in Chinese?\n",
      "姚明进入总决赛  “Yao Ming reaches the finals”\n",
      "3 words?\n",
      "姚明        进入\n",
      "总决赛\n",
      "YaoMing  reaches  finals \n",
      "5 words?\n",
      "姚      明\n",
      "进入\n",
      "总\n",
      "决赛\n",
      "Yao    Ming    reaches    overall    finals \n",
      "7 characters? (don't use words at all):\n",
      "姚明\n",
      "进\n",
      "入\n",
      "总\n",
      "决\n",
      "赛\n",
      "Yao Ming enter enter overall decision game\n",
      "\n",
      "\n",
      "Word tokenization / segmentation\n",
      "So in Chinese it's common to just treat each character \n",
      "(zi) as a token.\n",
      "• So the segmentation step is very simple\n",
      "In other languages (like Thai and Japanese), more \n",
      "complex word segmentation is required.\n",
      "• The standard algorithms are neural sequence models \n",
      "trained by supervised machine learning.\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Word tokenization\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Word Normalization and \n",
      "other issues\n",
      "\n",
      "\n",
      "Word Normalization\n",
      "Putting words/tokens in a standard format\n",
      "◦U.S.A. or USA\n",
      "◦uhhuh or uh-huh\n",
      "◦Fed or fed\n",
      "◦am, is, be, are \n",
      "\n",
      "\n",
      "Case folding\n",
      "Applications like IR: reduce all letters to lower case\n",
      "◦Since users tend to use lower case\n",
      "◦Possible exception: upper case in mid-sentence?\n",
      "◦e.g., General Motors\n",
      "◦Fed vs. fed\n",
      "◦SAIL vs. sail\n",
      "For sentiment analysis, MT, Information extraction\n",
      "◦Case is helpful (US versus us is important)\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "Represent all words as their lemma, their shared root \n",
      " \n",
      "= dictionary headword form:\n",
      "◦am, are, is ® be\n",
      "◦car, cars, car's, cars' ® car\n",
      "◦Spanish quiero (‘I want’), quieres (‘you want’) \n",
      "® querer ‘want'\n",
      "◦He is reading detective stories \n",
      "® He be read detective story \n",
      "\n",
      "\n",
      "Lemmatization is done by Morphological Parsing\n",
      "Morphemes:\n",
      "◦The small meaningful units that make up words\n",
      "◦Stems: The core meaning-bearing units\n",
      "◦Affixes: Parts that adhere to stems, often with grammatical \n",
      "functions\n",
      "Morphological Parsers:\n",
      "◦Parse  cats into two morphemes cat and s\n",
      "◦Parse Spanish amaren (‘if in the future they would love’) into \n",
      "morpheme amar ‘to love’, and the morphological features \n",
      "3PL and future subjunctive. \n",
      "\n",
      "\n",
      "Stemming\n",
      "Reduce terms to stems, chopping off affixes crudely\n",
      "This was not the map we \n",
      "found in Billy Bones’s \n",
      "chest, but an accurate \n",
      "copy, complete in all \n",
      "things-names and heights \n",
      "and soundings-with the \n",
      "single exception of the \n",
      "red crosses and the \n",
      "written notes. \n",
      "Thi wa not the map we \n",
      "found in Billi Bone s chest \n",
      "but an accur copi complet \n",
      "in all thing name and \n",
      "height and sound with the \n",
      "singl except of the red \n",
      "cross and the written note \n",
      ". \n",
      "\n",
      "\n",
      "Porter Stemmer\n",
      "Based on a series of rewrite rules run in series\n",
      "◦A cascade, in which output of each pass fed to next pass\n",
      "Some sample rules:\n",
      "and soundings-with the single exception of the red crosses\n",
      "and the written notes.\n",
      "produces the following stemmed output:\n",
      "Thi wa not the map we found in Billi Bone s chest but an\n",
      "accur copi complet in all thing name and height and sound\n",
      "with the singl except of the red cross and the written note\n",
      "The algorithm is based on series of rewrite rules run in series, as a cascade, in\n",
      "ascade\n",
      "which the output of each pass is fed as input to the next pass; here is a sampling of\n",
      "the rules:\n",
      "ATIONAL ! ATE (e.g., relational ! relate)\n",
      "ING ! ✏\n",
      "if stem contains vowel (e.g., motoring ! motor)\n",
      "SSES ! SS\n",
      "(e.g., grasses ! grass)\n",
      "Detailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)\n",
      "can be found on Martin Porter’s homepage; see also the original paper (Porter, 1980).\n",
      "Simple stemmers can be useful in cases where we need to collapse across differ-\n",
      "ent variants of the same lemma. Nonetheless, they do tend to commit errors of both\n",
      "over- and under-generalizing, as shown in the table below (Krovetz, 1993):\n",
      "Errors of Commission\n",
      "Errors of Omission\n",
      "organization organ\n",
      "European Europe\n",
      "doing\n",
      "doe\n",
      "analysis\n",
      "analyzes\n",
      "\n",
      "\n",
      "Dealing with complex morphology is necessary \n",
      "for many languages\n",
      "◦e.g., the Turkish word:\n",
      "◦Uygarlastiramadiklarimizdanmissinizcasina\n",
      "◦`(behaving) as if you are among those whom we could not civilize’\n",
      "◦Uygar `civilized’ + las `become’ \n",
      "+ tir `cause’ + ama `not able’ \n",
      "+ dik `past’ + lar ‘plural’\n",
      "+ imiz ‘p1pl’ + dan ‘abl’ \n",
      "+ mis ‘past’ + siniz ‘2pl’ + casina ‘as if’ \n",
      "\n",
      "\n",
      "Sentence Segmentation\n",
      "!, ? mostly unambiguous but period “.” is very ambiguous\n",
      "◦Sentence boundary\n",
      "◦Abbreviations like Inc. or Dr.\n",
      "◦Numbers like .02% or 4.3\n",
      "Common algorithm: Tokenize first: use rules or ML to \n",
      "classify a period as either (a) part of the word or (b) a \n",
      "sentence-boundary. \n",
      "◦An abbreviation dictionary can help\n",
      "Sentence segmentation can then often be done by rules \n",
      "based on this tokenization.\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Word Normalization and \n",
      "other issues\n",
      "\n",
      "\n",
      "Basic Text \n",
      "Processing\n",
      "Byte Pair Encoding\n",
      "\n",
      "\n",
      "Another option for text tokenization\n",
      "Instead of \n",
      "• white-space segmentation\n",
      "• single-character segmentation \n",
      "Use the data to tell us how to tokenize.\n",
      "Subword tokenization (because tokens can be parts \n",
      "of words as well as whole words)\n",
      "\n",
      "\n",
      "Subword tokenization\n",
      "Three common algorithms:\n",
      "◦Byte-Pair Encoding (BPE) (Sennrich et al., 2016)\n",
      "◦Unigram language modeling tokenization (Kudo, 2018)\n",
      "◦WordPiece (Schuster and Nakajima, 2012)\n",
      "All have 2 parts:\n",
      "◦A token learner that takes a raw training corpus and induces \n",
      "a vocabulary (a set of tokens). \n",
      "◦A token segmenter that takes a raw test sentence and \n",
      "tokenizes it according to that vocabulary\n",
      "\n",
      "\n",
      "Byte Pair Encoding (BPE) token learner\n",
      "Let vocabulary be the set of all individual characters \n",
      " \n",
      "= {A, B, C, D,…, a, b, c, d….}\n",
      "Repeat:\n",
      "◦Choose the two symbols that are most frequently \n",
      "adjacent in the training corpus (say 'A', 'B') \n",
      "◦Add a new merged symbol 'AB' to the vocabulary\n",
      "◦Replace every adjacent 'A' 'B' in the corpus with 'AB'. \n",
      "Until k merges have been done.\n",
      "\n",
      "\n",
      "BPE token learner algorithm\n",
      "2.4\n",
      "•\n",
      "TEXT NORMALIZATION\n",
      "19\n",
      "function BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\n",
      "V all unique characters in C\n",
      "# initial set of tokens is characters\n",
      "for i = 1 to k do\n",
      "# merge tokens til k times\n",
      "tL, tR  Most frequent pair of adjacent tokens in C\n",
      "tNEW  tL + tR\n",
      "# make new token by concatenating\n",
      "V V + tNEW\n",
      "# update the vocabulary\n",
      "Replace each occurrence of tL, tR in C with tNEW\n",
      "# and update the corpus\n",
      "return V\n",
      "Figure 2.13\n",
      "The token learner part of the BPE algorithm for taking a corpus broken up\n",
      "into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\n",
      "Figure adapted from Bostrom and Durrett (2020).\n",
      "from the training data, greedily, in the order we learned them. (Thus the frequencies\n",
      "in the test data don’t play a role, just the frequencies in the training data). So first\n",
      "we segment each test sentence word into characters. Then we apply the first rule:\n",
      "\n",
      "\n",
      "Byte Pair Encoding (BPE) Addendum\n",
      "Most subword algorithms are run inside space-\n",
      "separated tokens. \n",
      "So we commonly first add a special end-of-word \n",
      "symbol '__' before space in training corpus\n",
      "Next, separate into letters.\n",
      "\n",
      "\n",
      "BPE token learner\n",
      "ER 2\n",
      "•\n",
      "REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\n",
      "The algorithm is usually run inside words (not merging across word boundaries),\n",
      "so the input corpus is first white-space-separated to give a set of strings, each corre-\n",
      "sponding to the characters of a word, plus a special end-of-word symbol\n",
      ", and its\n",
      "counts. Let’s see its operation on the following tiny input corpus of 18 word tokens\n",
      "with counts for each word (the word low appears 5 times, the word newer 6 times,\n",
      "and so on), which would have a starting vocabulary of 11 letters:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w e r\n",
      "3\n",
      "w i d e r\n",
      "2\n",
      "n e w\n",
      "The BPE algorithm first count all pairs of adjacent symbols: the most frequent\n",
      "is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\n",
      "3) for a total of 9 occurrences1. We then merge these symbols, treating er as one\n",
      "symbol, and count again:\n",
      "Original (very fascinating🙄) corpus:\n",
      "low low low low low lowest lowest newer newer newer    \n",
      "newer newer newer wider wider wider new new\n",
      "Add end-of-word tokens, resulting in this vocabulary:\n",
      "representation\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide discussing the BPE (Byte Pair Encoding) token learner algorithm. The original, very fascinating corpus mentioned refers to the input text that the algorithm processes. The slide explains how BPE works by iteratively replacing the lowest frequency word pair with a new token until no more pairs can be replaced. The resulting vocabulary, listed as an example in the slide, is a set of tokens derived from the corpus through this process. This technique is essential in NLP for creating better word representations and improving models' performance on tasks like language translation and text summarization. [IDE]\n",
      "\n",
      "\n",
      "BPE token learner\n",
      "sponding to the characters of a word, plus a special end-of-word symbol\n",
      ", and its\n",
      "counts. Let’s see its operation on the following tiny input corpus of 18 word tokens\n",
      "with counts for each word (the word low appears 5 times, the word newer 6 times,\n",
      "and so on), which would have a starting vocabulary of 11 letters:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w e r\n",
      "3\n",
      "w i d e r\n",
      "2\n",
      "n e w\n",
      "The BPE algorithm first count all pairs of adjacent symbols: the most frequent\n",
      "is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\n",
      "3) for a total of 9 occurrences1. We then merge these symbols, treating er as one\n",
      "symbol, and count again:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "Merge e r to er\n",
      "and so on), which would have a starting vocabulary of 11 letters:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w e r\n",
      "3\n",
      "w i d e r\n",
      "2\n",
      "n e w\n",
      "The BPE algorithm first count all pairs of adjacent symbols: the most frequent\n",
      "is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\n",
      "3) for a total of 9 occurrences1. We then merge these symbols, treating er as one\n",
      "symbol, and count again:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er\n",
      "2\n",
      "l o w e s t\n",
      "\n",
      "\n",
      "BPE\n",
      "Merge er  _ to er_\n",
      "The BPE algorithm first count all pairs of adjacent symbols: the most frequent\n",
      "is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\n",
      "3) for a total of 9 occurrences1. We then merge these symbols, treating er as one\n",
      "symbol, and count again:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Next n e (total count of 8) get merged to ne:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "ne w er\n",
      "is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\n",
      "3) for a total of 9 occurrences1. We then merge these symbols, treating er as one\n",
      "symbol, and count again:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Next n e (total count of 8) get merged to ne:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "ne w er\n",
      "\n",
      "\n",
      "BPE\n",
      "Merge n  e  to ne\n",
      "3\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Next n e (total count of 8) get merged to ne:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "ne w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "ne w\n",
      "If we continue, the next merges are:\n",
      "Merge\n",
      "Current Vocabulary\n",
      "(ne, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new\n",
      "(l, o)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo\n",
      "(lo, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low\n",
      "(new, er )\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Now the most frequent pair is er\n",
      ", which we merge; our system has learned\n",
      "that there should be a token for word-final er, represented as er :\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "n e w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "n e w\n",
      "Next n e (total count of 8) get merged to ne:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "ne w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "ne w\n",
      "If we continue, the next merges are:\n",
      "Merge\n",
      "Current Vocabulary\n",
      "(ne, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new\n",
      "(l, o)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo\n",
      "(lo, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low\n",
      "(new, er )\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer\n",
      "\n",
      "\n",
      "BPE\n",
      "The next merges are:\n",
      "corpus\n",
      "vocabulary\n",
      "5\n",
      "l o w\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne\n",
      "2\n",
      "l o w e s t\n",
      "6\n",
      "ne w er\n",
      "3\n",
      "w i d er\n",
      "2\n",
      "ne w\n",
      "If we continue, the next merges are:\n",
      "Merge\n",
      "Current Vocabulary\n",
      "(ne, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new\n",
      "(l, o)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo\n",
      "(lo, w)\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low\n",
      "(new, er )\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer\n",
      "(low,\n",
      ")\n",
      ", d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer , low\n",
      "Once we’ve learned our vocabulary, the token parser is used to tokenize a te\n",
      "sentence. The token parser just runs on the test data the merges we have learne\n",
      "1\n",
      "Note that there can be ties; we could have instead chosen to merge r\n",
      "first, since that also has\n",
      "frequency of 9.\n",
      "\n",
      "\n",
      "BPE token segmenter algorithm\n",
      "On the test data, run each merge learned from the \n",
      "training data:\n",
      "◦Greedily\n",
      "◦In the order we learned them\n",
      "◦(test frequencies don't play a role)\n",
      "So: merge every e r to er, then merge er _ to er_, etc.\n",
      "Result: \n",
      "◦Test set \"n e w e r _\" would be tokenized as a full word \n",
      "◦Test set \"l o w e r _\" would be two tokens: \"low er_\"\n",
      "\n",
      "\n",
      "Properties of BPE tokens\n",
      "Usually include frequent words\n",
      "And frequent subwords\n",
      "• Which are often morphemes like -est or –er\n",
      "A morpheme is the smallest meaning-bearing unit of a \n",
      "language\n",
      "• unlikeliest has 3 morphemes un-, likely, and -est \n",
      "\n",
      "lecture_questions:\n",
      "In the context of BPE tokenization, what is the significance of merging less frequent pairs before more frequent ones during the training phase?\n",
      "What role do morphemes play in subword tokenization methods like BPE and why are they often represented as part of a token in text processed by such algorithms? \n",
      "In the context of NLP, how might sentence segmentation impact the effectiveness of a byte pair encoding token learner and what precautions can be taken to avoid any negative impact of mis-segmented sentences on the learning process?\n",
      "What is the role of the Byte Pair Encoding (BPE) token learner algorithm in text preprocessing for NLP?\n",
      "\n",
      "A) It separates punctuation from words.\n",
      "\n",
      "B) It normalizes case folding, making everything lowercase or uppercase as needed.\n",
      "\n",
      "C) It learns subword tokens that can represent frequent word subparts and are often morphemes\n",
      "\n",
      "D) It performs full stemming of the words in a corpus.\n",
      "Based on the function `get_splits`, what does the dictionary created by this function represent? Can you provide an example of a key and its associated value in the output of this function for the input `pretokenize([[\"apple\", \"banana\", \"apple\"], [\"apple\"]])`?\n",
      "\n",
      "\n",
      "\n",
      "Code Snippet:\n",
      "\n",
      "```python\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "from typing import Dict, Tuple\n",
      "\n",
      "\n",
      "\n",
      "def get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\n",
      "\n",
      "    \"\"\"Get subword splits of tokens in a corpus.\n",
      "\n",
      "    \n",
      "\n",
      "    Args:\n",
      "\n",
      "        corpus (List[List[str]]): A list of sentences where each sentence is represented\n",
      "\n",
      "            as a list of tokens.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\n",
      "\n",
      "            and values are the counts of occurrences of those splits in the corpus.\n",
      "\n",
      "    \n",
      "\n",
      "    Example: \n",
      "\n",
      "        >>> corpus = [[\"apple\", \"banana\", \"apple\"], [\"apple\"]]\n",
      "\n",
      "        >>> get_splits(corpus)\n",
      "\n",
      "        {('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
      "\n",
      "```\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Large Language Models\n",
      "Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)\n",
      "\n",
      "[IDS] The image you're seeing is a visual representation of the evolution and structure of Large Language Models (LLMs) in Natural Language Processing (NLP). It's likely part of a lecture or presentation on NLP, specifically focusing on the Transformer decoder and Large Language Models. The figure shows how different models have been developed over time, starting from GPT-1 in 2015 to more recent models like GPT-7 and GShard. Each model is represented by a node in the tree, with lines indicating the direction of development and connections showing the relationships between them. This kind of visualization helps students and professionals understand the progression and interconnections within the field of LLMs. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Large Language Models\n",
      "Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)\n",
      "Encoder Models (previous \n",
      "lecture on Transformers)\n",
      "\n",
      "[IDS] The image is a visual representation of the evolution of transformer models and large language models, which are key topics in natural language processing (NLP). The flowchart starts with the basic architecture of transformers and moves towards more advanced and recent models. It's likely that this chart was used in a lecture to help students understand how these models have developed over time and their interconnections. As we are in a NLP lecture, this image serves as a guide to understanding the complex landscape of transformer models and their role in NLP tasks such as language generation and translation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Large Language Models\n",
      "Fig. 1: LLM Evolutionary Tree (https://github.com/Mooler0410/LLMsPracticalGuide)\n",
      "Decoder Models \n",
      "(this lecture)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the evolution of language models and their impact on natural language processing. The slide presents a visual representation of the progression from early language models to more advanced ones like GPT-3 and beyond. It highlights the role of large language models in tasks such as translation and summarization, emphasizing their importance in the field. Additionally, there's a reference to a decoder model, which is likely a specific type of neural network used for generating text or making predictions in sequence-to-sequence tasks. The lecture aims to provide an overview of the current state-of-the-art in language modeling and its potential applications. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Architecture\n",
      "Fig. 2: Transformer Encoder Decoder \n",
      "(source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image you see is a schematic representation of the Transformer architecture, which is a fundamental component in large language models. The Transformer model, proposed by Vaswani et al. in 2017, has become a cornerstone in natural language processing tasks due to its ability to process sequential data effectively. The diagram illustrates the encoder and decoder components of the Transformer model, with attention mechanisms that allow the model to focus on different parts of the input sequence during translation. Understanding this architecture is crucial for NLP lectures as it forms the basis for many state-of-the-art models used in tasks such as machine translation, text generation, and question answering. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Architecture\n",
      "Fig. 2: Transformer Encoder Decoder \n",
      "(source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In the context of a NLP lecture, specifically on the topic \"Transformer Decoder and Large Language Models,\" this image serves as a visual aid to explain the architecture of Transformer models. It highlights the components and flow of information within a Transformer Encoder Decoder structure. Understanding this architecture is crucial for grasping how these models process input data and generate outputs in tasks such as language translation or text generation. The image's focus on the encoder-decoder aspect suggests that attention mechanisms are key to the model's ability to understand and respond to input, which is a central theme in large language models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder\n",
      "Fig. 3: Transformer Decoder (source: \n",
      "Attention is all you need. Vaswani et \n",
      "al. 2017)\n",
      "●We do not have an encoder so we throw \n",
      "away the Multi-Head Attention block\n",
      "\n",
      "[IDS] The image represents a simplified explanation of how the Transformer Decoder works in the context of Large Language Models within Natural Language Processing (NLP). The lecture seems to focus on understanding the architecture and components of these models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder\n",
      "●We do not have an encoder so we throw \n",
      "away the Multi-Head Attention block\n",
      "●This is the basic GPT architecture \n",
      "(generative pretrained transformer)\n",
      "●This decoder can generate text by \n",
      "predicting the next token\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the Transformer Decoder and its role in Large Language Models. The slide illustrates the architecture of a Transformer Decoder, which is a key component in GPT (Generative Pre-trained Transformer) models. It emphasizes that unlike traditional encoders, these decoders do not throw away Multi-Head Attention blocks, which are essential for generating text predictions. The diagram shows the flow of data through the decoder, including positional encoding and masked multi-head attention, which allows the model to generate text from the next token. The slide also references Figure 4, which details the Transformer Decoder architecture and mentions Vaswani et al. (2017), indicating that this is a foundational work in the field. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder - Inputs\n",
      "●We start with a partial sequence. \n",
      "Example: <s> Hello\n",
      "●We tokenize this: [‘<s>’, ‘Hello’]\n",
      "●We lookup the token ids: [0, 245]\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the Transformer Decoder, a key component of large language models. The diagram illustrates the architecture of the Transformer Decoder, which consists of multiple layers of attention mechanisms and feed-forward networks. These layers process the input tokens sequentially to generate the output tokens. The decoder starts with a partial sequence of examples and tokenizes it using a special token like '<s>'. It then looks up the token IDs in a vocabulary of size 245. This process involves positional encoding to maintain the order of tokens and outputs a shifted right sequence of outputs. The lecture emphasizes the importance of attention in the Transformer Decoder, as it allows the model to focus on different parts of the input sequence while generating the output. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder - Inputs\n",
      "●We start with a partial sequence. \n",
      "Example: <s> Hello\n",
      "●We tokenize this: [‘<s>’, ‘Hello’]\n",
      "●We lookup the token ids: [0, 245]\n",
      "●We start with initial embeddings: \n",
      "[E<s>,EHello]\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual representation of a Transformer Decoder, a component used in large language models within the field of Natural Language Processing (NLP). It illustrates the process of decoding and generating text from encoded input. The decoder consists of multiple layers, each with self-attention mechanisms that allow it to consider different parts of the input sequence simultaneously. This architecture enables the model to generate coherent and contextually relevant outputs. The figure serves as an educational tool to help students and researchers understand how the Transformer Decoder works and its role in NLP tasks such as machine translation and language generation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder - Inputs\n",
      "●We start with a partial sequence. \n",
      "Example: <s> Hello\n",
      "●We tokenize this: [‘<s>’, ‘Hello’]\n",
      "●We lookup the token ids: [0, 245]\n",
      "●We start with initial embeddings: \n",
      "[y<s>,yHello]\n",
      "●We create the positional embeddings:\n",
      "[P0, P1]\n",
      "●We add these together:\n",
      "[y<s>+P0, yHello+P1] = [x<s>, xHello]\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image represents a section of a lecture on Natural Language Processing (NLP), specifically focusing on Transformer Decoders and Large Language Models. It visually outlines the process of how a decoder, part of a Transformer architecture, works to generate text from a partial sequence. The diagram breaks down the steps taken by the decoder, including tokenization, initial embeddings, positional encodings, and finally outputs the next tokens in the sequence. This is a fundamental concept in NLP, particularly in the field of machine translation and language generation tasks, where such models are used to produce coherent and contextually relevant text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder - Attention\n",
      "●Our embeddings: [x<s>, xHello]\n",
      "●Perform Multi-Head Attention:\n",
      "Attention weight matrix = QKT\n",
      "These tell us how much information about \n",
      "the other tokens we should take into \n",
      "account for each token.\n",
      "●We mask the attention weights, such that \n",
      "a token can not attend to any subsequent \n",
      "token. In this example the embedding for \n",
      "<s> should not get any information about \n",
      "what comes after it.\n",
      "This is called a causal model.\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual aid from a lecture on NLP, specifically focusing on the Transformer Decoder and Large Language Models. It outlines the process of how embeddings are used to inform attention weights in a multi-head attention mechanism within a Transformer Decoder architecture. The flowchart details the sequence of operations starting from input embeddings, through positional encoding, to the final outputs. This process is crucial for models like GPT-3 or BERT that rely on attention mechanisms to understand and generate text. The lecture aims to provide insight into how these models work internally, emphasizing the importance of understanding what information tokens should take into account for each token in the sequence. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder - Outputs\n",
      "●After feeding the embeddings through N \n",
      "transformer layers, we get our output \n",
      "embeddings: [o<s>, oHello]. This is a matrix \n",
      "of sequence length times embedding size\n",
      "●Now we feed this into a linear layer to \n",
      "map this to a vector of vocabulary size.\n",
      "●Finally we perform a softmax.\n",
      "●We now have a probability distribitution \n",
      "over all tokens in the vocabulary\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the Transformer Decoder and its role in large language models. The diagram illustrates the process of output probabilities and how they are derived from embedding sequences. The Transformer Decoder is a key component in these models, enabling the generation of output tokens by processing input sequences through a series of attention mechanisms and feed forward layers. The lecture emphasizes the importance of positional encoding to maintain the order of the sequence and references the work of Vaswani et al. (2017) for further understanding. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder – Next Token\n",
      "●We now have a probability vector that \n",
      "tells us how likely the next word is token \n",
      "0, 1, 2, 3, …\n",
      "●We can either take the argmax or sample \n",
      "from this distribution (roulette wheel \n",
      "sampling)\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "Next Token \n",
      "ID\n",
      "Next Token\n",
      "Probability\n",
      "75\n",
      "World\n",
      "0.75\n",
      "5\n",
      "there\n",
      "0.12\n",
      "109\n",
      ",\n",
      "0.08\n",
      "20343\n",
      "everyone\n",
      "0.03\n",
      "3\n",
      "folks\n",
      "0.02\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture, specifically discussing the Transformer Decoder and its role in large language models. It outlines the process of decoding next tokens using a probability vector, which is determined by the softmax function applied to linear transformations of the encoder's outputs. The decoder can either take argmax or sample from the distribution provided by the roulette wheel sampling method. This is illustrated with a flowchart showing the components involved, such as masked multi-head attention, positional encoding, and output embedding. Additionally, there's a mention of Figure 4, which explains how the Transformer Decoder works (adapted from Vaswani et al., 2017). The slide also references the Hochschule Bonn-Rhein-Sieg and Tim Metzler, possibly indicating the institution and the lecturer or author of the presentation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder – Next Token\n",
      "●We now have a probability vector that \n",
      "tells us how likely the next word is token \n",
      "0, 1, 2, 3, …\n",
      "●We can either take the argmax or sample \n",
      "from this distribution (roulette wheel \n",
      "sampling)\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "Next Token \n",
      "ID\n",
      "Next Token\n",
      "Probability\n",
      "75\n",
      "World\n",
      "0.75\n",
      "5\n",
      "there\n",
      "0.12\n",
      "109\n",
      ",\n",
      "0.08\n",
      "20343\n",
      "everyone\n",
      "0.03\n",
      "3\n",
      "folks\n",
      "0.02\n",
      "\n",
      "[IDS] In the context of a NLP lecture on Transformer Decoder and Large Language Models, the image represents a crucial aspect of how these models process and generate text. It illustrates the mechanism behind predicting the next word in a sequence, which is a fundamental component of language generation tasks. The diagram provides a visual explanation of the softmax function and the probability vectors used to determine the most likely next word based on the context provided by the model's previous outputs. This understanding is essential for students studying NLP as it helps them grasp how these advanced models can be trained to produce coherent and contextually relevant text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder – Next Iteration\n",
      "●Assume we picked the token “there” with \n",
      "the id 5 as the next token.\n",
      "●We append this to the previous input \n",
      "(shifted right)\n",
      "●We repeat this process until we hit a \n",
      "maximum sequence length or produce the \n",
      "end of sentence token </s>\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "there\n",
      "<s> Hello there\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the concept of a Transformer Decoder as part of large language models. The slide explains the process of picking a token from the previous input (shifted right) and then repeating the process until a maximum sequence length is reached or the end of the sentence is encountered. This is illustrated with a flowchart that includes components like attention, positional encoding, and output embedding. Additionally, there's a mention of a paper by Vaswani et al. from 2017, which is likely the foundational research for this topic. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Decoder – Training\n",
      "●Take a training sequence:\n",
      "”<s> I like to eat pizza </s>”\n",
      "●Predict the next token from “<s>”\n",
      "BCE Loss between the output \n",
      "probabilities and the correct token “I”.\n",
      "●Feed the sequence “<s> I” into the model.\n",
      "●Predict the next token\n",
      "BCE Loss\n",
      "●Feed the correct next token to the input:\n",
      "”<s> I like”\n",
      "●Repeat for all training sequences\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual representation of the Transformer Decoder and its role in large language models, specifically focusing on how it's used during training. It illustrates the process of taking a training sequence, predicting the next token from the sequence, and feeding that prediction back into the model for further training. The diagram shows the structure of the Transformer Decoder with attention mechanisms, indicating how it processes and learns from input sequences to generate coherent outputs. This is a fundamental concept in natural language processing (NLP), where such models are trained to understand and generate human-like text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Generative Pre-trained Transformer\n",
      "●GPT-1 (06/2018): 117M parameters\n",
      "Trained on BookCorpus (7000 unpublished \n",
      "books)\n",
      "●GPT-2 (02/2019, 11/2019): 1.5B parameters\n",
      "Trained on Web, Reddit, BookCorpus\n",
      "●GPT-3 (05/2020): 175B parameters\n",
      "Trained on filtered Common Crawl, Web, \n",
      "Books, Wikipedia\n",
      "Fig. 4: Transformer Decoder (adapted \n",
      "from: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image represents a diagram of the architecture of a Generative Pre-trained Transformer (GPT) model, which is a type of large language model commonly used in natural language processing (NLP). In the context of an NLP lecture, this diagram would be used to explain how the GPT model processes input text and generates output. It illustrates the flow of information through various layers of the model, including the embedding layer, the transformer decoder, and the attention mechanism. This is crucial for understanding how these models are trained on large datasets like BookCorpus and filtered Common Crawl data, and how they can be fine-tuned for specific tasks such as language translation or summarization. The reference to \"Transformer Decoder\" adapted from Vaswani et al. 2017 further emphasizes the foundational research in the field that has led to the development of these advanced language models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Generative Pre-trained Transformer\n",
      "What can these models do?\n",
      "●Predict the next token\n",
      "●Translate\n",
      "●Summarize\n",
      "●Code generation\n",
      "●...\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture discussing the capabilities of Generative Pre-trained Transformer models. It poses the question of what these models can do, listing tasks such as predicting the next token, translating, summarizing, and code generation. The slide is likely part of a presentation given by Tim Metzler at Hochschule Bonn-Rhein-Sieg, focusing on the decoder aspect of Transformer models and their applications in large language models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Generative Pre-trained Transformer\n",
      "What can these models do?\n",
      "●Predict the next token\n",
      "●Translate\n",
      "●Summarize\n",
      "●Code generation\n",
      "●...\n",
      "Why can these models do that?\n",
      "\n",
      "[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP). It discusses Generative Pre-trained Transformer models, which are a type of deep learning model used in NLP tasks. The slide is asking questions about the capabilities of these models, such as predicting the next token, translating, summarizing, and code generation. It also prompts the audience to consider why these models can perform these tasks. The slide includes logos of the Hochschule Bonn-Rhein-Sieg and Tim Metzler, suggesting that this is an educational context, possibly a course or seminar at the mentioned institution. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Language Models are Unsupervised Multitask Learners”\n",
      "Radford et al (OpenAI), 2019\n",
      "From the abstract:\n",
      "”Natural language processing tasks, such as \n",
      "question answering, machine translation, reading \n",
      "comprehension, and summarization, are typically\n",
      "approached with supervised learning on task-\n",
      "specific datasets. We demonstrate that language\n",
      "models begin to learn these tasks without any ex-\n",
      "plicit supervision when trained on a new dataset\n",
      "of millions of webpages called WebText. “\n",
      "\n",
      "[IDS] The image seems to be a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the Transformer Decoder and Large Language Models. It references an abstract from a paper by Radford et al published in OpenAI in 2019, which discusses the use of unsupervised multitask learning for tasks such as question answering, machine translation, reading comprehension, and summarization. The slide also includes logos of Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler's name, indicating the possible affiliation or authorship related to the content of the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Language Models are Unsupervised Multitask Learners”\n",
      "Radford et al (OpenAI), 2019\n",
      "Closer look into the training data:\n",
      "\n",
      "[IDS] The image depicts a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of Transformer decoder and Large Language Models. The slide, titled \"Language Models are Unsupervised Multitask Learners,\" is authored by Radford et al (OpenAI), dated 2019. It provides insights into how language models can learn to perform various tasks without explicit supervision. The slide contains text in French and English, highlighting the versatility of these models. Additionally, there's a reference to a study by Hochschule Bonn-Rhein-Sieg, suggesting a practical application or research related to the topic. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Language Models are Unsupervised Multitask Learners”\n",
      "Radford et al (OpenAI), 2019\n",
      "Closer look into the training data:\n",
      "Using this knowledge to translate sentences \n",
      "with GPT2 \n",
      "(https://transformer.huggingface.co/doc/gpt2-\n",
      "large)\n",
      "\n",
      "[IDS] The image is a snapshot from an NLP lecture focused on \"Transformer_decoder_and_Large_Langauge_Models\". It showcases the capabilities of language models like GPT2 in understanding and generating human-like responses. The lecture includes a demonstration of how a German friend's statement is translated to English, emphasizing the model's ability to handle multilingual conversations and context-aware responses. The slide also provides examples from a textbook to illustrate the natural demonstrations of English to French and French to English translation, highlighting the WebText training set used for this purpose. The content suggests that the lecture aims to educate about the workings and applications of transformer-based large language models in natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Language Models are Unsupervised Multitask Learners”\n",
      "Radford et al (OpenAI), 2019\n",
      "Exercise:\n",
      "Go to this website and try to get the model to \n",
      "perform one of the following tasks:\n",
      "- Translation\n",
      "- Sentiment Analysis\n",
      "- Summarization\n",
      "- Question Answering \n",
      "(https://transformer.huggingface.co/doc/gpt2-\n",
      "large)\n",
      "\n",
      "[IDS] In the image, we see a slide from an NLP lecture discussing \"Language Models are Unsupervised Multitask Learners\" by Radford et al (OpenAI), 2019. The slide is numbered 2 and contains a set of tasks for students to practice their skills in website navigation and model performance evaluation. These tasks include Translation, Sentiment Analysis, Summarization, Question Answering, and more. There's also a QR code linking to a repository on GitHub for further exploration. The footer of the slide mentions Hochschule Bonn-Rhein-Sieg and the names Tim Metzler and Fachbereich Informatik. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Instruct GPT\n",
      "GPT has the ability to perform a variety of \n",
      "tasks but it is hard to engineer the correct \n",
      "prompt.\n",
      "→ We need to train the model to respond \n",
      "correctly to our prompts:\n",
      "●Current Prompt:\n",
      "My German friend says “Pizza ist mein \n",
      "Lieblingsessen” or in short “\n",
      "●Desired Prompt:\n",
      "Translate “Pizza ist mein Lieblingsessen” to \n",
      "German. \n",
      "\n",
      "[IDS] The image appears to be a slide from an NLP lecture focused on transformer decoders and large language models. The main content is about the capabilities of GPT (Generative Pre-trained Transformer) in performing tasks and its need for training to respond correctly to prompts. The slide mentions a German friend's statement, \"Pizza ist mein Lieblingsessen,\" which translates to \"Pizza is my favorite food.\" This example is used to demonstrate the model's ability to understand and respond in German. The lecture is likely discussing how transformer decoders, like GPT, can be fine-tuned to perform specific tasks or respond to certain prompts effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Instruct GPT\n",
      "\n",
      "[IDS] The image is a screenshot of a lecture slide on Natural Language Processing (NLP), specifically focusing on the topic \"Transformer_decoder_and_Large_Langauge_Models.\" It contains a prompt and a code snippet related to GPT-3, a large language model developed by OpenAI. The code demonstrates how to calculate the binomial coefficient using the GPT-3 model and its instruction set. The slide also includes the names of two contributors, Hochschule Bonn-Rhein-Sieg and Tim Metzler, suggesting their involvement in the content or presentation of the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Instruct GPT\n",
      "How to get from \n",
      "here to there?\n",
      "\n",
      "[IDS] In this NLP lecture, we are exploring the intricacies of Transformer decoders and their role in Large Language Models. The slide provides a detailed explanation of the binomial coefficient function, which is used within the context of GPT models to determine the values stored in a list C. This understanding is crucial for grasping how these models operate and the mathematical principles that underpin them. As we delve deeper into the topic, we will learn about the components and mechanisms that enable these models to process and generate human-like language, a testament to the power of computational linguistics. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "“These models are not aligned with their user.”\n",
      "→ The model might produce content that is \n",
      "not helpful for users, toxic or untruthful. So \n",
      "they need to be aligned to produce the desired \n",
      "output.\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It discusses the importance of training language models to follow instructions with human feedback. The slide highlights that the models should not be aligned with their users and emphasizes the need for alignment between the model's content and the desired output. It also mentions that toxic or untruthful content is not helpful for users, suggesting that the model should be designed to produce content that is aligned with what is considered desirable by society. The slide includes logos of the Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler, who might be the presenter or author of the lecture material. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "\n",
      "[IDS] In this NLP lecture, you're learning about the role of transformer decoders and large language models in natural language processing. Specifically, you're being introduced to the concept of fine-tuning these models with human feedback, as demonstrated by the OpenAI paper \"Ouyang et al (OpenAI), 2021\". This involves collecting demonstration data and training a supervised policy, which is then used to guide the model towards desired outputs. The lecture emphasizes the importance of understanding how these models work internally, as shown by the illustration of the three steps of the fine-tuning process using the SFT (Supervised Fine-tuning) method. This method, which includes a supervised fine-tuning step for one year, is highlighted as a crucial part of the process. The lecture also touches on the broader context of language models and their applications, such as generating text or answering questions, and how they can be improved through fine-tuning and human interaction. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "OpenAI hired 40 contractors to \n",
      "label data. They were selected \n",
      "based on a screening test.\n",
      "\n",
      "[IDS] The image is a visual representation of the process involved in training language models to follow human feedback. In the context of an NLP lecture on transformer decoder and large language models, this diagram illustrates the steps taken by OpenAI in their study using fine-tuning (SFT) and supervised fine-tuning (SFT) with 40 contractors to collect and label data. The labeled data was then used to train the AI model. This process is a common practice in NLP to improve the performance of language models by incorporating human feedback and expertise. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "OpenAI hired 40 contractors to \n",
      "label data. They were selected \n",
      "based on a screening test.\n",
      "Problems:\n",
      "This is an expensive manual \n",
      "process. \n",
      "There are no good large \n",
      "prompt datasets.\n",
      "\n",
      "[IDS] The image depicts a flowchart illustrating the process of training language models to follow instructions with human feedback, which is likely a topic in a NLP lecture. The flowchart outlines the steps taken by OpenAI in their 2021 study using OpenAI hired contractors to label data based on a screening test. This labeled data was then used for fine-tuning GPT-3 with supervised learning. The diagram emphasizes the complexity and the need for an expensive manual process to achieve good large prompt datasets. The lecture might discuss various aspects of transformer decoder and large language models, such as how they are trained, how they handle human feedback, and the challenges associated with creating high-quality datasets for these models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "Training a reward model to \n",
      "predict the ranking allows for \n",
      "large scale training.\n",
      "\n",
      "[IDS] The image illustrates the process of training language models to follow instructions with human feedback, specifically using the OpenAI model from 2021. It's a part of a lecture on NLP (Natural Language Processing) focusing on Transformer Decoder and Large Language Models. The steps depicted are a simplified fine-tuning method (SFT), which is a technique used to improve the performance of pre-trained language models like GPT-3 by training them on specific tasks or datasets with human guidance. This approach allows for large-scale training that can enhance the model's ability to understand and generate human-like text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Training language models to follow instructions with human feedback”\n",
      "Ouyang et al (OpenAI), 2021 \n",
      "\n",
      "[IDS] In the context of a NLP lecture on Transformer decoder and large language models, this image represents the process of training a model to generate human-like responses. The figure illustrates the three-step method used for fine-tuning supervised learning models, which involves collecting demonstration data, comparing and training a reward model, and optimizing the policy against the reward using reinforcement learning. This approach is essential in teaching AI systems to understand and mimic human communication patterns effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Learning to summarize from human feedback”\n",
      "Stiennon et al (OpenAI), 2022 \n",
      "Train several policies using the Reddit TL;DR \n",
      "(too long, didn’t read) dataset.\n",
      "Task: Produce a summary from a text\n",
      "\n",
      "[IDS] In this NLP lecture, we are learning about the various components and techniques used in natural language processing. Specifically, we are focusing on transformer decoders and large language models. These are advanced tools in NLP that help computers understand and generate human-like text. By understanding these concepts, we can improve communication between humans and machines, enabling more efficient and accurate information retrieval and generation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Learning to summarize from human feedback”\n",
      "Stiennon et al (OpenAI), 2022 \n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the process of summarizing information from human feedback using a large language model. This is illustrated in a diagram that outlines the steps taken to collect and analyze human feedback. The feedback is gathered from a Reddit post and used to inform the training of a model, which in turn is evaluated through summaries provided by human judges. The goal is to improve the model's ability to generate better summaries, as indicated by the feedback from both experts and non-experts. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Learning to summarize from human feedback”\n",
      "Stiennon et al (OpenAI), 2022 \n",
      "\n",
      "[IDS] The image depicts a flowchart titled \"Learning to summarize from human feedback\" by Stiennon et al (OpenAI), 2022. It illustrates the process of collecting human feedback, training a reward model based on that feedback, and using it to improve the performance of a language model. The flowchart is divided into three main sections: Collect human feedback, Train reward model, and Use reward model. Each section contains detailed steps such as posting summaries to Reddit for evaluation, calculating rewards based on human judges' feedback, and updating the reward model. This process is part of a larger effort in NLP to develop more effective language models that can generate summaries or responses tailored to human preferences, as indicated by the lecture topic \"Transformer_decoder_and_Large_Langauge_Models.\" [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Learning to summarize from human feedback”\n",
      "Stiennon et al (OpenAI), 2022 \n",
      "\n",
      "[IDS] The image is a visual representation of the process involved in summarizing human feedback using a reward model. This is relevant to NLP (Natural Language Processing) as it demonstrates how models can be trained and fine-tuned based on human input, which is a key aspect of developing large language models. In the context of this lecture, it would highlight the importance of incorporating human feedback into machine learning algorithms to improve their performance in tasks such as text summarization. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Learning to summarize from human feedback”\n",
      "Stiennon et al (OpenAI), 2022 \n",
      "\n",
      "[IDS] In the context of an NLP lecture on Transformer Decoder and Large Language Models, the image represents a discussion about summarizing human feedback from an AI model. The example provided in the image illustrates how to extract relevant information from a text conversation between two individuals to understand their preferences and plans. This process involves identifying key phrases and summarizing the content to capture the essence of the dialogue. In an NLP setting, such techniques are crucial for interpreting user feedback and improving the performance of language models by incorporating real-world interactions and understanding user needs. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "In the context of large language models, what is the purpose of fine-tuning with human feedback and how does it improve a model's performance?\n",
      "What are the potential challenges associated with creating high-quality datasets for training transformer decoders, as depicted in the process by OpenAI in 2021?\n",
      "How can multi-task learning be harnessed when training large language models to generate more coherent and human-like text?\n",
      "What was the foundational paper in the field of transformers that sparked research into their applications within natural language processing?\n",
      "Which of the following statements is true regarding the process discussed in \"Learning to summarize from human feedback\" by Stiennon et al (OpenAI), 2022?\n",
      "\n",
      "A) The researchers only used non-expert judges for evaluation.\n",
      "\n",
      "B) The summaries were not posted on a public platform for evaluation but assessed solely in-house.\n",
      "\n",
      "C) Both expert and non-expert judges evaluated the summaries, giving them rewards based on quality as perceived by their respective levels of expertise. \n",
      "\n",
      "D) The model was only fine-tuned once after collecting the initial batch of human feedback.\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Decoder\n",
      "Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "\n",
      "[IDS] The image represents a Transformer Decoder, which is a component of transformer-based language models. In the context of NLP (Natural Language Processing), this decoder is used to generate outputs or predictions based on the inputs it receives. The architecture consists of multiple layers that process tokens, with each layer containing self-attention mechanisms and feed-forward networks. These layers are stacked together to form the decoder, which is responsible for producing the final output sequence. The diagram emphasizes the complexity and depth of the model, highlighting the importance of understanding how these components work together to generate coherent and contextually relevant responses. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Decoder\n",
      "Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "1. Adapt the prompt\n",
      "\n",
      "[IDS] The image illustrates the concept of a Transformer Decoder, a crucial component in natural language processing (NLP). As we're in a lecture on FineTuning_of_LLMs, which refers to fine-tuning large language models, this diagram serves as an educational tool to explain how such models process and generate text. The Transformer Decoder is a key part of these models, responsible for generating the output sequence based on the input embeddings and tokens. The process involves adding and normalizing attention masks, feeding forward information, and repeating these steps multiple times to produce the final output. The lecture would likely cover the importance of fine-tuning these models to improve their performance on specific tasks or domains. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Decoder\n",
      "Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "1. Adapt the prompt\n",
      "2. Adapt the input embeddings \n",
      "(prefix tuning)\n",
      "\n",
      "[IDS] The image illustrates the concept of fine-tuning pre-trained language models, which is a crucial topic in Natural Language Processing (NLP). Fine-tuning involves taking a large pre-trained model, such as BERT or GPT, and adapting it for a specific task by training it on a smaller dataset relevant to that task. This process is depicted through the Transformer Decoder architecture, where the input embeddings are adapted using prefix tuning, and attention is prompted with a special token. The image serves as a visual aid to help NLP students understand how these models can be effectively fine-tuned for better performance on specific tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Decoder\n",
      "Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "1. Adapt the prompt\n",
      "2. Adapt the input embeddings \n",
      "(prefix tuning)\n",
      "3. Adapt the model weights\n",
      "\n",
      "[IDS] The image is a visual representation of the Transformer Decoder architecture, which is a key component in the field of Natural Language Processing (NLP). In the context of our lecture on FineTuning_of_LLMs, this diagram illustrates how the Transformer Decoder works by processing input embeddings through positional encoding and adapting both the model weights and the input embeddings using prefix tuning. This process is essential for fine-tuning Large Language Models (LLMs) to better understand and generate human-like text. The figure is adapted from the research paper \"Attention is All You Need\" by Vaswani et al., published in 2017, and it provides a simplified overview of the model's structure and its fine-tuning process. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Decoder\n",
      "Fig. 1: Transformer Decoder (adapted from: Attention is all you need. Vaswani et al. 2017)\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "1. Adapt the prompt\n",
      "2. Adapt the input embeddings \n",
      "(prefix tuning)\n",
      "3. Adapt the model weights\n",
      "4. Adapt the output layer\n",
      "\n",
      "[IDS] The image is a visual representation of the Transformer Decoder, a component commonly used in NLP tasks such as language translation. In the context of a lecture on FineTuning_of_LLMs (Large Language Models), this diagram serves as an educational tool to illustrate how the model processes input embeddings and adapts them for generating outputs. The lecture might cover techniques like prompt adaptation and adjusting model weights to improve the performance of LLMs in specific tasks. The reference to \"Hochschule Bonn-Rhein-Sieg Fachbereich Informatik Tim Metzler\" suggests that this is part of a course curriculum at the mentioned institution, emphasizing the importance of understanding the inner workings of these models for fine-tuning purposes. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Adapt the prompts\n",
      "Types of prompting:\n",
      "1) Zero Shot Prompting\n",
      "2) Zero Shot Train of Thought Prompting\n",
      "3) Few Shot Prompting\n",
      "4) Few Shot Train of Though Prompting\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "1. Adapt the prompt\n",
      "\n",
      "[IDS] The image depicts a diagram illustrating the process of fine-tuning large language models (LLMs) in the context of Natural Language Processing (NLP). It emphasizes the importance of adapting prompts to suit the specific task at hand, such as generating text or answering questions. The diagram includes a flowchart with input embeddings, positional encoding, and attention mechanisms, highlighting how these components work together during the fine-tuning process. The lecture is likely focused on teaching students how to effectively fine-tune LLMs for different NLP tasks by selecting appropriate types of prompting. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Zero Shot Prompting\n",
      "Definition:\n",
      "Model is provided with prompt that is not part of the \n",
      "training data.\n",
      "The model performs the task without being explicitly \n",
      "trained on it.\n",
      "The model is given no examples (zero shot).\n",
      "Problem:\n",
      "Does not work for more complex tasks.\n",
      "\n",
      "[IDS] The image you're seeing is likely from a lecture on fine-tuning large language models (LLMs). The slide focuses on the concept of \"Zero Shot Prompting,\" which is a technique in NLP where a model performs a task without explicit training on it. This is an important topic in the field, as it demonstrates the model's ability to generalize and apply knowledge from its pre-trained state to new tasks. The slide also includes a definition and examples of zero-shot prompts, along with some information about the institutions and people involved in the development of this technology. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Zero Shot Prompting\n",
      "Definition:\n",
      "Model is provided with prompt that is not part of the \n",
      "training data.\n",
      "The model performs the task without being explicitly \n",
      "trained on it.\n",
      "The model is given no examples (zero shot).\n",
      "Problem:\n",
      "Does not work for more complex tasks.\n",
      "Example Prompt:\n",
      "Prompt:\n",
      "Classify the following text into the labels [funny, not \n",
      "funny, slightly funny]. Only respond with the label:\n",
      "Text: Tom went to the pub.\n",
      "Response (ChatGPT 3.5):\n",
      "not funny\n",
      "\n",
      "[IDS] The image is a slide from a lecture on fine-tuning large language models (LLMs), specifically focusing on the concept of \"Zero Shot Prompting.\" The slide explains that this technique involves providing a model with a prompt that isn't part of its training data, and it then performs a task without being explicitly trained on it. An example of this is given in the form of a text response task where the model is asked to respond to a label without having been previously trained on it. This aligns with the topic of fine-tuning LLMs, as it showcases the model's ability to adapt and generate relevant responses without direct training. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Zero Shot Prompting\n",
      "Complex Prompt:\n",
      "Prompt:\n",
      "A juggler can juggle 16 balls, half of which are golf \n",
      "balls, half of them are green. How many green balls \n",
      "does she have?\n",
      "Response (Llama 2, 70B):\n",
      "The juggler has 16 balls in total, and half of them are \n",
      "green. Therefore, the number of blue balls is 16/2 = 8.\n",
      "Since half of the balls are golf balls, the number of golf \n",
      "balls is also 8.\n",
      "So, the juggler has 8 green balls and 8 golf balls.\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on \"FineTuning_of_LLMs\", which stands for fine-tuning of large language models. The slide's content is centered around the Zero-Shot Prompting technique, a method used in natural language processing to enable a language model to perform tasks without explicit training on the specific task. The example provided illustrates how a juggling prompt can be used to elicit responses from a language model about various aspects of juggling, such as the number of balls, the number of green balls, and the total number of balls. This is indicative of how Zero-Shot Prompting allows for flexible and creative generation of text based on given prompts, without the need for extensive training on each possible input. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Zero Shot Train of Thought Prompting\n",
      "Tell the LLM to explain the process step by step1.\n",
      "Prompt:\n",
      "A juggler can juggle 16 balls, half of which are \n",
      "golf balls, half of them are green. How many \n",
      "green balls does she have?\n",
      "Lets think step by step and answer how many \n",
      "golf balls there are. Then answer how many of \n",
      "the golf balls are green.\n",
      "1 Wei, J., Wang, X., Schuurmans, D., et al. (2023). \n",
      "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. \n",
      "Google Research, Brain team.\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents the concept of fine-tuning pre-trained language models like LLMs (Large Language Models). The slide titled \"Zero Shot Train of Thought Prompting\" suggests an approach where a juggling analogy is used to explain the process of fine-tuning. This process involves starting with a basic number of balls (green for half and uncolored for half), then adding green balls to reach a desired number (16 balls in this case). The prompt encourages thinking step by step and answering how many golf balls there are after the process. This metaphor likely relates to the incremental updates and adjustments made to LLMs during fine-tuning to improve their performance on specific tasks or domains. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Zero Shot Train of Thought Prompting\n",
      "Tell the LLM to explain the process step by step1.\n",
      "Prompt:\n",
      "A juggler can juggle 16 balls, half of which are \n",
      "golf balls, half of them are green. How many \n",
      "green balls does she have?\n",
      "Lets think step by step and answer how many \n",
      "golf balls there are. Then answer how many of \n",
      "the golf balls are green.\n",
      "Response (Llama 2, 70B):\n",
      "Sure, let's break this down step by step!\n",
      "1) First, we know that the juggler can juggle 16 balls.\n",
      "2) We also know that half of the balls are golf balls. \n",
      "Therefore, the number of golf balls is 16/2 = 8.\n",
      "3) Now, we know that half of the golf balls are green. \n",
      "Therefore, the number of green golf balls is 8/2 = 4.\n",
      "4) Finally, we add the number of green golf balls to the \n",
      "number of green balls that are not golf balls to find \n",
      "the total number of green balls. Since we don't know \n",
      "how many green balls that are not golf balls, we'll \n",
      "just add 4 to the number of green balls that are not \n",
      "golf balls.\n",
      "So, the answer to the question \"How many green balls \n",
      "does she have?\" is 4 + x, where x is the number of \n",
      "green balls that are not golf balls.\n",
      "1 Wei, J., Wang, X., Schuurmans, D., et al. (2023). \n",
      "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. \n",
      "Google Research, Brain team.\n",
      "\n",
      "[IDS] In the context of a NLP lecture focused on FineTuning_of_LLMs, the image likely represents a slide or visual aid that explains the process of fine-tuning large language models. The slide may include diagrams, flowcharts, or other graphical representations to illustrate the steps involved in fine-tuning these models, such as data preparation, model selection, training, and evaluation. The overall goal would be to provide a clear and concise understanding of how to improve the performance of pre-trained language models for specific tasks or domains. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Few Shot Prompting\n",
      "Definition:\n",
      "Model is provided with prompt that is not part of the \n",
      "training data.\n",
      "The model performs the task without being explicitly \n",
      "trained on it.\n",
      "The model is given a few examples (few shot).\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the image represents a slide titled \"Few Shot Prompting\" which is likely a part of the topic \"FineTuning_of_LLMs\". The slide explains that a model is provided with a prompt that is not part of the training data and the model performs the task without being explicitly trained on it. It also mentions that the model is given a few examples (few shot) to perform the task. The logos at the bottom suggest that this information might be from Hochschule Bonn-Rhein-Sieg and could be related to their Fachbereich Informatik (Department of Computer Science), presented by Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Few Shot Prompting\n",
      "Definition:\n",
      "Model is provided with prompt that is not part of the \n",
      "training data.\n",
      "The model performs the task without being explicitly \n",
      "trained on it.\n",
      "The model is given a few examples (few shot).\n",
      "Advantage:\n",
      "We can even give completely new tasks and learn \n",
      "by example.\n",
      "Problem:\n",
      "Does not work for more complex tasks.\n",
      "Example Prompt:\n",
      "Prompt:\n",
      "Complete this with a single answer (red, green):\n",
      "These are the examples:\n",
      "This is great → red\n",
      "This is good → red\n",
      "This is awful → green\n",
      "Complete this:\n",
      "This is horrible →\n",
      "This is nice → \n",
      "Response (ChatGPT 3.5):\n",
      "This is horrible → green\n",
      "This is nice → red\n",
      "\n",
      "[IDS] In the context of a NLP lecture focused on FineTuning_of_LLMs, this image represents a slide discussing the concept of \"Few Shot Prompting.\" It emphasizes that a model provided with a prompt that is not part of the training data performs the task without being explicitly trained on it. The slide also explains the advantages and disadvantages of this approach, such as the ability to learn new tasks quickly but also the risk of incompatibility with more complex tasks. Examples are given for both successful and unsuccessful prompts, showcasing how few-shot prompting can be applied in practice. The slide is likely part of a larger discussion on the versatility and limitations of fine-tuning large language models (LLMs) using minimal training data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Few Shot Prompting\n",
      "Complex example.\n",
      "Prompt:\n",
      "Q: A juggler can juggle 16 balls, half of which are \n",
      "golf balls, half of them are green. How many \n",
      "green balls does she have?\n",
      "A: 4+x, where x is the number of green balls that \n",
      "are not golf balls.\n",
      "Q: Susan buys bottles and cans of soda. She \n",
      "has 12 bottles and cans. Half of them are cans. \n",
      "Half of them contain lemonade. How many \n",
      "lemonades does she have?\n",
      "A:\n",
      "Response (ChatGPT 3.5):\n",
      "Let's break down the information given:\n",
      "1. Susan has 12 bottles and cans of soda.\n",
      "2. Half of them are cans.\n",
      "So, the number of cans is 12 / 2 = 6.\n",
      "3. Half of the bottles and cans contain lemonade.\n",
      "Now, half of the total number of bottles and cans is 12 / \n",
      "2 = 6. Since this is also the number of cans, we can \n",
      "conclude that all the cans contain lemonade.\n",
      "Therefore, Susan has 6 lemonades.\n",
      "\n",
      "[IDS] The image is a visual aid from a lecture on Fine-Tuning of Large Language Models (LLMs), likely discussing the process of adapting these models for specific tasks. The slide titled \"Few Shot Prompting\" suggests a focus on using few-shot learning, where models are trained with a small number of examples to perform well on a new task. The complex example provided involves counting green balls and cans, which is a simple task that could be used to illustrate how LLMs can be fine-tuned for various tasks through prompt engineering. The presence of the Hochschule Bonn-Rhein-Sieg logo indicates the institution hosting or affiliated with the lecture, and the name Tim Metzler might refer to the lecturer or presenter of the material. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Few Shot Train of Thought Prompting\n",
      "Complex example with train of thought.\n",
      "Prompt:\n",
      "Q: A juggler can juggle 16 balls, half of which are \n",
      "golf balls, half of them are green. How many \n",
      "green balls does she have?\n",
      "A: Let’s think step by step: There are 8 golf balls. \n",
      "4 golf balls are green. She has 4 green balls plus \n",
      "how many other balls are green\n",
      "Q: Susan buys bottles and cans of soda. She \n",
      "has 12 bottles and cans. Half of them are cans. \n",
      "Half of them contain lemonade. How many \n",
      "lemonades does she have?\n",
      "A:\n",
      "Response (ChatGPT 3.5):\n",
      "Let's break it down step by step:\n",
      "1. Half of the bottles and cans are cans.\n",
      "   So, 12 / 2 = 6 of them are cans.\n",
      "2. Half of the cans contain lemonade.\n",
      "   So, 6 / 2 = 3 cans contain lemonade.\n",
      "Therefore, Susan has 3 lemonades.\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on \"FineTuning_of_LLMs\", which likely stands for Fine-Tuning of Large Language Models. The slide seems to be focused on the concept of thought prompting, specifically using the ChatGPT 3.5 model as an example. It includes a complex example with a train of thought, showing how to use the model to generate a response to a given prompt. This is a common practice in NLP, where models are fine-tuned to perform specific tasks or generate coherent responses to prompts. The presence of the names Tim Metzler and Hochschule Bonn-Rhein-Sieg suggests that this might be part of a course or seminar at that institution, possibly taught by Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "2. Adapt the input embeddings \n",
      "(prefix tuning)\n",
      "\n",
      "[IDS] The image is a visual representation of the process of fine-tuning language models, specifically focusing on the use of prefix tuning. This technique involves optimizing continuous prompts for generating outputs in NLP tasks. The diagram shows how input embeddings are processed through positional encoding and then passed through layers such as linear, add & norm, feed forward, and masked multi-head attention. The purpose of this lecture, titled \"FineTuning_of_LLMs,\" is likely to educate students about how to effectively adapt and fine-tune pre-trained language models like BONN-RHEIN-SIEG for specific tasks by adjusting the input embeddings. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "Recap:\n",
      "In a transformer model we:\n",
      "●\n",
      "tokenize the text into token ids,\n",
      "●\n",
      "look up the initial embeddings for these tokens\n",
      "●\n",
      "add position information\n",
      "●\n",
      "feed this trough attention layers\n",
      "Input text: \n",
      "”Summarize: NLP is cool. …”\n",
      "Token ids:\n",
      "[15, 8, 14, 3, 200, …]\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, ...]\n",
      "\n",
      "[IDS] The image you're seeing is a visual representation of the concept of \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\" in the context of NLP, or Natural Language Processing. In a NLP lecture focused on FineTuning_of_LLMs (Large Language Models), this diagram would likely be used to explain how adding and normalizing features, such as tokens, attention, and position embeddings, can improve the performance of language models. The diagram illustrates the process of input embedding and encoding, which are crucial steps in fine-tuning these models. It's a tool to help students and professionals understand the complex interactions within NLP models and how they can be optimized for better generation of text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "Recap:\n",
      "In a transformer model we:\n",
      "●\n",
      "tokenize the text into token ids,\n",
      "●\n",
      "look up the initial embeddings for these tokens\n",
      "●\n",
      "add position information\n",
      "●\n",
      "feed this trough attention layers\n",
      "Input text: \n",
      "”Summarize: NLP is cool. …”\n",
      "Token ids:\n",
      "[15, 8, 14, 3, 200, …]\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, ...]\n",
      "Prompt engineering optimizes this text\n",
      "\n",
      "[IDS] In the context of a NLP lecture, specifically focusing on FineTuning_of_LLMs, we are likely discussing the process of adapting pre-trained language models to perform specific tasks. The image represents a transformer model architecture, which is commonly used in NLP tasks such as language generation or translation. The model consists of an input layer that processes embeddings from tokens, followed by multiple layers of transformer blocks for processing these inputs. The output layer then generates probabilities for different outputs.\n",
      "\n",
      "The lecture might cover topics like how to fine-tune pre-trained models using labeled data for a specific task, adjusting the model's parameters to improve its performance on the target task, and evaluating the effectiveness of the fine-tuned model. The goal is to leverage the knowledge gained from pre-training on a large dataset to achieve better results on a specific application domain. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "Recap:\n",
      "In a transformer model we:\n",
      "●\n",
      "tokenize the text into token ids,\n",
      "●\n",
      "look up the initial embeddings for these tokens\n",
      "●\n",
      "add position information\n",
      "●\n",
      "feed this trough attention layers\n",
      "Input text: \n",
      "”Summarize: NLP is cool. …”\n",
      "Token ids:\n",
      "[15, 8, 14, 3, 200, …]\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, ...]\n",
      "Prompt engineering optimizes this text\n",
      "Can we optimize this instead?\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the concept of fine-tuning large language models (LLMs). The image presents a diagram illustrating the process of fine-tuning an LLM. It shows how input embeddings are processed through positional encoding and then fed into a transformer model, which consists of layers for adding and normalizing attention masks, feeding forward, and summing up the attention values. The output probabilities from the transformer model are then used to generate a response. The text in the image emphasizes the importance of optimizing continuous prompts for generating text. Additionally, there's a prompt asking if we can optimize the process further by engineering the text. This lecture seems to be focused on teaching the techniques and considerations involved in fine-tuning LLMs for natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, …]\n",
      "Add a prefix to the embeddings:\n",
      "[p0, y15, y8, y14, y3, y200, …]\n",
      "\n",
      "[IDS] The image depicts a slide from a lecture focused on \"FineTuning_of_LLMs,\" which likely stands for fine-tuning of large language models. These are advanced tools in Natural Language Processing (NLP) that have been pre-trained on vast amounts of data and can be further refined to perform specific tasks such as sentiment analysis, translation, or question answering. The presence of logos suggests collaboration between the Hochschule Bonn-Rhein-Sieg and Tim Metzler, possibly indicating that this lecture is part of an academic course or research project at the mentioned institution. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, …]\n",
      "Add a prefix to the embeddings:\n",
      "[p0, y15, y8, y14, y3, y200, …]\n",
      "The prefix p0 is now a vector of embedding size (e.g. \n",
      "768).\n",
      "Fine-tune this vector on a supervised dataset.\n",
      "In general: Have one or more task dependent prefix \n",
      "vectors\n",
      "\n",
      "[IDS] The image is a snapshot of a presentation slide from an NLP (Natural Language Processing) lecture, specifically focusing on the concept of fine-tuning large language models (LLMs). The title \"FineTuning_of_LLMs\" suggests that the content is about optimizing these models for specific tasks or domains. The slide includes a reference to a paper titled \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\" by Xiang Lisa Li and Percy Liang from Stanford University in 2021. This indicates that the lecture might be discussing recent research in the field, particularly how to improve the performance of LLMs through prefix tuning, which is a technique for adjusting the model's embeddings to enhance its task-specific capabilities. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“Prefix-Tuning: Optimizing Continuous Prompts for Generation”\n",
      "Xiang Lisa Li and Percy Liang, Stanford 2021\n",
      "Input Embeddings:\n",
      "[y15, y8, y14, y3, y200, …]\n",
      "Add a prefix to the embeddings:\n",
      "[p0, y15, y8, y14, y3, y200, …]\n",
      "The prefix p0 is now a vector of embedding size (e.g. \n",
      "768).\n",
      "Fine-tune this vector on a supervised dataset.\n",
      "In general: Have one or more task dependent prefix \n",
      "vectors.\n",
      "Advantage:\n",
      "Instead of manually optimizing text prompts we can \n",
      "automatically optimize the parameters of the prefix vector.\n",
      "Instead of fine-tuning the whole model we only fine-tune the \n",
      "input.\n",
      "Works with relatively small amount of examples (50-500 \n",
      "examples).\n",
      "Disadvantage:\n",
      "We only do instruction tuning. We can not learn completely \n",
      "new tasks.\n",
      "\n",
      "[IDS] The image represents a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Fine-Tuning of Large Language Models\" or LLMs. The slide discusses the concept of \"Prefix-Tuning,\" which is a method to optimize continuous prompts for generating text. It explains how to add a prefix to embeddings and mentions that the prefix is now a vector of embedding size, for example, 768. The slide also introduces the idea of fine-tuning the whole model instead of just fine-tuning the input. It compares this method with instruction tuning and emphasizes the advantages such as manually optimizing text prompts and working with relatively small amounts of examples, like 50-500 examples. The disadvantages mentioned include the need for more data and computational resources. The slide also includes logos indicating the affiliation of the lecture with Hochschule Bonn-Rhein-Sieg and Tim Metzler's involvement in the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fine-Tuning the Output Layer\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "4. Adapt the output layer\n",
      "\n",
      "[IDS] The image is a visual representation of the process for fine-tuning the output layer in language models. In the context of NLP (Natural Language Processing), this diagram illustrates how an existing language model can be adapted to perform a specific task by modifying its output layer. The steps shown are part of the training process where additional layers are added to enhance the model's ability to generate more accurate outputs. This is a common technique used in machine learning to improve the performance of models on specific tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Feature-Based Approaches and Fine-Tuning Approaches\n",
      "Prompt\n",
      "Tokens\n",
      "Input\n",
      "Embeddings\n",
      "4. Adapt the last layers or train \n",
      "classifier on top of output\n",
      "Fig. 2: Fine-Tuning Approaches (adapted from \n",
      "https://magazine.sebastianraschka.com/p/finetuning-large-language-models, accessed \n",
      "18.01.2024)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image illustrates the process of fine-tuning large language models. These models are pre-trained transformer networks that have been trained on vast amounts of data to understand the nuances of human language. The fine-tuning process involves adapting these models for specific tasks or domains. In the diagram, we see the architecture of a model with multiple layers, where the input embeddings are processed through several transformer blocks, resulting in an output. The arrows indicate the flow of information through the network. This is a crucial concept in NLP as it allows for the development of domain-specific language models that can perform better on tasks like sentiment analysis or text classification within their respective fields. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Feature-Based Approaches and Fine-Tuning Approaches\n",
      "Feature-Based Approach:\n",
      "Take output embeddings of model\n",
      "Train classifier on top (e.g. sentiment classification)\n",
      "Advantages:\n",
      "Only inference is done.\n",
      "Few parameters in classifier to train. (fast and efficient)\n",
      "Embeddings can be stored beforehand.\n",
      "Disadvantages:\n",
      "Low accuracy\n",
      "Fig. 2: Fine-Tuning Approaches (adapted from \n",
      "https://magazine.sebastianraschka.com/p/finetuning-large-language-models, \n",
      "accessed 18.01.2024)\n",
      "\n",
      "[IDS] In the image, we see a slide from a lecture on \"Fine-Tuning of LLMs,\" which stands for Large Language Models. The slide is titled \"Feature-Based Approaches and Fine-Tuning Approaches\" and focuses specifically on approach number 1: Feature-Based Approach. This approach involves taking output embeddings of a model, such as a transformer, and training a classifier on top of it to perform tasks like sentiment classification. The advantages listed include the fact that inference is done only once, parameters are kept frozen, and embeddings can be stored before handling. However, the disadvantages mentioned are low accuracy and the need for a classifier to train, which can be fast and efficient but also time-consuming. The slide is part of a series, as indicated by the references to other slides and individuals involved in the presentation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Feature-Based Approaches and Fine-Tuning Approaches\n",
      "Feature-Based Approach:\n",
      "Take output embeddings of model\n",
      "Train classifier on top (e.g. sentiment classification)\n",
      "Advantages:\n",
      "Only inference is done.\n",
      "Few parameters in classifier to train. (fast and efficient)\n",
      "Embeddings can be stored beforehand.\n",
      "Disadvantages:\n",
      "Low accuracy\n",
      "Fine-Tuning I:\n",
      "Train the output layers of the model on your data.\n",
      "Advantages:\n",
      "Slightly higher accuracy.\n",
      "Disadvantages:\n",
      "Slightly longer training time.\n",
      "For inference we need to run the full model.\n",
      "Fig. 2: Fine-Tuning Approaches (adapted from \n",
      "https://magazine.sebastianraschka.com/p/finetuning-large-language-models, \n",
      "accessed 18.01.2024)\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP), specifically focused on the topic of \"Fine-Tuning of Large-Language Models\" or LLMs. It compares two approaches to fine-tuning LLMs: a feature-based approach and a fine-tuning approach. The feature-based approach uses a pre-trained transformer model that's kept frozen, only updating the classifier. This method has advantages such as being fast and efficient with embeddings stored before-hand, but also has disadvantages like low accuracy and the need for a large training set. On the other hand, the fine-tuning approach trains the output layers of the model on your data, which offers slightly higher accuracy but requires longer training time and a full model for inference. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fine-Tuning the Full Model\n",
      "Fig. 2: Fine-Tuning Approaches (adapted from \n",
      "https://magazine.sebastianraschka.com/p/finetuning-large-language-models, \n",
      "accessed 18.01.2024)\n",
      "\n",
      "[IDS] The image is a visual representation of the process involved in fine-tuning large language models (LLMs) as discussed in an NLP lecture. It illustrates the steps taken to adapt pre-trained transformer models to a specific domain or task, which is a common practice in natural language processing to improve the performance of language models on particular tasks. The diagram provides a structured overview of the feature-based approach and the fine-tuning process, including the use of labeled training sets and updates to the model's parameters. This process is crucial for NLP applications where domain-specific knowledge can significantly enhance the accuracy and relevance of language models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fine-Tuning the Full Model\n",
      "Fine-Tuning the Full Model:\n",
      "Train the full model on new data.\n",
      "Advantages:\n",
      "Potentially higher accuracy\n",
      "Disadvantages:\n",
      "Overfitting\n",
      "Needs a lot of examples\n",
      "Catastrophic forgetting\n",
      "Fig. 2: Fine-Tuning Approaches (adapted from \n",
      "https://magazine.sebastianraschka.com/p/finetuning-large-language-models, \n",
      "accessed 18.01.2024)\n",
      "\n",
      "[IDS] The image is a visual representation of the process and benefits of fine-tuning large language models (LLMs) in the context of Natural Language Processing (NLP). It's part of a lecture that explains how to adapt pre-trained transformer models, such as those from Hugging Face or GPT, for specific NLP tasks by fine-tuning them on new data. The figure illustrates this process with two main approaches: Feature-based approach and Fine-tuning I & II methods. The advantages of fine-tuning include potentially higher accuracy, while the disadvantages involve overfitting and the need for a lot of examples. The lecture also emphasizes the importance of catastrophic forgetting and the use of techniques like update or keep frozen to mitigate it. This information is crucial for NLP practitioners who want to leverage the power of pre-trained models for their specific applications without losing the generalization capabilities they have developed during training on diverse datasets. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Problem: \n",
      "We only want to adapt the parameters of the model a little. \n",
      "Typically the model already has knowledge which is \n",
      "beneficial for the task.\n",
      "How to determine which parameters to update?\n",
      "\n",
      "[IDS] In the context of an NLP lecture, the image represents a discussion on the process of FineTuning of Large Language Models (LLMs). The slide titled \"LoRa: Low-Rank Adaptation of Large Language Models\" by Hu et al., published in Microsoft 2021, focuses on parameter-efficient fine-tuning. This technique is crucial for NLP as it allows models to adapt to specific tasks while keeping the number of parameters manageable. The slide suggests that the problem lies in adapting the model's parameters with limited knowledge of which parameters are beneficial for the task. It also introduces a method to determine which parameters to update, emphasizing the importance of efficient adaptation in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Problem: \n",
      "We only want to adapt the parameters of the model a little. \n",
      "Typically the model already has knowledge which is \n",
      "beneficial for the task.\n",
      "How to determine which parameters to update?\n",
      "Solution:\n",
      "Decompose the weight matrices W into an original part W \n",
      "and an update part ΔW.\n",
      "\n",
      "[IDS] The image is a slide from a lecture on \"FineTuning_of_LLMs,\" which stands for Fine-Tuning of Large Language Models. It discusses the challenge of adapting these models, which typically have a lot of knowledge beneficial for tasks, to new parameters without compromising their existing capabilities. The slide outlines the problem and solution approaches, such as decomposing weight matrices into original parts and updating part W. This is likely part of a larger discussion on how to effectively fine-tune large language models for specific tasks while maintaining their overall performance and understanding. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Problem: \n",
      "We only want to adapt the parameters of the model a little. \n",
      "Typically the model already has knowledge which is \n",
      "beneficial for the task.\n",
      "How to determine which parameters to update?\n",
      "Solution:\n",
      "Decompose the weight matrices W into an original part W \n",
      "and an update part ΔW.\n",
      "Fig. 3: Frozen original weights and trainable update (from \n",
      "https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-\n",
      "finetuning-large-models-936bce1a07c6, accessed 18.01.2024)\n",
      "\n",
      "[IDS] In the image, we see a slide from a lecture on \"FineTuning of LLMs,\" which stands for Large Language Models. The slide is titled \"LoRa: Low-Rank Adaptation of Large Language Models\" and is authored by Hu et al., published in Microsoft 2021. It presents a parameter-efficient fine-tuning method for adapting the parameters of a model, especially those that already have knowledge beneficial for the task at hand. The slide outlines a solution to determine which parameters to update using a pre-trained model W and an original part W. It also mentions an update part ΔW and includes a diagram showing the flow of operations in the fine-tuning process. Additionally, there are references to frozen original weights and a trainable update from a specific URL. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Low-Rank Assumption:\n",
      "The update weight matrix ΔW does not contain a lot of \n",
      "new information.\n",
      "This means the rank is lower than the original dimension.\n",
      "→ We can represent the weight update using two smaller \n",
      "matrices:\n",
      "ΔW = AB\n",
      "A and B have a lower dimensionality than ΔW\n",
      "Fig. 3: Frozen original weights and trainable update (from \n",
      "https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-\n",
      "finetuning-large-models-936bce1a07c6, accessed 18.01.2024)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept known as \"Low-Rank Adaptation of Large Language Models\" or LoRa for short. It's a technique that allows for efficient fine-tuning of large pre-trained language models like BERT or GPT. The slide explains how by assuming low-rank and utilizing the fact that the update weight matrix ΔW is not very large, we can significantly reduce the computational resources needed during the fine-tuning process. This is done by representing ΔW using two smaller matrices: A and B, which are lower in dimensionality than ΔW itself. This method, as shown in the flowchart, can be applied to pre-trained weights to achieve faster and more efficient fine-tuning, which is essential in NLP tasks where fine-tuning large models on specific tasks is often necessary but computationally expensive. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Low-Rank Assumption:\n",
      "The update weight matrix ΔW does not contain a lot of \n",
      "new information.\n",
      "This means the rank is lower than the original dimension.\n",
      "→ We can represent the weight update using two smaller \n",
      "matrices:\n",
      "ΔW = AB\n",
      "A and B have a lower dimensionality than ΔW\n",
      "Fig. 4: Frozen original weights and trainable update A and B (from \n",
      "https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-\n",
      "finetuning-large-models-936bce1a07c6, accessed 18.01.2024)\n",
      "\n",
      "[IDS] The image you see is a visual representation of the concept \"FineTuning_of_LLMs,\" which stands for fine-tuning of large language models. It illustrates the process of adapting these models to specific tasks or domains. The diagram shows how a low-rank assumption can be used to represent the weight matrix with two smaller matrices, reducing the dimensionality and potentially improving the efficiency of the model. This is a common technique in machine learning, particularly in natural language processing (NLP), where large language models are fine-tuned for various applications such as sentiment analysis, text classification, and language translation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "“LoRa: Low-Rank Adaptation of Large Language Models”\n",
      "Hu et al, Microsoft 2021\n",
      "Parameter-efficient Fine-Tuning\n",
      "Advantages\n",
      "●\n",
      "Instead of training large weight matrices ΔW we only \n",
      "train smaller matrices A and B\n",
      "●\n",
      "We can publish the new weights ΔW\n",
      "Anyone who wants to use our model downloads the \n",
      "original weights and our update and applies it.\n",
      "(Llama → Alpaca → Vicuna etc)\n",
      "●\n",
      "We avoid licensing issues (we only publish the new \n",
      "part)\n",
      "●\n",
      "We can have a large base model and switch different \n",
      "update matrices to get a model family (e.g. one for \n",
      "summarization, one for sentiment, etc)\n",
      "Disadvantages:\n",
      "●\n",
      "How to choose dimensionality r?\n",
      "●\n",
      "Which weight matrices to fine-tune?\n",
      "●\n",
      "Low rank may cause low performance\n",
      "Fig. 4: Frozen original weights and trainable update A and B (from \n",
      "https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-\n",
      "finetuning-large-models-936bce1a07c6, accessed 18.01.2024)\n",
      "\n",
      "[IDS] The image is a visual aid from a lecture on the topic of \"FineTuning_of_LLMs\", which stands for Fine-Tuning of Large Language Models. It illustrates the concept of using pre-trained weights and applying them to a specific task, like updating a model family for different tasks such as sentiment analysis or fine-tuning. The diagram shows how the original large language model (LLM) can be modified by adding new layers on top and adjusting the existing layers with new weights to suit a particular task. This process is beneficial as it allows for more efficient training by leveraging the knowledge already present in the pre-trained model, rather than starting from scratch. The slide also mentions challenges associated with this approach, such as choosing the right weight matrices and dealing with low rank issues that could cause poor performance. The slide is likely part of a larger presentation aimed at educating attendees about the techniques and considerations involved in fine-tuning large language models. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the potential issue with a low-rank assumption in LoRa, and how might it impact model performance?\n",
      "In few-shot learning, what's the difference between zero-shot and few-shot prompting, and why might one be more effective than the other for certain tasks? \n",
      "How does prefix tuning differ from parameter-efficient fine-tuning methods like LoRa, in terms of their approach to updating parameters while maintaining model efficiency?\n",
      "In an NLP context, what are some potential advantages and disadvantages of fine-tuning a large pre-trained LLM on a specific task rather than starting with a model trained directly for the target task from scratch?\n",
      "What is the low-rank assumption in LoRa (Low-Rank Adaptation of Large Language Models) and how does it help with fine-tuning these models?\n",
      "\n",
      "A) It assumes that only a few new parameters are needed to adapt the model, reducing the computational resources required during training.\n",
      "\n",
      "B: It assumes the update weight matrix contains just as much information as the original weights, requiring no reduction in dimensionality or computation.\n",
      "\n",
      "C: It assumes the update weight matrix is of higher rank than the original weights, actually increasing the complexity and parameters of the fine-tuned model. \n",
      "\n",
      "D: It makes no assumptions about the weight matrices' ranks, making the approach computationally intensive like a full parameter update.\n",
      "lecture_content: Hi, I'm Daniel\n",
      "I studied here at H-BRS (Embedded - and Autonomous Systems.)\n",
      "Did a PhD at the Univerity of Bonn in the field of ML.\n",
      "Worked at Fraunhofer IAIS.\n",
      "For the last few years I worked at Telekom Techn1k in a department helps the\n",
      "organization with digitalization and automatization.\n",
      "Digitalization has a lot of room for ML/AI.\n",
      "1\n",
      "\n",
      "\n",
      "Before we dive into RAG systems,\n",
      "bare with me to understand why it is needed.\n",
      "2\n",
      "\n",
      "\n",
      "ChatGPT hit the industry like a truck!\n",
      "OpenAI has demonstrated with the introduction of ChatGPT\n",
      "how well language models work and\n",
      "they created an extremely usable interface.\n",
      "This is why everybody in the industry loves language models \n",
      "3\n",
      "\n",
      "\n",
      "Think about this\n",
      "A large part of knowledge, processes, documentation, communication, FAQs,\n",
      "regulations, etc. are in written form.\n",
      "In addition to programming assistance, there exist thousands of use cases:\n",
      "Ask your documentation directly\n",
      "Consult customers without having to call an employee.\n",
      "Use a chatbot for website navigation; very modern!\n",
      "Simplify onboarding\n",
      "4\n",
      "\n",
      "\n",
      "Language models are sequence models\n",
      "5\n",
      "\n",
      "\n",
      "Modelling sequences, classic edition: Element by element\n",
      "Language models are seq2seq  models.\n",
      "6\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Retrival_augemented_generation\". It illustrates the concept of 'Modelling sequences, classic edition: Element by element', which is likely a method or technique used in NLP for processing and analyzing sequential data. The slide also mentions that language models are models, suggesting a discussion on the nature of language models within this context. The presence of the WXYZ sequence, along with an encoder-decoder architecture diagram, indicates a possible exploration of sequence-to-sequence models in the lecture. [IDE]\n",
      "\n",
      "\n",
      "Modelling sequences, element by element\n",
      "Language models are seq2seq  models.\n",
      "So far this was done element by element, using\n",
      "Regression on time windowed input, Markov chains,\n",
      "recurrence e.g. RNNs, or\n",
      "memory e.g. GRUs. (LSTM 1997)\n",
      "But this approach cannot easily be parallelized!\n",
      "6\n",
      "\n",
      "[IDS] The image is a visual representation of sequence-to-sequence models in NLP, which are used for tasks such as machine translation, text summarization, and chatbots. These models consist of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence. The lecture highlights that these models can be difficult to parallelize due to their sequential nature, making them less efficient on parallel hardware like GPUs. [IDE]\n",
      "\n",
      "\n",
      "Transformers solved this problem...\n",
      "... by consuming the entire sequence at once as input.\n",
      "Think about it, there are many sequences that are finished.\n",
      "We humans just consume it as sequence.\n",
      "Books, emails, videos, images, webpages, ...\n",
      "7\n",
      "\n",
      "[IDS] In this NLP lecture, the focus is on transformer models and their ability to process data in sequence. The slide emphasizes that transformers consume an entire sequence at once as input, which is a characteristic feature of these models. It also highlights that humans tend to think about sequences that are finished, such as books or emails, rather than ongoing processes like videos or webpages. The content suggests a discussion around the nature of sequence processing in natural language understanding and generation tasks. [IDE]\n",
      "\n",
      "\n",
      "Embedding the tokens\n",
      "The tokens are each embedded into a numerical domain.\n",
      "Have a look at the word2vec  paper:\n",
      "Efficient Estimation of Word Representations in Vector Space\n",
      "So the input to the transformer are not vectors, but vectors of vectors.\n",
      "8\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the concept of embedding tokens into a numerical domain for efficient estimation of word representations in vector space. This process involves converting words into vectors to facilitate better understanding and analysis by machines. The presentation highlights that all transformers are cool, suggesting the importance of transformer models in NLP tasks. It also mentions that the input to the transformer is not vectors but vectors of vectors, indicating the complexity involved in these representations. [IDE]\n",
      "\n",
      "\n",
      "Calculate the matrix of\n",
      "pairwise attentions\n",
      "In a sequence, the elements are\n",
      "related to each other. (not iid)\n",
      "For each possible pair of\n",
      "embedded tokens the\n",
      "corresponding attention-score is\n",
      "calculated.\n",
      "This attention matrix is the heart\n",
      "of the transformer block .\n",
      "9\n",
      "\n",
      "[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP), specifically discussing the concept of \"Retrival_augemented_generation\". It explains how to calculate the matrix of pairwise attentions in a sequence. This is a technique used in NLP models, like transformers, to determine the relevance or importance of each element in a sequence relative to other elements. The attention score is calculated for each possible pair of embedded tokens, and this attention matrix is considered the heart of the transformer block in these models. The slide includes visual aids like color-coded squares to help understand the concept of context length and position in relation to the attention mechanism. [IDE]\n",
      "\n",
      "\n",
      "The transformer block\n",
      "Consists of two main components:\n",
      "A filter for what should be\n",
      "computed.\n",
      "A classic neural network layer.\n",
      "This calculation of all attention-\n",
      "scores can be heavily\n",
      "parallelized, and that's why\n",
      "Transformers are so popular!\n",
      "10\n",
      "\n",
      "[IDS] The image is a visual representation of the Transformer block, a key component in transformer-based neural networks, particularly in the context of Natural Language Processing (NLP). The Transformer block is a fundamental part of the Transformer architecture, which has been widely used in various NLP tasks such as machine translation, question answering, and text generation. It consists of two main components: a multi-head attention mechanism and a feed-forward network. These components work together to process input sequences in parallel, allowing the model to capture long-range dependencies between words or tokens. The lecture you are attending is likely discussing how this architecture contributes to advancements in NLP, specifically focusing on retrieval-augmented generation, which combines the strengths of both retrieval-based and generative models for improved performance. [IDE]\n",
      "\n",
      "\n",
      "Faster training means larger models in the same time\n",
      "Much larger transformer networks can be trained in the same time as other\n",
      "seq2seq models.\n",
      "... and for language models, larger is (one parameter for) better.\n",
      "See: Scaling Laws for Neural Language Models\n",
      "11\n",
      "\n",
      "[IDS] The image depicts a slide from an NLP lecture, specifically discussing the concept of \"Faster training means larger models in the same time.\" It explains that bigger transformer networks can be trained simultaneously with seq2seq models and that for language models, larger is better. The slide includes graphs to illustrate this point, with one graph showing test loss over compute and another graph displaying dataset size vs. parameters. The lecture seems to emphasize the importance of model size in achieving better performance in neural language models. [IDE]\n",
      "\n",
      "\n",
      "If transformers are so good\n",
      "What's the problem?\n",
      "12\n",
      "\n",
      "\n",
      "Sequence length is the problem\n",
      "Computing all attention-scores is quadratic in storage and runtime!\n",
      "(You can battle that by parallelization, but this scales only linear.)\n",
      "... and then there is also\n",
      "Multi-Headed Attention\n",
      "Multiple Transformer blocks in a row.\n",
      "13\n",
      "\n",
      "\n",
      "Have a look at the \"Chat with your own data\" use case\n",
      "Present your data to the LLM and just chat with it.\n",
      "Highly useful - very flexible - easy to implement!\n",
      "However, prompts from this world can quickly become huge.\n",
      "System prompt: You are a helpful assistant that helps Telekom with fiber optic\n",
      "expansion and...\n",
      "User prompt: Under what circumstances am I allowed to drill into a listed\n",
      "building for a fiber optic connection?\n",
      "Context documents: Attached are all building regulations for Telekom\n",
      "employees...\n",
      "... that's a lot of tokens...and it's going to make our attention matrix explode.\n",
      "14\n",
      "\n",
      "\n",
      "Academia is looking for ways to extend the context length\n",
      "15\n",
      "\n",
      "\n",
      "Some ideas are\n",
      "State spaces instead of attention:\n",
      "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
      "Hierarchical attention:\n",
      "Hierarchical Attention Networks for Document Classification\n",
      "I/O aware attention to reduce the number of memory reads/writes:\n",
      "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
      "Parallelizable LSTMs:\n",
      "Extended Long Short-Term Memory\n",
      "16\n",
      "\n",
      "\n",
      "Some more ideas are\n",
      "Sparse attention\n",
      "Generating Long Sequences with Sparse Transformers\n",
      "Optimizing the attention calculation:\n",
      "You Need to Pay Better Attention:\n",
      "Rethinking the Mathematics of Attention Mechanism\n",
      "Compressing attention:\n",
      "Leave No Context Behind:\n",
      "Efficient Infinite Context Transformers with Infini-attention\n",
      "17\n",
      "\n",
      "\n",
      "Practitioners use Information Retrieval methods\n",
      "to reduce context size\n",
      "18\n",
      "\n",
      "\n",
      "RAG system\n",
      "A clever way to reduce the context size of the prompt is to\n",
      "not use all documents, but rather a selection.\n",
      "This is known as a Retrieval Augmented Generator.\n",
      "19\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the slide presents the RAG system as a method to enhance the context size for document retrieval and generation. It's a clever approach that avoids the limitations of using all documents by selecting a subset, which is then processed through a Retrieval-Augmented Generator. This process likely involves using a coordinator to manage the flow of information between the question, the document database, and the language model (LLM). The LLM would be responsible for generating the final output based on the information retrieved from the database and the input question. The slide number 19 suggests this is part of a larger presentation, and the visual elements help to explain the complex interaction between different components in the RAG system. [IDE]\n",
      "\n",
      "\n",
      "How to retrieve the documents?\n",
      "20\n",
      "\n",
      "\n",
      "Vector search (index documents)\n",
      "Embed text sequences into a vecor space. E.g. by using:\n",
      "Token embedder, like BERT, or a headless LLM\n",
      "Specially trained models, like SentenceBert\n",
      "21\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It specifically focuses on the topic of retrieval-augmented generation. The slide outlines the concept of embedding text sequences into a vector space, using token embeddings like BERT or a headless LLM. It also mentions specially trained models such as SentenceBert. These techniques are essential in NLP for tasks like language understanding and generation. [IDE]\n",
      "\n",
      "\n",
      "Vector search (retrieve similar documents)\n",
      "Retrieve semantically similar documents by comparing the vectors. (see nice blog post)\n",
      "Angular/Cosine- or \n",
      "-distance\n",
      "Karpathy's idea of kernel-distance\n",
      "22\n",
      "\n",
      "[IDS] The image you're seeing is a visual representation of the concept of vector search in the context of Natural Language Processing (NLP). It illustrates how vector search algorithms, such as the Sentence Embedding Algorithm shown here, can be used to retrieve semantically similar documents by comparing their vectors. The example given in the image uses the cosine or L1 distance as a measure to determine similarity between the vectors of two documents. The presence of terms like \"nice blog post\" suggests that the lecture might be discussing practical applications of this technique in retrieving relevant blog posts from a larger corpus of text. This is a fundamental concept in information retrieval and NLP, allowing systems to understand the meaning behind words and retrieve content that is semantically related. [IDE]\n",
      "\n",
      "\n",
      "The retrieval based approach has many parameters\n",
      "Which embedding algorithm  to use?\n",
      "Which similarity measure  to use?\n",
      "How do you chunk  the documents to get a good embedding?\n",
      "How to include metadata ?\n",
      "Process tables  in the document?\n",
      "How to represent images ?\n",
      "Do you want to embedd a summary ?\n",
      "Retrieve surrounding chunks  as well?\n",
      "Embed the query, or rather a hypothetical answer  to the query\n",
      "23\n",
      "\n",
      "\n",
      "You can also just use a keyword related search\n",
      "Considering the RAG architecture, you don't need a VectorDB.\n",
      "You just need to find documents fitting a given text query.\n",
      "Algorithms that come to mind are:\n",
      "Keyword search\n",
      "TF-IDF\n",
      "BM25\n",
      "24\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on Retrieval-Augmented Generation. It explains that one can use a keyword-related search rather than the RAG architecture, which is not necessary for VectorDB. The slide emphasizes the need to find documents fitting a given text query using algorithms like keyword search, TF-IDF, and BM25. This concept is essential in NLP as it aids in retrieving relevant information from large datasets by matching keywords or phrases from a query with documents in a database. [IDE]\n",
      "\n",
      "\n",
      "Hybrid search: Combining search results\n",
      "Of course you can apply several retrieval strategies and merge search results:\n",
      "Just use all retrieved documents (not recommended; adds to context size)\n",
      "Use the top-  documents of each retrieval algorithm\n",
      "Top-  mean reciprocal re-ranked documents\n",
      "Top-  documents of a machine-learned ranking\n",
      "Given feedback, you can mix utilizing multi-armed-bandit theory\n",
      "25\n",
      "\n",
      "\n",
      "Dynamic RAG with intermediate queries\n",
      "Improve on the \"ask-once, retrieve once\" workflow.\n",
      "Utilize LLMs with text understanding tasks:\n",
      "Ask \"Are these documents interesting for the following question?\"\n",
      "Ask \"Is this an answer to the question?\"\n",
      "Generate sub-queries\n",
      "You can use frameworks like instructor or autogen to process the LLMs answers.\n",
      "26\n",
      "\n",
      "\n",
      "RAG in a corporate environment is special\n",
      "Data may not be allowed to leave the company\n",
      "Who is paying for the hardware or the service?\n",
      "Competing groups building the same thing\n",
      "Networks inside company\n",
      "User authentication / robot users\n",
      "People abusing your service\n",
      "Corporate internal certification\n",
      "27\n",
      "\n",
      "lecture_questions:\n",
      "What are the key differences between a retrieval-based approach and a keyword search in an NLP context, especially in relation to the RAG system? \n",
      "How can multi-armed bandit theory be leveraged in hybrid document retrieval for a RAG system, given user feedback on relevancy of retrieved documents?\n",
      "In a corporate environment, what are some potential challenges or special considerations when implementing an NLP RAG system and how might these require adaptations to the standard approach? \n",
      "How can intermediate queries improve the effectiveness of the interaction between users, document retrieval, and LLMs in a RAG system?\n",
      "What is the primary motivation behind using a Retrieval-Augmented Generator (RAG) in natural language processing, and what key components does it consist of?\n",
      "\n",
      "A) To reduce computation time and resources; vector search and document retrieval\n",
      "\n",
      "B) To increase context length and handle larger datasets; LLMs and multi-head attention\n",
      "\n",
      "C) To improve the understanding of visual content and tables in documents; computer vision models and table parsing algorithms\n",
      "\n",
      "D) To personalize text generation for different users; user modeling and reinforcement learning\n",
      "lecture_content: Dan Jurafsky\n",
      "Probabilistic Language Models\n",
      "• Today’s goal: assign a probability to a sentence\n",
      "• Machine Translation:\n",
      "• P(high winds tonite) > P(large winds tonite)\n",
      "• Spell Correction\n",
      "• The office is about fifteen minuets from my house\n",
      "• P(about fifteen minutes from) > P(about fifteen minuets from)\n",
      "• Speech Recognition\n",
      "• P(I saw a van) >> P(eyes awe of an)\n",
      "• + Summarization, question-answering, etc., etc.!!\n",
      "Why?\n",
      "\n",
      "[IDS] The image is a snapshot from an NLP (Natural Language Processing) lecture by Dan Jurafsky at Stanford University. The slide is titled \"Probabilistic Language Models\" and focuses on the concept of assigning a probability to a sentence. It outlines today's goal, which is to assign probabilities to sentences using machine translation as an example. The slide mentions the P(high winds tonight) and P(large winds tonight) as examples of assigning probabilities. Additionally, it introduces spell correction and speech recognition as other topics covered in the lecture. The office hours are mentioned for fifteen minutes, with a note about fifteen-minute meetings. The slide concludes with a reference to summarization, question-answering, etc., indicating these are additional topics or activities related to the lecture. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Probabilistic Language Modeling\n",
      "• Goal: compute the probability of a sentence or \n",
      "sequence of words:\n",
      "     P(W) = P(w1,w2,w3,w4,w5…wn)\n",
      "• Related task: probability of an upcoming word:\n",
      "      P(w5|w1,w2,w3,w4)\n",
      "• A model that computes either of these:\n",
      "          P(W)     or     P(wn|w1,w2…wn-1)         is called a language model.\n",
      "• Better: the grammar       But language model or LM is standard\n",
      "\n",
      "[IDS] In this NLP lecture, we're discussing Probabilistic Language Modeling. The goal is to compute the probability of a sentence or sequence of words. This involves a related task: predicting an upcoming word given a sequence of previous words. A model that computes either of these is called a language model. It's worth noting that while the grammar model is standard, the language model or LM is considered better. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "How to compute P(W)\n",
      "• How to compute this joint probability:\n",
      "• P(its, water, is, so, transparent, that)\n",
      "• Intuition: let’s rely on the Chain Rule of Probability\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on statistical language models. It explains how to compute the probability P(W) of a sequence of words W in a given context, emphasizing the importance of intuition and understanding the Chain Rule of Probability. The slide serves as a visual aid to help students grasp the concept of joint probabilities and their application in NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Reminder: The Chain Rule\n",
      "• Recall the definition of conditional probabilities\n",
      "p(B|A) = P(A,B)/P(A) \n",
      "Rewriting:   P(A,B) = P(A)P(B|A)\n",
      "• More variables:\n",
      " P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n",
      "• The Chain Rule in General\n",
      "  P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)\n",
      "\n",
      "[IDS] The image displays a slide from an NLP (Natural Language Processing) lecture, specifically discussing statistical language models. It emphasizes the concept of conditional probabilities and their role in defining the Chain Rule, which is a fundamental principle in probability theory and statistics. The slide also highlights the importance of understanding variables and their relationships within the context of language modeling. This topic is crucial for NLP as it helps in predicting the likelihood of words or phrases occurring in a given sequence, which is essential for tasks such as speech recognition, machine translation, and text generation. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "The Chain Rule applied to compute \n",
      "joint probability of words in sentence\n",
      "P(“its water is so transparent”) =\n",
      " P(its) × P(water|its) ×  P(is|its water) \n",
      "         ×  P(so|its water is) ×  P(transparent|its water is so)\n",
      "  \n",
      " \n",
      "P(w1w2…wn) =\n",
      "P(wi | w1w2…wi−1)\n",
      "i\n",
      "∏\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the Chain Rule applied to compute joint probabilities of words in a sentence. This is a concept from statistical language models, which are used to predict the likelihood of sequences of words in natural language. The slide illustrates the Chain Rule with an equation that shows how to calculate the probability of a sequence of words (P(w1...wi|wi-1)) by multiplying the individual probabilities of each word given the previous word. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "How to estimate these probabilities\n",
      "• Could we just count and divide?\n",
      "• No!  Too many possible sentences!\n",
      "• We’ll never see enough data for estimating these\n",
      "P(the |its water is so transparent that) =\n",
      "Count(its water is so transparent that the)\n",
      "Count(its water is so transparent that)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide that discusses the challenges and limitations of estimating probabilities in statistical language models. The slide is likely part of a presentation by Dan Jurafsky from Stanford University's NLP program, emphasizing the difficulty in counting and dividing to estimate probabilities when dealing with data such as water, which is considered transparent and abundant. This illustrates the complexity of modeling language patterns and the need for innovative methods to overcome these challenges in NLP. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Markov Assumption\n",
      "• Simplifying assumption:\n",
      "• Or maybe\n",
      "P(the |its water is so transparent that) ≈P(the |that)\n",
      "P(the |its water is so transparent that) ≈P(the |transparent that)\n",
      "Andrei Markov\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture, specifically focusing on the Markov Assumption. This assumption is crucial in statistical language models, which are used to predict the likelihood of sequences of words or characters in natural language. The slide simplifies the concept by suggesting that the probability of a sequence of words (P(the lit water is so transparent that)) is equal to the product of individual probabilities (P(the | that) * P(transparent | that)). This is a common approach in Markov models, where the future state is determined by the current state, and it's applied to NLP tasks like text generation and speech recognition. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Markov Assumption\n",
      "• In other words, we approximate each \n",
      "component in the product\n",
      "  \n",
      "P(w1w2…wn) ≈\n",
      "P(wi | wi−k…wi−1)\n",
      "i\n",
      "∏\n",
      "  \n",
      "P(wi | w1w2…wi−1) ≈P(wi | wi−k…wi−1)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept known as Markov Assumption. It illustrates how in NLP models, we often approximate the probability of a sequence of words by considering the probability of each word given its immediate predecessor, ignoring the rest of the context. This is a simplification that allows us to work with large amounts of data and complex language patterns. The equation shown is a key component of many statistical language models used in NLP to predict the next word in a sentence or sequence. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Simplest case: Unigram model\n",
      "fifth, an, of, futures, the, an, incorporated, a, \n",
      "a, the, inflation, most, dollars, quarter, in, is, \n",
      "mass\n",
      "thrift, did, eighty, said, hard, 'm, july, bullish\n",
      "that, or, limited, the\n",
      "Some automatically generated sentences from a unigram model\n",
      "  \n",
      "€ \n",
      "P(w1w2…wn) ≈\n",
      "P(wi)\n",
      "i\n",
      "∏\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture that focuses on the topic of statistical language models. It introduces the Unigram model as a simple case of such models, which predicts the probability of a sentence by considering each word independently. The slide provides a formula for the probability of a sentence using the unigram model and includes an example sentence with its corresponding probability calculation. The context of the lecture is to educate students about how these models work and their application in natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Condition on the previous word:\n",
      "Bigram model\n",
      "texaco, rose, one, in, this, issue, is, pursuing, growth, in, \n",
      "a, boiler, house, said, mr., gurria, mexico, 's, motion, \n",
      "control, proposal, without, permission, from, five, hundred, \n",
      "fifty, five, yen\n",
      "outside, new, car, parking, lot, of, the, agreement, reached\n",
      "this, would, be, a, record, november\n",
      "  \n",
      "P(wi | w1w2…wi−1) ≈P(wi | wi−1)\n",
      "\n",
      "[IDS] The image is a visual representation of a concept from Natural Language Processing (NLP), specifically related to Statistical Language Models. It illustrates the probability of a word given the previous word in a sequence, which is a key component of understanding language patterns and predicting the next word in a sentence or text. This model helps NLP systems to make more informed decisions when processing and generating human language. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "N-gram models\n",
      "• We can extend to trigrams, 4-grams, 5-grams\n",
      "• In general this is an insufficient model of language\n",
      "• because language has long-distance dependencies:\n",
      "“The computer which I had just put into the machine room on \n",
      "the fifth floor crashed.”\n",
      "• But we can often get away with N-gram models\n",
      "\n",
      "[IDS] In the image, we see a slide from a lecture on NLP (Natural Language Processing) that specifically focuses on statistical language models. The slide is titled \"N-gram models\" and discusses the limitations of these models when extended to longer sequences of words. It mentions the issue of long-distance dependencies and how they can cause computational problems, such as a computer crashing. The slide also suggests that these problems can be mitigated by using N-gram models. This information is presented in bullet points for clarity and emphasis. [IDE]\n",
      "\n",
      "\n",
      "Introduction to N-grams\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] In this NLP lecture, you are likely learning about statistical language models. These models use probabilities to predict the likelihood of a sequence of words or characters in a given text. They are fundamental in understanding how language is structured and can be used for tasks such as language translation, speech recognition, and text generation. The image serves as an introduction to the topic, using a word cloud to visually represent the complexity and diversity of concepts within the field of language modeling. [IDE]\n",
      "\n",
      "\n",
      "Estimating N-gram \n",
      "Probabilities\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] The image is a visual representation of the topic \"Statistical Language Models,\" which is a key concept in Natural Language Processing (NLP). It features a word cloud with various terms related to NLP and language modeling, emphasizing the complexity and interdisciplinary nature of the field. The word cloud serves as a metaphor for the interconnectedness of ideas within the subject, highlighting the importance of understanding how language works at both the sentence and paragraph levels. This kind of visualization helps students grasp the vast scope of topics that fall under the umbrella of NLP, especially statistical language models, which are used to estimate the probability of sequences of words in a given language. The presence of this image in a lecture setting suggests that the instructor is likely discussing the fundamentals of language modeling, its applications, and its role in advancing NLP technologies. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Estimating bigram probabilities\n",
      "• The Maximum Likelihood Estimate\n",
      "€ \n",
      "P(wi | wi−1) = count(wi−1,wi)\n",
      "count(wi−1)\n",
      "P(wi | wi−1) = c(wi−1,wi)\n",
      "c(wi−1)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the concept of \"Estimating bigram probabilities\" which is a fundamental aspect of statistical language models. These models use probability distributions to predict the likelihood of sequences of words in natural language. The slide presents the maximum likelihood estimate formula for calculating bigram probabilities, which is a key technique used in language modeling to determine how likely it is to see a sequence of words in a given context. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "An example\n",
      "<s> I am Sam </s>\n",
      "<s> Sam I am </s>\n",
      "<s> I do not like green eggs and ham </s>\n",
      "P(wi | wi−1) = c(wi−1,wi)\n",
      "c(wi−1)\n",
      "\n",
      "[IDS] The image displays an example from a Natural Language Processing (NLP) lecture, specifically related to the topic of Statistical language models. It illustrates how probabilities are calculated in such models, using the context of a person named Sam and their statements about green eggs and ham. The equations provided show how the probability of a sequence of words is determined based on the individual probabilities of each word within that sequence, which is a fundamental concept in NLP for understanding and generating human-like language. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "More examples: \n",
      "Berkeley Restaurant Project sentences\n",
      "• can you tell me about any good cantonese restaurants close by\n",
      "• mid priced thai food is what i’m looking for\n",
      "• tell me about chez panisse\n",
      "• can you give me a listing of the kinds of food that are available\n",
      "• i’m looking for a good place to eat breakfast\n",
      "• when is caffe venezia open during the day\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture, discussing examples of Berkeley Restaurant Project sentences. These sentences are used to illustrate the topic of statistical language models in NLP. The examples include various questions and statements about food preferences and availability at restaurants, such as asking for recommendations on good Cantonese restaurants close by, inquiring about mid-priced Thai food options, and seeking information on places to eat breakfast or have coffee in Venice. These sentences serve as practical examples of how statistical language models can be applied in real-world scenarios, particularly in understanding user queries and providing relevant responses. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Raw bigram counts\n",
      "• Out of 9222 sentences\n",
      "\n",
      "[IDS] The image displays a table representing the raw bigram counts in the context of a Natural Language Processing (NLP) lecture on Statistical language models. It illustrates how to calculate the probability of a sentence using bigrams, which are pairs of adjacent words in a sequence. This is a fundamental concept in NLP for modeling the structure of language and predicting the next word in a sentence based on the previous one. The table shows the number of times each bigram appears in a given set of sentences, emphasizing the importance of frequency in statistical language modeling. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Raw bigram probabilities\n",
      "•\n",
      "Normalize by unigrams:\n",
      "•\n",
      "Result:\n",
      "\n",
      "[IDS] In the context of an NLP lecture, the image represents a practical demonstration of statistical language models. It illustrates the concept of probability normalization and how it is applied to predict the likelihood of different words appearing in a sequence. The table shows the raw probabilities for various words such as \"i\", \"want\", \"to\", \"eat\", \"chinese\", \"food\", \"lunch\", and \"spend\". These probabilities are then normalized to ensure they sum up to 1, reflecting the model's confidence in each word given the context. This process is crucial for understanding the context in which words are used and for making accurate predictions about what might come next in a text. As we delve deeper into NLP, these statistical models form the foundation for tasks like language translation, sentiment analysis, and text generation. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Bigram estimates of sentence probabilities\n",
      "P(<s> I want english food </s>) =\n",
      " P(I|<s>)   \n",
      " ×  P(want|I)  \n",
      " ×  P(english|want)   \n",
      " ×  P(food|english)   \n",
      " ×  P(</s>|food)\n",
      "       =  .000031\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a fundamental concept in statistical language modeling. The slide is titled \"Bigram estimates of sentence probabilities,\" indicating that it discusses how to estimate the probability of a sequence of two words in a sentence, also known as bigrams. The equation P(<s> I want english food <s>) = P(I|<s>) x P(want|I) x P(english|want) x P(food|english) x P(<s>/|food) =.000031 illustrates how these probabilities are calculated and combined to determine the overall probability of a given sentence. This is a crucial aspect of understanding how computers can predict the next word in a sentence or evaluate the likelihood of a sentence being part of a certain language or text corpus. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "What kinds of knowledge?\n",
      "• P(english|want)  = .0011\n",
      "• P(chinese|want) =  .0065\n",
      "• P(to|want) = .66\n",
      "• P(eat | to) = .28\n",
      "• P(food | to) = 0\n",
      "• P(want | spend) = 0\n",
      "• P (i | <s>) = .25\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing statistical language models. It provides examples of how probability is used to represent the likelihood of different linguistic structures, such as sentences or phrases, within a language. The slide is likely part of a larger presentation that would delve into the applications and methodologies of statistical language models in NLP tasks like language generation or sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Practical Issues\n",
      "• We do everything in log space\n",
      "• Avoid underflow\n",
      "• (also adding is faster than multiplying)\n",
      "log(p1 × p2 × p3 × p4) = log p1 + log p2 + log p3 + log p4\n",
      "\n",
      "[IDS] In the context of a NLP lecture, this image represents a slide discussing practical issues related to statistical language models. It emphasizes the need to handle these challenges in log space, avoid underflow, and mentions that addition is faster than multiplication when dealing with logarithms. The formula provided demonstrates how to compute the logarithm of a product using properties of logarithms. This information is crucial for understanding and implementing efficient algorithms in natural language processing tasks that involve statistical models. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Language Modeling Toolkits\n",
      "• SRILM\n",
      "• http://www.speech.sri.com/projects/srilm/\n",
      "• KenLM\n",
      "• https://kheafield.com/code/kenlm/\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture, specifically focusing on \"Statistical language models\". It highlights three language modeling toolkits: SRI LM, KenLM, and https://kheafield.com/code/kenlm/. These toolkits are likely used for generating statistical models of language to improve various NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Google N-Gram Release, August 2006\n",
      "…\n",
      "\n",
      "[IDS] In the image, you see a screenshot of an announcement for the release of the Google N-Gram Release from August 2006. The announcement is made by Alex Franz and Thorsten Brants from the Google Machine Translation team. The context of this lecture is likely related to Natural Language Processing (NLP), specifically focusing on statistical language models. The Google N-Gram data set is a significant resource in NLP research, providing information about word frequencies across various corpora. In this case, they have processed over a trillion words, which is a testament to the scale of data used in modern NLP applications. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Google N-Gram Release\n",
      "•\n",
      "serve as the incoming 92\n",
      "•\n",
      "serve as the incubator 99\n",
      "•\n",
      "serve as the independent 794\n",
      "•\n",
      "serve as the index 223\n",
      "•\n",
      "serve as the indication 72\n",
      "•\n",
      "serve as the indicator 120\n",
      "•\n",
      "serve as the indicators 45\n",
      "•\n",
      "serve as the indispensable 111\n",
      "•\n",
      "serve as the indispensible 40\n",
      "•\n",
      "serve as the individual 234\n",
      "http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html\n",
      "\n",
      "[IDS] The image represents a Google N-Gram Release, which is a dataset containing information about the frequency of words and phrases in books from Google's massive book digitization project. In the context of an NLP (Natural Language Processing) lecture, this data can be used to study language trends and patterns over time. It is particularly useful for statistical language models, which aim to predict the likelihood of sequences of words in a given text by analyzing their historical usage. The presence of this image in the lecture suggests that the topic might involve discussing how such datasets contribute to the development and training of statistical language models. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Google Book N-grams\n",
      "• http://ngrams.googlelabs.com/\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a reference to a resource related to statistical language models. The Google Book N-grams data set is a significant resource in NLP, providing insights into how frequently sequences of n items occur together. This can be particularly useful for understanding language patterns and trends. The URL provided leads to a Google Labs project that likely offers access to this valuable data set, which would be essential for students or researchers working on statistical language modeling tasks. [IDE]\n",
      "\n",
      "\n",
      "Estimating N-gram \n",
      "Probabilities\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] In the image, you can see a visual representation of a lecture focused on \"Statistical Language Models,\" which is a topic within the field of Natural Language Processing (NLP). The word cloud in the background is composed of various words related to language and modeling, such as \"probability,\" \"grammar,\" \"modeling,\" \"estimating,\" and \"probabilities.\" These words highlight the key concepts discussed in the lecture. The presence of these terms suggests that the lecture might cover how statistical models are used to estimate probabilities in language processing tasks. The visual design effectively conveys the central theme of the lecture and serves as an engaging backdrop for the topic. [IDE]\n",
      "\n",
      "\n",
      "Language \n",
      "Modeling\n",
      "Evaluation and Perplexity\n",
      "\n",
      "\n",
      "How to evaluate N-gram models\n",
      "\"Extrinsic (in-vivo) Evaluation\"\n",
      "To compare models A and B\n",
      "1. Put each model in a real task\n",
      "• Machine Translation, speech recognition, etc. \n",
      "2. Run the task, get a score for A and for B\n",
      "• How many words translated correctly\n",
      "• How many words transcribed correctly\n",
      "3. Compare accuracy for A and B\n",
      "\n",
      "\n",
      "Intrinsic (in-vitro) evaluation \n",
      "Extrinsic evaluation not always possible\n",
      "• Expensive, time-consuming \n",
      "• Doesn't always generalize to other applications\n",
      "Intrinsic evaluation: perplexity\n",
      "• Directly measures language model performance at \n",
      "predicting words.\n",
      "• Doesn't necessarily correspond with real application \n",
      "performance\n",
      "• But gives us a single general metric for language models\n",
      "• Useful for large language models (LLMs) as well as n-grams\n",
      "\n",
      "\n",
      "Training sets and test sets\n",
      "We train parameters of our model on a training set.\n",
      "We test the model’s performance on data we \n",
      "haven’t seen.\n",
      "◦A test set is an unseen dataset; different from training set.\n",
      "◦Intuition: we want to measure generalization to unseen data\n",
      "◦An evaluation metric (like perplexity) tells us how well \n",
      "our model does on the test set.\n",
      "\n",
      "\n",
      "Choosing training and test sets\n",
      "• If we're building an LM for a specific task\n",
      "• The test set should reflect the task language we \n",
      "want to use the model for\n",
      "• If we're building a general-purpose model\n",
      "• We'll need lots of different kinds of training \n",
      "data\n",
      "• We don't want the training set or the test set to \n",
      "be just from one domain or author or language.\n",
      "\n",
      "\n",
      "Training on the test set\n",
      "We can’t allow test sentences into the training set\n",
      "• Or else the LM will assign that sentence an artificially \n",
      "high probability when we see it in the test set\n",
      "• And hence assign the whole test set a falsely high \n",
      "probability.\n",
      "• Making the LM look better than it really is\n",
      "This is called “Training on the test set”\n",
      "Bad science! \n",
      "33\n",
      "\n",
      "\n",
      "Dev sets\n",
      "•If we test on the test set many times we might \n",
      "implicitly tune to its characteristics\n",
      "•Noticing which changes make the model better. \n",
      "•So we run on the test set only once, or a few times\n",
      "•That means we need a third dataset: \n",
      "• A development test set or, devset. \n",
      "• We test our LM on the devset until the very end\n",
      "• And then test our LM on the test set once\n",
      "\n",
      "\n",
      "Intuition of perplexity as evaluation metric: \n",
      "How good is our language model?\n",
      "Intuition: A good LM prefers \"real\" sentences\n",
      "•\n",
      "Assign higher probability to “real” or “frequently \n",
      "observed” sentences \n",
      "•\n",
      "Assigns lower probability to “word salad” or \n",
      "“rarely observed” sentences?\n",
      "\n",
      "\n",
      "Intuition of perplexity 2: \n",
      "Predicting upcoming words\n",
      "The Shannon Game: How well can we \n",
      "predict the next word?\n",
      "•  Once upon a ____\n",
      "•  That is a picture of a  ____\n",
      "•  For breakfast I ate my usual ____\n",
      "Unigrams are terrible at this game (Why?)\n",
      " \n",
      " \n",
      " \n",
      "time \n",
      "0.9\n",
      "dream \n",
      "0.03\n",
      "midnight 0.02\n",
      "…\n",
      "and \n",
      "1e-100\n",
      "Picture credit: Historiska bildsamlingen\n",
      "https://creativecommons.org/licenses/by/2.0/\n",
      "Claude Shannon\n",
      "A good LM is one that assigns a higher probability \n",
      "to the next word that actually occurs\n",
      "\n",
      "[IDS] The image is a slide from a lecture on statistical language models, specifically focusing on the Shannon Game. It demonstrates the concept of perplexity, which measures how well a model predicts the next word in a sequence. The slide features a picture of Claude Shannon and includes a text-based example of the game, along with a graph that illustrates the perplexity values over time. This visual aid helps students understand how a higher likelihood (lower perplexity) correlates with a better prediction of upcoming words in a text. [IDE]\n",
      "\n",
      "\n",
      "Intuition of perplexity 3: The best language model \n",
      "is one that best predicts the entire unseen test set\n",
      "• We said: a good LM is one that assigns a higher \n",
      "probability to the next word that actually occurs. \n",
      "• Let's generalize to all the words!\n",
      "• The best LM assigns high probability to the entire test \n",
      "set.\n",
      "• When comparing two LMs, A and B\n",
      "• We compute PA(test set) and PB(test set)\n",
      "• The better LM will give a higher probability to (=be less \n",
      "surprised by) the test set than the other LM.\n",
      "\n",
      "\n",
      "• Probability depends on size of test set\n",
      "• Probability gets smaller the longer the text\n",
      "• Better: a metric that is per-word, normalized by length\n",
      "• Perplexity is the inverse probability of the test set, \n",
      "normalized by the number of words\n",
      "Intuition of perplexity 4: Use perplexity instead of \n",
      "raw probability\n",
      "PP(W)\n",
      "=\n",
      "P(w1w2...wN )\n",
      "−1\n",
      "N\n",
      "           \n",
      "=\n",
      "1\n",
      "P(w1w2...wN )\n",
      "N\n",
      "\n",
      "\n",
      "Perplexity is the inverse probability of the test set, \n",
      "normalized by the number of words\n",
      "(The inverse comes from the original definition of perplexity \n",
      "from cross-entropy rate in information theory)\n",
      "Probability range is  [0,1], perplexity range is [1,∞]\n",
      "Minimizing perplexity is the same as maximizing probability\n",
      "Intuition of perplexity 5: the inverse\n",
      "PP(W)\n",
      "=\n",
      "P(w1w2...wN )\n",
      "−1\n",
      "N\n",
      "           \n",
      "=\n",
      "1\n",
      "P(w1w2...wN )\n",
      "N\n",
      "\n",
      "\n",
      "Intuition of perplexity 6: N-grams\n",
      "PP(W)\n",
      "=\n",
      "P(w1w2...wN )\n",
      "−1\n",
      "N\n",
      "           \n",
      "=\n",
      "1\n",
      "P(w1w2...wN )\n",
      "N\n",
      "Bigrams:\n",
      "Chain rule:\n",
      "\n",
      "[IDS] The image is a visual aid for a lecture on \"Statistical language models,\" specifically focusing on the concept of perplexity and N-grams. Perplexity is a measure of how well a statistical model predicts a sample. A lower perplexity score indicates that the model has a better fit to the data. In this context, N-grams are sequences of n items from a given sample of text or speech. The slide provides a mathematical formula for calculating PP(W) in terms of P(w1w2...wn), which represents the probability of a sequence of words. This formula is a part of the larger equation for PP(W), which involves summing over all possible sequences of words up to length N, and then dividing by the number of such sequences. The slide also introduces the chain rule and Bigrams, which are special cases of N-grams where n=2. Understanding these concepts is crucial for developing effective language models that can predict the next word in a sentence based on the context provided by previous words. [IDE]\n",
      "\n",
      "\n",
      "Intuition of perplexity 7: \n",
      "Weighted average branching factor\n",
      "Perplexity is also the weighted average branching factor of a language. \n",
      "Branching factor: number of possible next words that can follow any word\n",
      "Example: Deterministic language L = {red,blue, green} \n",
      " \n",
      "Branching factor = 3 (any word can be followed by red, blue, green)\n",
      "Now assume LM A where each word follows any other word with equal probability ⅓\n",
      "Given a test set T = \"red red red red blue\"\n",
      "PerplexityA(T) = PA(red red red red blue)-1/5 = \n",
      "But now suppose red was very likely in training set, such that for LM B:\n",
      "◦\n",
      "P(red) = .8   p(green) = .1  p(blue) = .1\n",
      "We would expect the probability to be higher, and hence the perplexity to be smaller:\n",
      "PerplexityB(T) = PB(red red red red blue)-1/5 \n",
      "((⅓)5)-1/5 = (⅓)-1\n",
      "=3\n",
      "= (.8 * .8 * .8 * .8 * .1) -1/5\n",
      "=.04096 -1/5\n",
      "= .527-1\n",
      "= 1.89\n",
      "\n",
      "\n",
      "Holding test set constant:\n",
      "Lower perplexity = better language model\n",
      "Training 38 million words, test 1.5 million words, WSJ\n",
      "N-gram \n",
      "Order\n",
      "Unigram\n",
      "Bigram\n",
      "Trigram\n",
      "Perplexity 962\n",
      "170\n",
      "109\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>N-gram\n",
      "Order</th>\n",
      "      <th>Unigram</th>\n",
      "      <th>Bigram</th>\n",
      "      <th>Trigram</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Perplexity</td>\n",
      "      <td>962</td>\n",
      "      <td>170</td>\n",
      "      <td>109</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Language \n",
      "Modeling\n",
      "Evaluation and Perplexity\n",
      "\n",
      "\n",
      "Language \n",
      "Modeling\n",
      "Sampling and Generalization\n",
      "\n",
      "\n",
      "The Shannon (1948) Visualization Method\n",
      "Sample words from an LM\n",
      "Unigram:\n",
      "REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME \n",
      "CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO \n",
      "OF TO EXPERT GRAY COME TO FURNISHES THE LINE \n",
      "MESSAGE HAD BE THESE. \n",
      "Bigram:\n",
      "THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER \n",
      "THAT THE CHARACTER OF THIS POINT IS THEREFORE \n",
      "ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO \n",
      "EVER TOLD THE PROBLEM FOR AN UNEXPECTED.\n",
      "Claude Shannon\n",
      "\n",
      "[IDS] The image you're seeing is likely a slide from a lecture on NLP (Natural Language Processing), specifically focusing on statistical language models. It discusses two methods: the Shannon (1948) Visualization Method and the Bigram method, both used to represent and analyze the probability of word sequences in text. The slide features a quote from Claude Shannon, emphasizing the importance of representing and analyzing natural language effectively, which is crucial for understanding how language works and for developing advanced NLP applications. [IDE]\n",
      "\n",
      "\n",
      "How Shannon sampled those words in 1948\n",
      "\"Open a book at random and select a letter at random on the page. \n",
      "This letter is recorded. The book is then opened to another page \n",
      "and one reads until this letter is encountered. The succeeding \n",
      "letter is then recorded. Turning to another page this second letter \n",
      "is searched for and the succeeding letter recorded, etc.\"\n",
      "\n",
      "[IDS] The image you're viewing is a visual representation of the process described in the text, which is related to statistical language models. These models are used in natural language processing (NLP) to predict the next word in a sequence based on statistical patterns learned from a large corpus of text data. The book in the image symbolizes the vast amount of data that such models are trained on, while the open pages represent the randomness and variability of language. The act of selecting a letter at random and recording it in the book illustrates how statistical language models learn by observing and predicting the frequency and co-occurrence of words in texts. This process helps the model to generate coherent and contextually relevant text, which is essential for tasks like language translation, speech recognition, and text completion. [IDE]\n",
      "\n",
      "\n",
      "Sampling a word from a distribution\n",
      "0\n",
      "1\n",
      "0.06\n",
      "the\n",
      ".06\n",
      "0.03\n",
      "of\n",
      "0.02\n",
      "a\n",
      "0.02\n",
      "to in\n",
      ".09 .11 .13 .15\n",
      "…\n",
      "however\n",
      "(p=.0003)\n",
      "polyphonic\n",
      "p=.0000018\n",
      "…\n",
      "0.02\n",
      ".66\n",
      ".99\n",
      "…\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>0-0.06</th>\n",
      "      <th>1-0.03</th>\n",
      "      <th>2-0.02</th>\n",
      "      <th>3-0.02</th>\n",
      "      <th>4-0.02</th>\n",
      "      <th>Col5</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "[IDS] The image is a visual representation of a concept in statistical language models, specifically illustrating the process of sampling from a distribution. It demonstrates how words are sampled with probabilities proportional to their frequency in a text corpus. The polyphonic however, with a probability of.0003, is sampled as an example. This image helps learners understand how language models predict the likelihood of a word occurring in a sequence by assigning probabilities based on the frequency of word co-occurrence in large datasets. The slide serves as a practical explanation of the theoretical concepts taught in NLP lectures, making it easier for students to grasp the underlying principles of statistical language modeling. [IDE]\n",
      "\n",
      "\n",
      "Visualizing Bigrams the Shannon Way\n",
      "Choose a random bigram (<s>, w) \n",
      "         according to its probability p(w|<s>)\n",
      "Now choose a random bigram        (w, x) \n",
      "according to its probability p(x|w)\n",
      "And so on until we choose </s>\n",
      "Then string the words together\n",
      "<s> I\n",
      "    I want\n",
      "      want to\n",
      "           to eat\n",
      "              eat Chinese\n",
      "                  Chinese food\n",
      "                          food </s>\n",
      "I want to eat Chinese food\n",
      "\n",
      "\n",
      "Note: there are other sampling methods\n",
      "Used for neural language models\n",
      "Many of them avoid generating words from the very \n",
      "unlikely tail of the distribution\n",
      "We'll discuss when we get to neural LM decoding:\n",
      "◦Temperature sampling\n",
      "◦Top-k sampling\n",
      "◦Top-p sampling\n",
      "\n",
      "\n",
      "Approximating Shakespeare\n",
      "of modeling the training corpus as we increase the value of N.\n",
      "We can use the sampling method from the prior section to visualize both of\n",
      "these facts! To give an intuition for the increasing power of higher-order n-grams,\n",
      "Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n",
      "gram models trained on Shakespeare’s works.\n",
      "1\n",
      "–To him swallowed confess hear both. Which. Of save on trail for are ay device and\n",
      "rote life have\n",
      "gram\n",
      "–Hill he late speaks; or! a more to leg less first you enter\n",
      "2\n",
      "–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n",
      "king. Follow.\n",
      "gram\n",
      "–What means, sir. I confess she? then all sorts, he is trim, captain.\n",
      "3\n",
      "–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n",
      "’tis done.\n",
      "gram\n",
      "–This shall forbid it should be branded, if renown made it empty.\n",
      "4\n",
      "–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n",
      "great banquet serv’d in;\n",
      "gram\n",
      "–It cannot be but so.\n",
      "Figure 3.4\n",
      "Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All\n",
      "characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\n",
      "for capitalization to improve readability.\n",
      "The longer the context on which we train the model, the more coherent the sen-\n",
      "tences. In the unigram sentences, there is no coherent relation between words or any\n",
      "sentence-final punctuation. The bigram sentences have some local word-to-word\n",
      "coherence (especially if we consider that punctuation counts as a word). The tri-\n",
      "\n",
      "\n",
      "Shakespeare as corpus\n",
      "N=884,647 tokens, V=29,066\n",
      "Shakespeare produced 300,000 bigram types out of \n",
      "V2= 844 million possible bigrams.\n",
      "◦So 99.96% of the possible bigrams were never seen (have \n",
      "zero entries in the table)\n",
      "◦That sparsity is even worse for 4-grams, explaining why \n",
      "our sampling generated actual Shakespeare.\n",
      "\n",
      "\n",
      "The Wall Street Journal is not Shakespeare\n",
      "3.5\n",
      "•\n",
      "GENERALIZATION AND ZEROS\n",
      "13\n",
      "1\n",
      "Months the my and issue of year foreign new exchange’s september\n",
      "were recession exchange new endorsed a acquire to six executives\n",
      "gram\n",
      "2\n",
      "Last December through the way to preserve the Hudson corporation N.\n",
      "B. E. C. Taylor would seem to complete the major central planners one\n",
      "gram\n",
      "point five percent of U. S. E. has already old M. X. corporation of living\n",
      "on information such as more frequently fishing to keep her\n",
      "3\n",
      "They also point to ninety nine point six billion dollars from two hundred\n",
      "four oh six three percent of the rates of interest stores as Mexico and\n",
      "gram\n",
      "Brazil on market conditions\n",
      "Figure 3.5\n",
      "Three sentences randomly generated from three n-gram models computed from\n",
      "40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\n",
      "tion as words. Output was then hand-corrected for capitalization to improve readability.\n",
      "we need a training corpus of legal documents. To build a language model for a\n",
      "question-answering system, we need a training corpus of questions.\n",
      "It is equally important to get training data in the appropriate dialect or variety,\n",
      "especially when processing social media posts or spoken transcripts. For exam-\n",
      "\n",
      "\n",
      "Can you guess the author? These 3-gram sentences \n",
      "are sampled from an LM trained on who?\n",
      "1) They also point to ninety nine point \n",
      "six billion dollars from two hundred four \n",
      "oh six three percent of the rates of \n",
      "interest stores as Mexico and gram Brazil \n",
      "on market conditions \n",
      "2) This shall forbid it should be branded, \n",
      "if renown made it empty. \n",
      "3) “You are uniformly charming!” cried he, \n",
      "with a smile of associating and now and \n",
      "then I bowed and they perceived a chaise \n",
      "and four to wish for. \n",
      "53\n",
      "\n",
      "\n",
      "Choosing training data\n",
      "If task-specific, use a training corpus that has a similar \n",
      "genre to your task.\n",
      "• If legal or medical, need lots of special-purpose documents\n",
      "Make sure to cover different kinds of dialects and \n",
      "speaker/authors.\n",
      "• Example: African-American Vernacular English (AAVE)\n",
      "• One of many varieties that can be used by African Americans and others\n",
      "• Can include the auxiliary verb finna that marks immediate future tense:\n",
      "• \"My phone finna die\"\n",
      "\n",
      "\n",
      "The perils of overfitting\n",
      "N-grams only work well for word prediction if the \n",
      "test corpus looks like the training corpus\n",
      "• But even when we try to pick a good training \n",
      "corpus, the test set will surprise us!\n",
      "• We need to train robust models that generalize!\n",
      "One kind of generalization: Zeros\n",
      "• Things that don’t ever occur in the training set\n",
      "• But occur in the test set\n",
      "\n",
      "\n",
      "Zeros\n",
      "Training set:\n",
      "… ate lunch\n",
      "… ate dinner\n",
      "… ate a\n",
      "… ate the\n",
      "P(“breakfast” | ate) = 0\n",
      "• Test set\n",
      "… ate lunch\n",
      "… ate breakfast\n",
      "\n",
      "\n",
      "Zero probability bigrams\n",
      "Bigrams with zero probability\n",
      "◦Will hurt our performance for texts where those words \n",
      "appear!\n",
      "◦And mean that we will assign 0 probability to the test set!\n",
      "And hence we cannot compute perplexity (can’t \n",
      "divide by 0)!\n",
      "\n",
      "\n",
      "Language \n",
      "Modeling\n",
      "Sampling and Generalization\n",
      "\n",
      "\n",
      "Smoothing: Add-one \n",
      "(Laplace) smoothing\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] The image is a visual representation of the concept of \"Language Modeling\" in Natural Language Processing (NLP). It emphasizes the importance of statistical language models in understanding and generating human language. The word cloud serves as a metaphor for the complexity and interconnectedness of words and their meanings within language. The term \"Smoothing: Add-one (Laplace) smoothing\" suggests that the lecture might cover techniques to improve the performance of language models by adding a small amount of probability mass to all events, which is a common method in statistical language modeling to handle the problem of unseen data. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "The intuition of smoothing (from Dan Klein)\n",
      "•\n",
      "When we have sparse statistics:\n",
      "•\n",
      "Steal probability mass to generalize better\n",
      "P(w | denied the)\n",
      "  3 allegations\n",
      "  2 reports\n",
      "  1 claims\n",
      "  1 request\n",
      "  7 total\n",
      "P(w | denied the)\n",
      "  2.5 allegations\n",
      "  1.5 reports\n",
      "  0.5 claims\n",
      "  0.5 request\n",
      "  2 other\n",
      "  7 total\n",
      "allegations\n",
      "reports\n",
      "claims\n",
      "attack\n",
      "request\n",
      "man\n",
      "outcome\n",
      "…\n",
      "allegations\n",
      "attack\n",
      "man\n",
      "outcome\n",
      "…\n",
      "allegations\n",
      "reports\n",
      "claims\n",
      "request\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the concept of \"The intuition of smoothing\" from Dan Klein. The slide explains how sparse statistics can be handled when working with language models. It introduces the concept of P(w | denied the) and mentions various statistics such as 3 allegations, 2 reports, 1 claim, 1 request, and a total count of 7. The slide also discusses the idea of stealing probability mass to generalize better, showing a probability distribution with different outcomes like allegations, claims, requests, and outcomes such as attack, man, and outcome. This is likely part of a larger discussion on how to improve the performance and robustness of statistical language models by considering the context and rarity of certain events or words in the data. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Add-one estimation\n",
      "• Also called Laplace smoothing\n",
      "• Pretend we saw each word one more time than we did\n",
      "• Just add one to all the counts!\n",
      "• MLE estimate:\n",
      "• Add-1 estimate:\n",
      "P\n",
      "MLE(wi | wi−1) = c(wi−1,wi)\n",
      "c(wi−1)\n",
      "P\n",
      "Add−1(wi | wi−1) = c(wi−1,wi)+1\n",
      "c(wi−1)+V\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image likely represents a slide discussing techniques for improving language models. The \"Add-one estimation\" method is a technique used to smooth the Laplace smoothing in NLP models. It's a way to prevent overfitting by adding a small value to the counts of each word in a document, which helps to deal with rare words or out-of-vocabulary (OOV) words. The formula shown calculates the probability of a word given its context in the model, taking into account the added value. This is an important concept in statistical language models as it improves the model's ability to predict the likelihood of a word appearing in a given context. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Maximum Likelihood Estimates\n",
      "•\n",
      "The maximum likelihood estimate\n",
      "• of some parameter of a model M from a training set T\n",
      "• maximizes the likelihood of the training set T given the model M\n",
      "•\n",
      "Suppose the word “bagel” occurs 400 times in a corpus of a million words\n",
      "•\n",
      "What is the probability that a random word from some other text will be \n",
      "“bagel”?\n",
      "•\n",
      "MLE estimate is 400/1,000,000 = .0004\n",
      "•\n",
      "This may be a bad estimate for some other corpus\n",
      "• But it is the estimate that makes it most likely that “bagel” will occur 400 times in \n",
      "a million word corpus.\n",
      "\n",
      "[IDS] In the image, we see a slide from a lecture on Natural Language Processing (NLP), specifically discussing statistical language models. The slide is titled \"Maximum Likelihood Estimates\" and explains the concept of likelihood estimation in NLP. It mentions that maximum likelihood estimation is used to estimate the parameters of a model M from a training set T and to maximize the likelihood of the training set given the model M. The slide also provides an example of a bad estimate for some other corpus and defines it as the estimate that makes it most likely that a \"bagel\" will occur 400 times in a million word corpus. This is likely part of a larger discussion on how these estimates are used to improve language understanding and generation in NLP applications. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Berkeley Restaurant Corpus: Laplace \n",
      "smoothed bigram counts\n",
      "\n",
      "[IDS] The image depicts a table illustrating the concept of bigram counts, which is a fundamental aspect of statistical language models in NLP. These models analyze patterns in large datasets to predict the likelihood of word sequences in human languages. The table shows how bigrams (pairs of adjacent words) can be counted and used to determine probabilities for predicting the next word in a sequence. This is essential for tasks like text prediction, machine translation, and speech recognition. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Laplace-smoothed bigrams\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a concept from the field of statistical language models. Specifically, it illustrates how to compute the probability of a sequence of words (in this case, \"Laplace-smoothed bigrams\") using the formula P(w_n|w_{n-1}) = C(w_{n-1}w_n)/C(w_{n-1}) + V. This formula is used to predict the likelihood of a word given the previous word in a sequence, with the addition of a Laplace smoothing technique to avoid zero probabilities. The table shows an example of this computation for various words and categories like 'i want to eat' and 'chinese food', which helps learners understand how these models can be applied to predict text or speech. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Reconstituted counts\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Statistical Language Models, specifically focusing on the concept of reconstituted counts. It illustrates the calculation of C(w|n) using the formula provided in the slide's header and demonstrates how to apply this formula to different words and their respective counts. This is a fundamental concept in NLP, as it helps in understanding how language patterns are represented and analyzed statistically. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Compare with raw bigram counts\n",
      "\n",
      "[IDS] The image is a visual representation of bigram counts in the context of Natural Language Processing (NLP), which is a field within artificial intelligence that focuses on the interaction between computers and humans in natural language. Bigrams are pairs of words that occur together in a text or speech, and counting them can provide insights into language patterns and relationships between words. The table compares two different datasets: one with 827 instances and another with 527 instances, highlighting the number of times each word pair appears together. This information is crucial for NLP tasks such as language modeling, where understanding how words co-occur can improve machine learning algorithms' ability to predict the next word in a sequence. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Add-1 estimation is a blunt instrument\n",
      "• So add-1 isn’t used for N-grams: \n",
      "• We’ll see better methods\n",
      "• But add-1 is used to smooth other NLP models\n",
      "• For text classification \n",
      "• In domains where the number of zeros isn’t so huge.\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the slide discusses the concept of add-1 estimation as a blunt instrument. It emphasizes that this method is not used for N-grams and instead serves to smooth other NLP models. The slide also mentions that add-1 estimation is particularly useful in domains with a large number of zeros, which could be related to the scarcity of certain words or phrases in a dataset. [IDE]\n",
      "\n",
      "\n",
      "Smoothing: Add-one \n",
      "(Laplace) smoothing\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] The image you've shared is a visual representation of the topic \"Statistical Language Models\" being discussed in an NLP lecture. It features a word cloud with terms related to language modeling, such as 'probability','sentence', 'word', 'language', and'model'. The central theme is emphasized by the large text \"Language Modeling\" at the top. Additionally, there's a highlighted section on \"Smoothing: Add-one (Laplace) smoothing\", which is a technique used in statistical language models to improve their performance. This particular method helps prevent zero probabilities in the model by adding a small constant value to the count of each word, making it more robust and accurate. [IDE]\n",
      "\n",
      "\n",
      "Interpolation, Backoff, \n",
      "and Web-Scale LMs\n",
      "Language \n",
      "Modeling\n",
      "\n",
      "[IDS] The image represents a visual representation of the topic \"Statistical language models\" in a lecture context. The word \"probability\" is prominently displayed, indicating its significance in understanding how statistical models work in language processing. The other words such as \"feature\", \"algorithms\", \"words\", and \"corpus\" suggest that the lecture will cover various aspects of language modeling, including the extraction of features from text data, the algorithms used to build language models, and the importance of a large corpus for training these models. The presence of words like \"tagging\", \"part-of-speech\", and \"language\" further emphasizes the linguistic focus of the lecture. Overall, the image conveys that the lecture will delve into the intricacies of statistical language models, their applications, and their role in natural language processing. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Backoff and Interpolation\n",
      "• Sometimes it helps to use less context\n",
      "• Condition on less context for contexts you haven’t learned much about \n",
      "• Backoff: \n",
      "• use trigram if you have good evidence,\n",
      "• otherwise bigram, otherwise unigram\n",
      "• Interpolation: \n",
      "• mix unigram, bigram, trigram\n",
      "• Interpolation works better\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Statistical language models\". The slide is titled \"Backoff and Interpolation\" and discusses techniques used in NLP to handle out-of-vocabulary words or unknown contexts. It mentions that sometimes using less context can be helpful, and it introduces the concept of backoff as a method to use trigrams if there's evidence, otherwise bigrams, and unigrams as the last resort. Additionally, interpolation is discussed as a technique that mixes different n-grams to improve the model's performance. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Linear Interpolation\n",
      "• Simple interpolation\n",
      "• Lambdas conditional on context:\n",
      "from all the N-gram estimators, weighing and combining the trigram, bigram, and\n",
      "unigram counts.\n",
      "In simple linear interpolation, we combine different order N-grams by linearly\n",
      "interpolating all the models. Thus, we estimate the trigram probability P(wn|wn−2wn−1)\n",
      "by mixing together the unigram, bigram, and trigram probabilities, each weighted\n",
      "by a l:\n",
      "ˆ\n",
      "P(wn|wn−2wn−1) = l1P(wn|wn−2wn−1)\n",
      "+l2P(wn|wn−1)\n",
      "+l3P(wn)\n",
      "(4.24)\n",
      "such that the ls sum to 1:\n",
      "X\n",
      "i\n",
      "li = 1\n",
      "(4.25)\n",
      "In a slightly more sophisticated version of linear interpolation, each l weight is\n",
      "computed in a more sophisticated way, by conditioning on the context. This way,\n",
      "if we have particularly accurate counts for a particular bigram, we assume that the\n",
      "counts of the trigrams based on this bigram will be more trustworthy, so we can\n",
      "make the ls for those trigrams higher and thus give that trigram more weight in\n",
      "ˆ\n",
      "P(wn|wn−2wn−1) = l1P(wn|w\n",
      "+l2P(wn|\n",
      "+l3P(wn)\n",
      "such that the ls sum to 1:\n",
      "X\n",
      "i\n",
      "li = 1\n",
      "In a slightly more sophisticated version of linear i\n",
      "computed in a more sophisticated way, by condition\n",
      "if we have particularly accurate counts for a particul\n",
      "counts of the trigrams based on this bigram will be\n",
      "make the ls for those trigrams higher and thus giv\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on statistical language models. It introduces the concept of linear interpolation, which is a method used to combine multiple language models into one more accurate model. The slide explains that in linear interpolation, multiple models are weighted and summed together to produce a final output probability. The lambda values (λ) represent the weights for each model, and the sum of these weights is equal to 1, ensuring that the combined model's probabilities add up to 1 as well. The context of this topic is further elaborated by mentioning that lambdas can be conditioned on certain factors, such as previous words in a sequence, which is a crucial aspect of NLP when dealing with language modeling. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "How to set the lambdas?\n",
      "• Use a held-out corpus\n",
      "• Choose λs to maximize the probability of held-out data:\n",
      "• Fix the N-gram probabilities (on the training data)\n",
      "• Then search for λs that give largest probability to held-out set:\n",
      "Training Data\n",
      "Held-Out \n",
      "Data\n",
      "Test \n",
      "Data\n",
      "logP(w1...wn | M(λ1...λk)) =\n",
      "logP\n",
      "M (λ1...λk )(wi | wi−1)\n",
      "i\n",
      "∑\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Statistical Language Models, focusing on the process of setting the lambdas. It explains that one should use a held-out corpus for training data and test data to maximize the probability of held-out data. The slide suggests fixing N-gram probabilities on the training data and then searching for λs that give the largest probability to held-out sets. This is represented by a mathematical formula that calculates the log probability of a sequence given a set of λs. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Unknown words: Open versus closed \n",
      "vocabulary tasks\n",
      "•\n",
      "If we know all the words in advanced\n",
      "• Vocabulary V is fixed\n",
      "• Closed vocabulary task\n",
      "•\n",
      "Often we don’t know this\n",
      "• Out Of Vocabulary = OOV words\n",
      "• Open vocabulary task\n",
      "•\n",
      "Instead: create an unknown word token <UNK>\n",
      "• Training of <UNK> probabilities\n",
      "• Create a fixed lexicon L of size V\n",
      "• At text normalization phase, any training word not in L changed to  <UNK>\n",
      "• Now we train its probabilities like a normal word\n",
      "• At decoding time\n",
      "• If text input: Use UNK probabilities for any word not in training\n",
      "\n",
      "[IDS] The image is a slide from a lecture on NLP (Natural Language Processing) focusing on statistical language models. It discusses the concept of \"unknown words\" and their handling in such models, particularly when vocabulary is either fixed or open. The slide suggests that in advanced language processing tasks like closed vocabulary tasks or tasks with a large number of unknown words (OOV), it's crucial to address the issue of OOV words. The slide provides strategies for dealing with these OOV words, such as creating a lexicon of sizes LV, training a fixed lexical size model at text normalization phase, and training the model with its probabilities like a normal word. Additionally, it mentions the use of decoding time and inputting UNK probabilities for any word not in training. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Huge web-scale n-grams\n",
      "• How to deal with, e.g., Google N-gram corpus\n",
      "• Pruning\n",
      "• Only store N-grams with count > threshold.\n",
      "• Remove singletons of higher-order n-grams\n",
      "• Entropy-based pruning\n",
      "• Efficiency\n",
      "• Efficient data structures like tries\n",
      "• Bloom filters: approximate language models\n",
      "• Store words as indexes, not strings\n",
      "• Use Huffman coding to fit large numbers of words into two bytes\n",
      "• Quantize probabilities (4-8 bits instead of 8-byte float)\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the image likely represents a slide discussing various techniques used in statistical language models. These models are essential for tasks like language understanding and generation. The slide highlights methods such as pruning, which involves removing less important elements to save space, and efficiency strategies like Bloom filters and Huffman coding. These techniques help in managing large datasets and improving computational efficiency, which is crucial for effective NLP processing. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Smoothing for Web-scale N-grams\n",
      "• “Stupid backoff” (Brants et al. 2007)\n",
      "• No discounting, just use relative frequencies \n",
      "75\n",
      "S(wi | wi−k+1\n",
      "i−1 ) =\n",
      "count(wi−k+1\n",
      "i\n",
      ")\n",
      "count(wi−k+1\n",
      "i−1 )   if  count(wi−k+1\n",
      "i\n",
      ") > 0\n",
      "0.4S(wi | wi−k+2\n",
      "i−1 )      otherwise\n",
      "\"\n",
      "#\n",
      "$\n",
      "$\n",
      "%\n",
      "$\n",
      "$\n",
      "S(wi) = count(wi)\n",
      "N\n",
      "\n",
      "[IDS] The image is from a lecture on NLP (Natural Language Processing), specifically focusing on \"Statistical language models.\" It introduces the concept of \"Smoothing for Web-scale N-grams,\" which is a technique used to handle the issue of \"Stupid backoff\" as mentioned in a paper by Brants et al. in 2007. The slide explains that without smoothing, just using relative frequencies can lead to the problem of \"stupid backoff.\" To address this, the slide presents a formula for computing the count of an i-th order word sequence, considering both the immediate past and other sequences. This method aims to improve the accuracy of language models by distributing the probability mass more evenly across different n-gram sequences. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "N-gram Smoothing Summary\n",
      "• Add-1 smoothing:\n",
      "• OK for text categorization, not for language modeling\n",
      "• The most commonly used method:\n",
      "• Extended Interpolated Kneser-Ney\n",
      "• For very large N-grams like the Web:\n",
      "• Stupid backoff\n",
      "76\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP). It discusses N-gram Smoothing, a technique used in statistical language models to improve the accuracy of predictions. The slide highlights the importance of adding 1 to smoothing and mentions two common methods: Extended Interpolated Kneser-Ney and For very large N-grams like the Web: Stupid backoff. These methods help in handling rare events or out-of-vocabulary words in text categorization tasks. [IDE]\n",
      "\n",
      "\n",
      "Dan Jurafsky\n",
      "Advanced Language Modeling\n",
      "• Discriminative models:\n",
      "•  choose n-gram weights to improve a task, not to fit the  \n",
      "training set\n",
      "• Parsing-based models\n",
      "• Caching Models\n",
      "• Recently used words are more likely to appear\n",
      "• These perform very poorly for speech recognition (why?)\n",
      "P\n",
      "CACHE(w | history) = λP(wi | wi−2wi−1)+(1−λ) c(w ∈history)\n",
      "| history |\n",
      "\n",
      "[IDS] The image depicts a slide from an NLP (Natural Language Processing) lecture focused on the topic of \"Statistical language models.\" The slide outlines the importance of choosing appropriate models for tasks in NLP. It emphasizes the use of discriminative models, which involve selecting n-gram weights to improve performance rather than fitting the training data. Additionally, it mentions parsing-based and caching models as alternatives. The slide also discusses the role of recently used words in improving model performance, suggesting that words like 'history' might be more likely to appear in future contexts. The formula provided is indicative of how such statistical models can predict the likelihood of word sequences based on historical data. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the intuition behind the smoothing techniques in statistical language modeling, and how do they help with the sparsity issue of n-gram models?\n",
      "How does the use of caching models impact speech recognition tasks as opposed to text-based applications? \n",
      "How can N-gram probabilities be combined through interpolation, and why might simple linear interpolation not always provide the best results in practice?\n",
      "What is the primary purpose of smoothing techniques in statistical language models? \n",
      "\n",
      "A) To improve the probability assigned to unseen n-grams during evaluation on test sets\n",
      "\n",
      "B) To reduce the complexity of the model for more efficient processing\n",
      "\n",
      "C) To increase the likelihood that all words are part of the model's vocabulary\n",
      "\n",
      "D) To specifically weight more recently used words as more likely to appear next in a text[IDE]\n",
      "Question:In the lecture content, a bigram model is built for text from the movie *Spider-Man Homecoming*. The `bigram_probability` method in the BigramModel class calculates the probability of one word given another. Can you explain, step-by-step, how the probabilities are calculated and what data structures are used to enable quick access to these probabilities?\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "Definition:\n",
      "Representing a word as a vector in a vector space.\n",
      "”Embedding” a word in a vector space.\n",
      "\n",
      "[IDS] In this NLP lecture, the topic of discussion is \"Static_embedding_OneHot_encoding,\" which refers to the concept of representing words as vectors in a vector space. This process is crucial for natural language processing tasks such as text classification, sentiment analysis, and language modeling. OneHot encoding is a technique used to convert categorical variables into numerical form that can be fed into machine learning algorithms. In the context of word embeddings, it helps in creating a dense vector representation of words that captures their semantic meaning. The slide likely contains further details about how one-hot encoding is applied to create these embeddings, and its importance in modern NLP applications. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "goose\n",
      "\n",
      "[IDS] The image is a visual representation of word embeddings, which are a method in Natural Language Processing (NLP) to convert words into vectors of numbers. The lecture topic \"Static_embedding_OneHot_encoding\" likely refers to the process of creating static embeddings using one-hot encoding, a technique where each word is represented as a binary vector indicating its presence or absence in a vocabulary. The x and y axes represent dimensions in the embedding space, with each word's position reflecting its semantic similarity to other words. This visualization helps in understanding how words are related semantically in a language model. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "Why do we need to turn words into vectors?\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on Natural Language Processing (NLP). It's likely part of a series, as indicated by the numbers on the left side. The main focus of this slide is to introduce the concept of word embeddings and one-hot encoding. Word embeddings are a way to represent words in a vector space where semantically similar words are closer together. One-hot encoding is a technique used to convert categorical data into a form that can be used by machine learning algorithms. In the context of NLP, it's often used to convert words into vectors for further processing. The slide poses a question about the necessity of turning words into vectors, which is a common practice in NLP to enable machines to understand and process language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "Why do we need to turn words into vectors?\n",
      "- Machine learning models work with vectors\n",
      "- We can do vector math\n",
      "- We can quantify similarity between vectors\n",
      "\n",
      "[IDS] The image is a slide from a Natural Language Processing (NLP) lecture, specifically focusing on the topic of word embeddings. It poses the question, \"What are word embeddings?\" as a starting point for the discussion. The slide emphasizes the need to turn words into vectors and lists several reasons why this is necessary. These reasons include working with machine learning models, performing vector math, and quantifying similarity between vectors. The slide also features logos for Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting that these institutions are involved in the lecture or the presentation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "How do we find the dimensions to use?\n",
      "\n",
      "[IDS] The image displays a slide from a Natural Language Processing (NLP) lecture. The slide is titled \"What are word embeddings?\" and poses the question of how one would find the dimensions to use in word embeddings. The term \"OneHot_encoding\" is mentioned, which refers to a technique used to convert categorical data into a form that can be used by machine learning algorithms. The presence of logos suggests that the lecture is part of a collaboration between Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, and the name Tim Metzler might be associated with the lecture or the course. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "How do we find the dimensions to use?\n",
      "- Expert knowledge\n",
      "\n",
      "[IDS] In the image, you see a slide from a Natural Language Processing (NLP) lecture that focuses on the topic of word embeddings and their dimensions. The slide poses the question \"What are word embeddings?\" and prompts the audience to consider how they find the dimensions to use in these embeddings. It also mentions \"Expert knowledge\" as a potential approach for answering this question. The logos at the bottom suggest that this lecture is part of a series or event involving Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and is presented by Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "How do we find the dimensions to use?\n",
      "- Expert knowledge\n",
      "- Domain dependent\n",
      "\n",
      "[IDS] In this NLP lecture, the focus is on understanding word embeddings and their dimensions. The slide titled \"What are word embeddings?\" introduces the concept of finding the dimensions to use for these embeddings. It emphasizes the importance of expertise in knowledge domains and the need for domain-dependent approaches to effectively utilize word embeddings. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What are word embeddings?\n",
      "How do we find the dimensions to use?\n",
      "- Expert knowledge\n",
      "- Domain dependent\n",
      "- What can be dimensions for all English words?\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the image represents a discussion about word embeddings and how to find dimensions to use for different types of language models. The slide outlines three main points: Expert knowledge, Domain dependent, and What can be dimensions for all English words? These points likely serve as a framework for understanding the complexity of word embeddings and the various approaches to selecting appropriate dimensions for language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "What can be dimensions for all English words?\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture focused on \"One Hot Encoding\". It poses the question: \"What can be dimensions for all English words?\" This suggests that the topic is about representing categorical data in a form that can be input into machine learning models. One hot encoding is a technique used to convert non-numerical data into a format that can be used by statistical algorithms. The presence of logos indicates the lecture is likely associated with the Hochschule Bonn-Rhein and Tim Metzler, who may be the lecturer or a contributor to the course material. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "What can be dimensions for all English words?\n",
      "Simplest approach: Using the index of the word\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of \"Static_embedding_OneHot_encoding\". It seems to be focusing on a method for encoding words into numerical vectors, which is a common technique in NLP to enable machine learning algorithms to process and analyze text data. The slide poses the question of what dimensions can be used for all English words and suggests a simple approach using the index of the word as a one-hot encoding. This is likely part of a larger discussion on how to represent words in a way that computers can understand and use in models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "What can be dimensions for all English words?\n",
      "Simplest approach: Using the index of the word\n",
      "→ Each word is its own “dimension”\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP). It discusses the concept of \"One Hot Encoding,\" which is a technique used in NLP and machine learning to convert categorical variables into a form that can be provided to machine learning algorithms. The slide poses the question, \"What can be dimensions for all English words?\" suggesting that it will explore ways to represent English words in a numerical format that can be used for computational purposes. The slide also mentions \"Simplest approach: Using the index of the word → Each word is its own 'dimension',\" indicating that one way to encode words is by assigning them a unique index as their representation. Additionally, there are logos at the bottom, likely representing the institutions or sponsors involved in the lecture: Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "Example corpus:\n",
      "A dog is an animal. A cat is an animal. My dog is \n",
      "playful. A cat is playful.\n",
      "\n",
      "[IDS] The image you're seeing is likely a slide from an NLP (Natural Language Processing) lecture, specifically discussing the concept of \"One Hot Encoding.\" One Hot Encoding is a technique used in machine learning and data preprocessing where categorical variables are converted into a form that can be provided to machine learning algorithms. It's represented here through an example of encoding a simple statement about a dog and a cat, indicating how each category is converted into a binary vector. This is a fundamental concept in preparing text data for models like decision trees or neural networks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "Example corpus:\n",
      "A dog is an animal. A cat is an animal. My dog is \n",
      "playful. A cat is playful.\n",
      "Index (lower case):\n",
      "0 → a\n",
      "1 → an\n",
      "2 → animal\n",
      "3 → cat\n",
      "4 → dog\n",
      "5 → is\n",
      "6 → my\n",
      "7 → playful\n",
      "\n",
      "[IDS] The image is a visual representation of a concept from NLP, specifically related to \"Static_embedding_OneHot_encoding\". It showcases a \"One Hot Encoding\" approach where each word in the corpus is represented as a binary vector. The lecture likely explains how this encoding technique is used to convert categorical variables into numerical form, which is essential for machine learning models that require numerical inputs. The example provided in the image demonstrates the process of converting words into their corresponding one-hot encoded vectors, which can then be used to train and evaluate NLP models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "Example corpus:\n",
      "A dog is an animal. A cat is an animal. My dog is \n",
      "playful. A cat is playful.\n",
      "Index (lower case):\n",
      "0 → a\n",
      "1 → an\n",
      "2 → animal\n",
      "3 → cat\n",
      "4 → dog\n",
      "5 → is\n",
      "6 → my\n",
      "7 → playful\n",
      "[([1, 0, 0, 0, 0, 0, 0, 0], 'a'),\n",
      " ([0, 1, 0, 0, 0, 0, 0, 0], 'an'),\n",
      " ([0, 0, 1, 0, 0, 0, 0, 0], 'animal'),\n",
      " ([0, 0, 0, 1, 0, 0, 0, 0], 'cat'),\n",
      " ([0, 0, 0, 0, 1, 0, 0, 0], 'dog'),\n",
      " ([0, 0, 0, 0, 0, 1, 0, 0], 'is'),\n",
      " ([0, 0, 0, 0, 0, 0, 1, 0], 'my'),\n",
      " ([0, 0, 0, 0, 0, 0, 0, 1], 'playful')]\n",
      "Embeddings\n",
      "\n",
      "[IDS] The image displays a slide from an NLP (Natural Language Processing) lecture that focuses on the concept of \"One Hot Encoding\". It serves as an example to illustrate how categorical variables can be converted into a form suitable for machine learning algorithms. The slide is structured to show the transformation of a simple sentence about a dog and its actions into a numerical representation using one-hot encoding. This process is essential in NLP, as it allows models to interpret and process text data more effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "One Hot Encoding\n",
      "Embed a document:\n",
      "”A dog is a playful animal” \n",
      "sum up the word embeddings for each word in the sentence\n",
      "[([1, 0, 0, 0, 0, 0, 0, 0], 'a'),\n",
      " ([0, 1, 0, 0, 0, 0, 0, 0], 'an'),\n",
      " ([0, 0, 1, 0, 0, 0, 0, 0], 'animal'),\n",
      " ([0, 0, 0, 1, 0, 0, 0, 0], 'cat'),\n",
      " ([0, 0, 0, 0, 1, 0, 0, 0], 'dog'),\n",
      " ([0, 0, 0, 0, 0, 1, 0, 0], 'is'),\n",
      " ([0, 0, 0, 0, 0, 0, 1, 0], 'my'),\n",
      " ([0, 0, 0, 0, 0, 0, 0, 1], 'playful')]\n",
      "Embeddings\n",
      "[2, 0, 1, 0, 1, 1, 0, 1]\n",
      "\n",
      "[IDS] In the image, you can see a slide from an NLP lecture that discusses \"Static_embedding_OneHot_encoding.\" It provides an example of how to encode words into vectors using one-hot encoding. The slide illustrates this concept with a simple sentence: \"A dog is a playful animal,\" and shows how each word in the sentence is represented as a vector in a high-dimensional space where all dimensions except for one are zero (the one corresponding to the index of the word in the vocabulary) and one dimension is set to 1. This method is used to convert categorical data into a form that can be inputted into machine learning models. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the main purpose of converting words into vector representations in natural language processing, and why can't machine learning models work directly with text data? \n",
      "Why might one-hot encoding not always be an ideal representation for word embeddings, and what are some alternative methods to capture semantic meaning in vectors?\n",
      "In the context of NLP, why is it important for a model to understand the similarity between words, and how does representing words as vectors help with this task?\n",
      "What is the primary purpose of one-hot encoding in NLP?\n",
      "\n",
      "A) To reduce dimensionality of word embeddings\n",
      "\n",
      "B) To convert categorical data into a usable format for machine learning algorithms\n",
      "\n",
      "C) To quantify the semantic similarity between words in vector space\n",
      "\n",
      "D) To create a sparse representation of text documents for improved computational efficiency\n",
      "**Question:**\n",
      "\n",
      "In the `TfIdfModel` class, the method `build_index` is implemented to create a dictionary that maps each word type in the documents to an index. The same functionality was already implemented in the `OneHotModel` class and can be reused there. Should you copy that implementation over or write your own? What are the trade-offs between these two decisions, considering the need for maintaining the code DRY while also avoiding unnecessary complexity due to shared code with a different model?\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "Why?\n",
      "Make embeddings that make it easy to distinguish \n",
      "between documents.\n",
      "Used in information retrieval (find relevant documents to a \n",
      "search query)\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "Two parts:\n",
      "- tf → How often does a term appear in a document\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "Two parts:\n",
      "- tf → How often does a term appear in a document\n",
      "- df → In how many documents does a term appear\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "Two parts:\n",
      "- tf → How often does a term appear in a document\n",
      "- df → In how many documents does a term appear\n",
      "- idf → Inverse of df. idf = N/df\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Idea:\n",
      "Take a corpus (collection of documents), count occurences of \n",
      "terms within documents. \n",
      "Normalize over documents.\n",
      "Two parts:\n",
      "- tf → How often does a term appear in a document\n",
      "- df → In how many documents does a term appear\n",
      "- idf → Inverse of df. idf = N/df\n",
      "Intuition:\n",
      "A term is important if it appears often in a document.\n",
      "A term is important if it only appears in a few documents.\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Example corpus:\n",
      "corpus = [\n",
      "    \"A dog is an animal. A dog is not a cat.\",\n",
      "    \"A cat is an animal.\",\n",
      "    \"My dog is playful.\",\n",
      "    \"I like animals. Linux is not an animal.\",\n",
      "    \"Cat is a linux command. Dog is not.\",\n",
      "    \"Dog is not a linux command.\",\n",
      "    \"My dog likes linux. my dog is playful\"\n",
      "]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Count occurences of words (terms) in each document\n",
      "(term frequency tf)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Count in how many documents a term appears \n",
      "(document frequency df)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Document \n",
      "Frequency\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "5\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Take inverse of document frequency by dividing number of documents by df\n",
      "(idf)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Inverse Document \n",
      "Frequency\n",
      "6/4\n",
      "6/3\n",
      "6/3\n",
      "6/1\n",
      "6/3\n",
      "6/2\n",
      "6/5\n",
      "6/1\n",
      "6/6\n",
      "6/1\n",
      "6/1\n",
      "6/4\n",
      "6/2\n",
      "6/4\n",
      "6/2\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Take inverse of document frequency by dividing number of documents by df\n",
      "(idf)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Inverse Document \n",
      "Frequency\n",
      "1.5\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "1.2\n",
      "6\n",
      "1\n",
      "6\n",
      "6\n",
      "1.5\n",
      "3\n",
      "1.5\n",
      "3\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Normalize idf with logarithm (log10)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Inverse Document \n",
      "Frequency (log)\n",
      "0.18\n",
      "0.3\n",
      "0.3\n",
      "0.78\n",
      "0.3\n",
      "0.48\n",
      "0.08\n",
      "0.78\n",
      "0\n",
      "0.78\n",
      "0.78\n",
      "0.18\n",
      "0.48\n",
      "0.18\n",
      "0.48\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Normalize idf with logarithm (log10)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Inverse Document \n",
      "Frequency (log)\n",
      "0.18\n",
      "0.3\n",
      "0.3\n",
      "0.78\n",
      "0.3\n",
      "0.48\n",
      "0.08\n",
      "0.78\n",
      "0\n",
      "0.78\n",
      "0.78\n",
      "0.18\n",
      "0.48\n",
      "0.18\n",
      "0.48\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Normalize idf with logarithm (log10)\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "A cat is an animal.\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "Dog is not a linux command. \n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "Inverse Document \n",
      "Frequency (log)\n",
      "0.18\n",
      "0.3\n",
      "0.3\n",
      "0.78\n",
      "0.3\n",
      "0.48\n",
      "0.08\n",
      "0.78\n",
      "0\n",
      "0.78\n",
      "0.78\n",
      "0.18\n",
      "0.48\n",
      "0.18\n",
      "0.48\n",
      "idf of 0 → can not be used to distinguish documents. \n",
      "Appears in all of them!\n",
      "idf of 0 → can not be used to distinguish documents. \n",
      "Appears in all of them!\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Multiply tf with idf \n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      ".54\n",
      ".30\n",
      ".30\n",
      "0\n",
      ".30\n",
      "0\n",
      ".16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      "A cat is an animal.\n",
      ".18\n",
      ".30\n",
      ".30\n",
      "0\n",
      ".30\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".48\n",
      "0\n",
      ".48\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      ".30\n",
      ".30\n",
      ".78\n",
      "0\n",
      "0\n",
      "0\n",
      ".78\n",
      "0\n",
      ".78\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      ".18\n",
      "0\n",
      "0\n",
      "0\n",
      ".30\n",
      ".48\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "Dog is not a linux command. \n",
      ".18\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".48\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".16\n",
      "0\n",
      "0\n",
      "0\n",
      ".78\n",
      ".18\n",
      ".96\n",
      "0\n",
      ".48\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Term Frequency – Inverse Document Frequency (TFIDF)\n",
      "Document embeddings with words as dimensions\n",
      "a\n",
      "an\n",
      "animal\n",
      "animals\n",
      "cat\n",
      "command\n",
      "dog\n",
      "i\n",
      "is\n",
      "like\n",
      "likes\n",
      "linux\n",
      "my\n",
      "not \n",
      "playful\n",
      "A dog is an animal. A dog is \n",
      "not a cat.\n",
      ".54\n",
      ".30\n",
      ".30\n",
      "0\n",
      ".30\n",
      "0\n",
      ".16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      "A cat is an animal.\n",
      ".18\n",
      ".30\n",
      ".30\n",
      "0\n",
      ".30\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "My dog is playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".48\n",
      "0\n",
      ".48\n",
      "I like animals. Linux is not an \n",
      "animal.\n",
      "0\n",
      ".30\n",
      ".30\n",
      ".78\n",
      "0\n",
      "0\n",
      "0\n",
      ".78\n",
      "0\n",
      ".78\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "Cat is a linux command. Dog \n",
      "is not.\n",
      ".18\n",
      "0\n",
      "0\n",
      "0\n",
      ".30\n",
      ".48\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "Dog is not a linux command. \n",
      ".18\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".48\n",
      ".08\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".18\n",
      "0\n",
      ".18\n",
      "0\n",
      "My dog likes linux. My dog is \n",
      "playful.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ".16\n",
      "0\n",
      "0\n",
      "0\n",
      ".78\n",
      ".18\n",
      ".96\n",
      "0\n",
      ".48\n",
      "\n",
      "lecture_questions:\n",
      "What is the purpose of normalizing term frequencies in the TF-IDF method?\n",
      "In the given example, what are the calculated TF-IDF values for \"a\" and \"linux\", respectively? \n",
      "Why would a term with an inverse document frequency of 0 not be useful for distinguishing between documents in this context?\n",
      "What is the effect of a high term frequency and low document frequency on a word's TF-IDF value in a text corpus?\n",
      "\n",
      "A) The value will be low, as high tf and low df could indicate the word is common within a single document but not very important across the entire corpus.\n",
      "\n",
      "B) The value will be moderate, as the high tf somewhat offsets the low df.\n",
      "\n",
      "C) The value will be high, as a high tf and a low df may suggest the word is particularly significant to one specific document in the corpus.\n",
      "\n",
      "D) It's impossible to say without more information on the inverse document frequency values.\n",
      "Based on the given excerpt, a potential question could be: \n",
      "\n",
      "\n",
      "\n",
      "In the 'TfIdfModel' class, what is the purpose of the 'df_vector' and how is it computed from the 'term_doc_matrix'? \n",
      "\n",
      "Code snippet for reference:\n",
      "\n",
      "```python\n",
      "\n",
      "df_vector = np.zeros(len(self.index))\n",
      "\n",
      "# Fill the matrix and compute the tfidf matrix\n",
      "\n",
      "raise NotImplementedError()\n",
      "\n",
      "```\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What have we seen so far?\n",
      "1) One Hot Encoding: \n",
      "●Representation of words as binary vectors\n",
      "●Sparse\n",
      "●Lack of semantic information\n",
      "●Size: |V|\n",
      "2) TFIDF Embedding\n",
      "●Term Frequency-Inverse Document Frequency\n",
      "●Reflects importance of word in document relative \n",
      "to document collection\n",
      "●Weighs down frequent terms, scales up rare ones\n",
      "●Still lacks contextual understanding\n",
      "●Size: |D| (number of documents)\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the topic of \"Static_embedding_word2Vec\". It highlights various aspects of Word2Vec, such as one-hot encoding, which uses binary vectors to represent words. It also mentions the term frequency-inverse document frequency (TF-IDF) embedding, reflecting the importance of words in documents and their relative frequency. The slide notes that Word2Vec still lacks contextual understanding and discusses the size of the embedding, mentioning that it's measured in terms of the number of documents. This information suggests that the lecture is focused on understanding how word embeddings, like those created by Word2Vec, can be used to represent words in a way that captures their semantic meaning within a given context. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Recap: Similarity and Relatedness\n",
      "●Similarity: Likeliness between two words in \n",
      "meaning or context. (Can I replace the word in the \n",
      "sentence with the other word?)\n",
      "●Relatedness: How strongly are two words \n",
      "associated. (Are the two words likely to appear \n",
      "together?)\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on word embeddings using the Word2Vec model. It highlights the concept of \"Static_embedding_word2Vec,\" which refers to the process of creating fixed representations of words in a vector space, where semantically similar words are close to each other. The slide emphasizes the importance of similarity and relatedness in understanding how these word embeddings capture the meaning and context of words. It also mentions that the strength of association between two words can be determined by their proximity in this vector space, with closely associated words appearing together. This concept is fundamental in NLP tasks such as text classification, machine translation, and language modeling. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Recap: Similarity and Relatedness\n",
      "●Similarity: Likeliness between two words in \n",
      "meaning or context. (Can I replace the word in the \n",
      "sentence with the other word?)\n",
      "●Relatedness: How strongly are two words \n",
      "associated. (Are the two words likely to appear \n",
      "together?)\n",
      "”I like to eat pizza. I like to eat stew” \n",
      "→ pizza and stew are similar in this context\n",
      "”Pizza is baked”\n",
      "→  pizza and baked are related since they often \n",
      "appear together\n",
      "There is no clear distinction between these two!\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Static_embedding_word2Vec.\" This slide aims to explain the concept of similarity and relatedness in the context of word embeddings, which are a method for representing words as vectors in a high-dimensional space. The slide emphasizes the importance of understanding how closely related two words are by discussing likeliness and relatedness. It uses examples like \"pizza\" and \"stew\" to illustrate that although these words are associated, they are not strongly related, whereas words like \"pizza\" and \"baked\" are more strongly related since they often appear together. This is likely part of a larger discussion on how word embeddings can be used to capture semantic relationships between words, which is a fundamental aspect of NLP tasks such as text classification, sentiment analysis, and machine translation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "Consider the following sentence:\n",
      "Berlin is the capital of Germany.\n",
      "What does this tell us about Berlin?\n",
      "Obvious: Berlin is the capital of Germany.\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of word embeddings. The title \"Static_embedding_word2Vec\" suggests that the lecture is discussing static word embeddings, which are a type of representation for words in a vector space, and it mentions Word2Vec as a particular algorithm used to create these embeddings. The bullet points provide further context, stating that Berlin is the capital of Germany and asking the audience to consider what this tells us about Berlin, with the answer being that Berlin is also the capital of Germany. This seems to be an example or exercise related to understanding the context and semantic relationships between words in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "Consider the following sentence:\n",
      "Walala is the capital of Lampukistan.\n",
      "What does this tell us about Walala?\n",
      "\n",
      "[IDS] The image appears to be a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of word embeddings using the Word2Vec algorithm. The title \"Distributional Structure of Language\" suggests that the lecture is discussing how words in a language are used together, which is a foundational concept in understanding how word embeddings work. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "Consider the following sentence:\n",
      "Walala is the capital of Lampukistan.\n",
      "What does this tell us about Walala?\n",
      "→ Walala is similar to words that appear in the\n",
      "    context of “is the capital of”.\n",
      "→ Walala is a city.\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image is likely discussing the topic of \"Static_embedding_word2Vec,\" which refers to a method for converting words into fixed-length vectors in a high-dimensional space. These vectors capture semantic relationships between words and can be used for various NLP tasks such as text classification, clustering, or language translation. The slide might be introducing this concept, explaining how word embeddings work, or discussing their applications in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "Words that appear in a similar context are similar.\n",
      "How do we learn embeddings that capture these \n",
      "semantic relationships?\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Static_embedding_word2Vec\". The title of the slide is \"Distributional Structure of Language\", which suggests that the lecture might be discussing how words appear in similar contexts and how we can learn embeddings that capture these semantic relationships. The slide also includes logos and names of institutions or individuals, such as Hochschule Bonn-Rhein-Sieg, Fachbereich Informatik, and Tim Metzler, indicating their involvement in the lecture or the course. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "Words that appear in a similar context are similar.\n",
      "How do we learn embeddings that capture these \n",
      "semantic relationships?\n",
      "Requirements:\n",
      "●Fixed vector size\n",
      "●Similar words should have similar representations in \n",
      "the vector space\n",
      "●General vectors, not optimized for a specific domain\n",
      "●Easy to learn\n",
      "●Can learn from vast amounts of data (e.g. Wikipedia, \n",
      "Common Crawl, etc)\n",
      "\n",
      "[IDS] In this NLP lecture, we are exploring the concept of static embeddings using Word2Vec. This involves learning vector representations for words that capture their semantic relationships in a similar context. The requirements for such a model include fixed vector size and similar representations in the vector space for semantically similar words. General vectors should not be optimized for a specific domain, making it easy to learn and transferable across different domains like Wikipedia, Common Crawl, or others. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Distributional Structure of Language\n",
      "We need a learning task that will produce these \n",
      "vectors.\n",
      "Idea: Train a simple classifier to predict word from \n",
      "context or predict context from word.\n",
      "\n",
      "[IDS] The image you're seeing is likely a slide from a Natural Language Processing (NLP) lecture, specifically focusing on the topic of word embeddings using the Word2Vec algorithm. The title \"Distributional Structure of Language\" suggests that the lecture will discuss how language can be represented in a mathematical form based on its distributional properties. The main idea presented is the need for a learning task that produces vectors to capture the semantic relationships between words. To achieve this, the lecture proposes training a simple classifier to predict a word from its context or predict the context from a word, which is essentially what Word2Vec does. The institutions mentioned at the bottom, Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, indicate that the lecture might be part of a course offered by these institutions. The name Tim Metzler could be the instructor or author of the lecture material. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec\n",
      "Developed by Mikolov et al, Google (2013)\n",
      "“Efficient Estimation of Word Representations in \n",
      "Vector Space” \n",
      "(https://arxiv.org/abs/1301.3781)\n",
      "“Distributed Representations of Words and \n",
      "Phrases and their Compositionality” in Advances \n",
      "in Neural Information Processing Systems \n",
      "(doi:10.48550/arXiv.1310.4546)\n",
      "Idea: Train a simple classifier to predict word from \n",
      "context or predict context from word.\n",
      "\n",
      "[IDS] The image displays a slide from a Natural Language Processing (NLP) lecture that focuses on the topic of static embeddings using the Word2Vec algorithm. The slide introduces the concept of \"Efficient Estimation of Word Representations in Vector Space\" developed by Mikolov et al., as presented in their 2013 paper published in Google. It highlights the significance of distributed representations of words and phrases, which are advanced in neural information processing systems. The slide also emphasizes the idea of training a simple classifier to predict word context or context from word, which is an essential aspect of understanding how Word2Vec works in NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec\n",
      "Example Sentence:\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Word2Vec, a popular technique in Natural Language Processing (NLP) for learning vector representations of words. The slide presents an example sentence to illustrate the concept of static embedding in Word2Vec. Static embedding refers to the process of mapping each word in a vocabulary to a fixed-length vector, which captures its semantic and syntactic properties. In this case, the example sentence is \"The quick brown fox jumps over the lazy dog.\" This sentence is often used as a test phrase in NLP tasks because it contains all the letters of the English alphabet. By analyzing such sentences, researchers can evaluate how well Word2Vec captures the meaning of individual words and their relationships with one another. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec\n",
      "Example Sentence:\n",
      "\n",
      "[IDS] The image is a visual representation of how Word2Vec, a popular technique in Natural Language Processing (NLP), embeds words into vectors. It illustrates the concept of static embeddings, which are fixed-length vectors that represent words in a high-dimensional space. In this context, each word is assigned a unique vector, and words with similar meanings or usage patterns are placed closer together in this space. The example sentence provided is \"The quick brown fox jumps over the lazy dog,\" which is used to demonstrate how words can be mapped to their corresponding vectors in a 2-dimensional space. This visualization helps NLP practitioners understand how Word2Vec captures semantic relationships between words by positioning them in proximity based on their semantic similarities. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec\n",
      "Continuous Bag-Of-Word Model\n",
      "Predict center word from context\n",
      "\n",
      "[IDS] The image depicts a Word2Vec model, which is a technique for creating word embeddings in Natural Language Processing (NLP). It's designed to map words into a vector space where semantically similar words are closer together. In this visualization, we see the continuous bag-of-words (CBOW) architecture of Word2Vec, where the goal is to predict the target word based on its context. The input layer consists of a classifier that processes the context words and predicts the target word. This model is commonly used in NLP tasks like language translation, sentiment analysis, and text generation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec\n",
      "Skip Gram Model\n",
      "Predict context from center word.\n",
      "We will use this approach for our \n",
      "examples!\n",
      "\n",
      "[IDS] The image is a visual representation of Word2Vec, a popular technique in Natural Language Processing (NLP) that maps words to vectors of real numbers. In this context, we are discussing the Skip Gram Model, which is a method used by Word2Vec to learn vector representations of words. The model predicts the context words surrounding a target word, and these predictions are used to train a classifier. The classifier then learns to associate certain properties with words, such as 'brown' being associated with 'dog', 'fox', and 'jumps'. This approach allows for the creation of word embeddings, which are dense vectors that capture semantic relationships between words. The lecture likely covers how these embeddings can be utilized in various NLP tasks, such as text classification or language translation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Creating Training Examples\n",
      "Create training examples from each sentence in the corpus.\n",
      "Training examples are of the form (word1, word2).\n",
      "Our context never crosses sentence boundaries!\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of Word2Vec. Word2Vec is a technique for converting words into vectors, which are numerical representations that capture semantic and syntactic features of the words. The slide provides an example of how to create training examples for Word2Vec by using sentences from a corpus. It explains that training examples are formed by taking individual words (word1, word2) from a sentence. This process helps in understanding the context and relationships between words, which is crucial for NLP tasks like language modeling, text classification, and machine translation. The slide also includes logos of the Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, indicating the institutions involved or associated with the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Creating Training Examples\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a slide focused on teaching how to create training examples for Word2Vec, which is a method for converting text into vector space. The slide is likely part of a presentation aimed at explaining the concept of static embeddings in Word2Vec. The diagram illustrates the process of taking a sentence, splitting it into individual words, and then creating pairs of words to be used as training examples. These examples are essential for the Word2Vec algorithm to learn how to represent each word as a vector in the vector space. The lecture aims to educate the audience on the mechanics behind this technique and its importance in NLP tasks such as text classification, language translation, and semantic analysis. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Creating Training Examples\n",
      "\n",
      "[IDS] The image represents a slide from a lecture on Natural Language Processing (NLP), specifically focusing on Word2Vec, which is a method for converting text into vectors of real numbers that capture the semantic meaning of words. The slide aims to provide training examples and context to help understand how Word2Vec works in creating embeddings. The embedding process is visualized through arrows connecting words like \"quick\", \"brown\", \"fox\", and \"dog\" to their corresponding vectors, indicating how each word is represented in the vector space. This representation allows machines to understand the relationships between words, such as synonymy or antonymy, and can be used in various NLP tasks like language translation, sentiment analysis, and text classification. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Creating Training Examples\n",
      "\n",
      "[IDS] The image illustrates a concept from NLP (Natural Language Processing) known as Word2Vec, specifically focusing on static embeddings. The diagram provides an example of how words are converted into vectors that capture their semantic meaning. In this context, the word \"brown\" is used to show how it can be represented as a vector in a higher-dimensional space, where similar words like \"quick\" and \"fox\" might also have similar representations. This technique helps computers understand the relationships between words and is a crucial step in various NLP tasks such as language translation, text classification, and sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Creating Training Examples\n",
      "\n",
      "[IDS] The image represents a concept from the lecture on static embeddings in Word2Vec, which is a technique used in natural language processing (NLP) to map words into vectors of real numbers. These vectors capture semantic and syntactic features of the words and are used for various NLP tasks such as text classification, machine translation, and question-answering systems. The diagram shows how the model learns to represent similar words in close proximity in the vector space, enabling word analogies and semantic searches. The context provided by the surrounding words is crucial for understanding the meaning of each word in isolation, highlighting the importance of word embeddings in capturing the nuances of language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Skip Gram Architecture\n",
      "\n",
      "[IDS] The image is a visual representation of the Word2Vec algorithm's skip-gram architecture, which is a type of neural network model used in natural language processing (NLP). In this context, we are learning about how static embeddings work within the Word2Vec framework. The diagram illustrates the process of training the model using an example input \"fox\" and its corresponding output \"jumps.\" The architecture consists of an input layer, a hidden layer (W1), and an output layer (W2). The goal of this model is to learn vector representations of words that capture their semantic meaning, allowing for better performance in tasks such as language translation or text classification. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Skip Gram Architecture\n",
      "y=softmax (xW1W 2)\n",
      "\n",
      "[IDS] The image is a visual representation of the skip-gram architecture used in the Word2Vec model, which is a technique for representing words in a high-dimensional vector space. This architecture is designed to predict the context words given a target word, with the goal of learning embeddings that capture semantic relationships between words. The skip-gram model depicted here uses two layers: an input layer and an output layer, connected by a matrix W1, which represents the learned embeddings. During training, the model aims to predict the surrounding words (jumps) given the current word (fox), using the softmax function to compute the probabilities. This process helps in capturing the semantic meaning of words and their relationships in the context of natural language processing. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Extracting the Embeddings\n",
      "\n",
      "[IDS] The image illustrates a concept from Word2Vec, a popular algorithm in Natural Language Processing (NLP) that assigns numerical vectors to words in a way that captures their semantic meaning. The figure represents the process of extracting embeddings, which are the numerical representations of words or phrases. In this case, it's showing how individual word embeddings are derived from a larger context, such as a sentence or paragraph. This is an essential step in NLP tasks like sentiment analysis, language translation, and text classification, where machine learning algorithms can process and analyze these numerical representations more efficiently than raw text data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Extracting the Embeddings\n",
      "\n",
      "[IDS] The image represents a visual aid from an NLP lecture, specifically focusing on the concept of \"Static_embedding_word2Vec\". It illustrates the process of extracting embeddings from word vectors using the Word2Vec algorithm. The slide shows a bar chart with a highlighted blue section labeled 'W1', which signifies a specific embedding for the word 'word 4'. This is likely a demonstration of how words are represented as vectors in a high-dimensional space and how certain dimensions (or features) of these vectors can be extracted to capture the meaning or context of the word within the language model. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Skip Gram Architecture\n",
      "y=softmax (xW1W 2)\n",
      "Recap Softmax:\n",
      "Softmax turns vector into probability distribution s.t. it sums to 1.\n",
      "softmax(z)j=\n",
      "e\n",
      "z j\n",
      "∑k=1\n",
      "K\n",
      "e\n",
      "zk\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing the Word2Vec algorithm, which is a method for converting words into vectors of real numbers in a high-dimensional space. The slide explains that Word2Vec uses two architectures: Continuous Bag of Words (CBOW) and Skip-Gram. The focus here is on the Skip-Gram architecture, which is used to learn vector representations of words by predicting the missing word in a context window. The slide also mentions the Softmax function and its role in converting the vector into a probability distribution. This is part of the NLP topic \"Static_embedding_word2Vec,\" which likely covers various aspects of how static embeddings are created using Word2Vec. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec – Skip Gram Architecture\n",
      "y=softmax (xW1W 2)\n",
      "Recap Softmax:\n",
      "Softmax turns vector into probability distribution s.t. it sums to 1.\n",
      "Problem: \n",
      "Vector z has vocabulary size (10,000; 100,000; 1,000,000?)\n",
      "→ Computationally expensive!!!\n",
      "softmax(z)j=\n",
      "e\n",
      "z j\n",
      "∑k=1\n",
      "K\n",
      "e\n",
      "zk\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the Word2Vec algorithm. This algorithm is used for creating vector representations of words, which helps in understanding the semantic relationships between them. The slide explains how the softmax function is applied to transform the probability distribution into a vector, which is then used in the skip-gram architecture of Word2Vec. The problem highlighted on the slide is related to the computational cost and memory requirements of using a vocabulary size of 100,000 words or more in this context. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "\n",
      "[IDS] The image is a diagram of the Word2Vec model, which is a popular technique in Natural Language Processing (NLP) for creating word embeddings. In this context, we are discussing static embeddings, meaning that the embeddings are not updated during training and remain fixed. The diagram shows two main components: the input layer and the output layer. The input layer takes in words as vectors, represented by circles with the word and its corresponding index. These words are then processed through a window of size 1, indicating that only one word at a time is being considered. This processed word is then passed through a dot product operation to produce an output vector. The output layer shows two dimensions, W1 and W2, which represent the learned weight matrices for the skip-gram model, a type of Word2Vec architecture that predicts the surrounding words given a target word. The entire process is depicted in a simple flowchart format, typical of NLP lectures to help students understand how word embeddings are created using the Word2Vec algorithm. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "y=σ (x1W 1W 2\n",
      "T x2\n",
      "T)\n",
      "\n",
      "[IDS] The image depicts a visualization of the Word2Vec model's Negative Sampling Skip Gram Model, which is a concept in Natural Language Processing (NLP). This model is used to learn word embeddings by predicting whether two words co-occur in a given context. The diagram illustrates the architecture of this model with input layers for words and their context, as well as output layers that determine if the predicted word and the target word are likely to appear together in a sentence. The lecture titled \"Static_embedding_word2Vec\" likely covers the basics of this model and its applications in NLP tasks such as language translation and text classification. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "Previous approach vs this approach:\n",
      "Time for one forward pass in a notebook:\n",
      "Previous: \n",
      "~150 ms for vocabulary size of 1,000,000!\n",
      "This approach:\n",
      "~30 µs for vocabulary size of 1,000,000!\n",
      "~5000 times faster!\n",
      "\n",
      "[IDS] The image displays a comparison between two approaches to word embeddings, specifically focusing on the Word2Vec model. The first approach is the traditional method which involves one forward pass in a notebook and takes approximately 150 milliseconds for a vocabulary size of 1,000,000. The second approach, referred to as \"Negative Sampling Skip Gram Model,\" significantly reduces the time required, taking around 30 microseconds for the same vocabulary size. This suggests that the Negative Sampling Skip Gram Model is much faster, reducing the time by about 5000 times compared to the traditional method. As we are in an NLP lecture, this information is particularly relevant as it highlights advancements in natural language processing techniques and the importance of efficient computational methods in handling large datasets. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "This is not the full truth!\n",
      "So far we have only created positive examples where:\n",
      "(word1, word2) → 1\n",
      "Without negative examples our classifier could always predict 1 \n",
      "and achieve a 100% accuracy.\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture, specifically focusing on the Word2Vec algorithm and its application in creating negative sampling skip gram models. The content suggests that the current model is not complete as it only contains positive examples (word1, word2) without any negative examples. This lack of negative examples hinders the model's ability to predict accurately, as evidenced by the claim that even with 100% accuracy on positive examples, the classifier would still predict 1 without considering negative examples. The slide seems to be emphasizing the importance of including negative examples in the training data to improve the performance of the Word2Vec model. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "Loss function for a batch of N examples (p is output, t is target label):\n",
      "Single positive example:\n",
      "Single negative example:\n",
      "L=−1\n",
      "N [∑j=1\n",
      "N\n",
      "[t j log( p j)+(1−t j)log(1−p j)]]\n",
      "L=−log( p j)\n",
      "L=−log(1−p j)\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture, specifically discussing the Word2Vec model and its Negative Sampling Skip Gram Model. The slide outlines the loss function used for a batch of N examples where the output is a target label. It explains the concept of a single positive example and a single negative example in the context of this model. The slide also includes logos of the Hochschule Bonn-Rhein-Sieg and Tim Metzler, presumably the instructor or author of the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "How do we create negative examples?\n",
      "For one positive example, do we need to create (|V| - 1) negative \n",
      "examples?\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture, specifically discussing the Word2Vec model and negative sampling skip gram model. It poses the question of how to create negative examples for this model, suggesting that they are essential for training the model. The slide also includes logos indicating the involvement of Hochschule Bonn-Rhein-Sieg and Tim Metzler, likely the presenter or instructor of the lecture. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "How do we create negative examples?\n",
      "For one positive example, do we need to create (|V| - 1) negative \n",
      "examples?\n",
      "No! Experiments show 5-20 negative examples per positive example are \n",
      "enough.\n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture, specifically focusing on the Word2Vec model and its application in creating negative examples for training. The title \"Word2Vec - Negative Sampling Skip Gram Model\" suggests that the slide is discussing a method for generating negative examples using Word2Vec, which is a word embedding algorithm used to convert words or phrases into vectors of real numbers that capture their semantic meaning. The content of the slide indicates that the audience is being asked to think about how they would create negative examples, with a specific example given (IV - 1 negative examples) and a statement suggesting that experiments show a range of negative examples per positive example are necessary. This implies that understanding the concept of negative sampling is crucial for fine-tuning the performance of the Word2Vec model. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "How do we create negative examples?\n",
      "We sample according to frequency of word.\n",
      "\n",
      "[IDS] The image displays a graph titled \"Word2Vec - Negative Sampling Skip Gram Model,\" which is a technique used in natural language processing (NLP) to learn word embeddings. The graph illustrates how negative examples are created and sampled according to the frequency of words. This is a fundamental concept in NLP, particularly in the context of static embeddings like Word2Vec, where the goal is to represent words as vectors in a high-dimensional space such that semantically similar words are close together. The graph's x-axis represents the token index, and the y-axis shows the frequency of words. The lecture is likely discussing how this model helps in understanding the relationships between words and improving the performance of NLP tasks such as language translation or sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "How do we create negative examples?\n",
      "We sample according to frequency of word.\n",
      "\n",
      "[IDS] The image displays a visualization of negative sampling skip gram model used in the Word2Vec algorithm, which is a popular technique for creating word embeddings in Natural Language Processing (NLP). This model aims to capture the semantic relationships between words by predicting the surrounding words in a given context. The graph shows the frequency distribution of tokens, where the x-axis represents the token index and the y-axis represents the frequency(index) of the tokens. The two lines represent the actual frequency distribution and the negative sampling distribution, indicating how the model adjusts the frequency of words to better capture their relationships. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Word2Vec - Negative Sampling Skip Gram Model\n",
      "How do we create negative examples?\n",
      "We sample according to frequency of word.\n",
      "f\n",
      "3\n",
      "4\n",
      "\n",
      "[IDS] The image is a visual representation of how negative examples are used to create a skip gram model in the Word2Vec algorithm, which is a technique used in Natural Language Processing (NLP) to improve word embeddings. The left graph shows the frequency distribution of words, where higher frequencies indicate more common words. The right graph demonstrates the negative sampling probability, which is a method of creating synthetic negative examples by sampling from a noise distribution, like a normal distribution with mean 0 and standard deviation 1. This process helps the model learn more about the context of words and their relationships. In NLP, such models are crucial for tasks like language translation, sentiment analysis, and text classification. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the main difference between the traditional approach to word embeddings and the Negative Sampling Skip Gram Model in terms of computational time? \n",
      "In the context of Word2Vec, why is it crucial for the model to incorporate negative examples along with positive examples during training?\n",
      "How does the frequency-based negative sampling method help improve the Word2Vec model's understanding of word relationships and context within a text corpus?\n",
      "Question: In the context of Word2Vec and negative sampling, what is one primary reason for creating synthetic negative examples during training?\n",
      "\n",
      "A) To outweigh the number of positive examples in a dataset\n",
      "\n",
      "B) To ensure that the model isn't \"tricked\" by only learning from positive examples \n",
      "\n",
      "C) To make the most of sparse data where there are limited negative examples available\n",
      "\n",
      "D) To artificially boost the performance of the classifier on test data[IDE]\n",
      "### Question:\n",
      "\n",
      "In the given lecture content, there's a function called `bagOfWords` that is used to create document embeddings. It takes in a model and a document as input and returns the document embedding by summing up word vectors. Can you explain why taking the mean of the word vectors would not work here?\n",
      "lecture_content: What do words mean?\n",
      "N-gram or text classification methods we've seen so far\n",
      "◦Words are just strings (or indices wi in a vocabulary list)\n",
      "◦That's not very satisfactory!\n",
      "Introductory logic classes:\n",
      "◦The meaning of \"dog\" is DOG;  cat is CAT\n",
      "∀x DOG(x) ⟶MAMMAL(x)\n",
      "Old linguistics joke by Barbara Partee in 1967:\n",
      "◦Q: What's the meaning of life?\n",
      "◦A: LIFE\n",
      "\n",
      "\n",
      "Desiderata\n",
      "What should a theory of word meaning do for us?\n",
      "Let's look at some desiderata\n",
      "From lexical semantics, the linguistic study of word \n",
      "meaning\n",
      "\n",
      "\n",
      "mouse (N)\n",
      "1. any of numerous small rodents...\n",
      "2. a hand-operated device that controls \n",
      "a cursor... \n",
      "Lemmas and senses\n",
      "sense\n",
      "lemma\n",
      "Modified from the online thesaurus WordNet\n",
      "\n",
      "\n",
      "Relations between senses: Synonymy\n",
      "Synonyms have the same meaning in some or all \n",
      "contexts.\n",
      "◦filbert / hazelnut\n",
      "◦couch / sofa\n",
      "◦big / large\n",
      "◦automobile / car\n",
      "◦vomit / throw up\n",
      "◦water / H20\n",
      "\n",
      "\n",
      "Relations between senses: Synonymy\n",
      "Note that there are probably no examples of perfect \n",
      "synonymy.\n",
      "◦Even if many aspects of meaning are identical\n",
      "◦Still may differ based on politeness, slang, register, genre, \n",
      "etc.\n",
      "\n",
      "\n",
      "Relation: Synonymy?\n",
      "water/H20\n",
      "\"H20\" in a surfing guide?\n",
      "big/large\n",
      "my big sister != my large sister\n",
      "\n",
      "\n",
      "The Linguistic Principle of Contrast\n",
      "Difference in form à difference in meaning\n",
      "\n",
      "\n",
      "Abbé Gabriel Girard 1718\n",
      " [I do not believe that there \n",
      "is a synonymous word in any \n",
      "language]\n",
      "\"\n",
      "\"\n",
      "Re: \"exact\" synonyms\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image serves as an illustration of how language can be represented and analyzed. The text from the 1718 book by Abbé Gabriel Girard is an example of the early stages of linguistic research, focusing on the precise meanings of words in different languages. This historical perspective is crucial for understanding how NLP has evolved over time. In modern NLP, techniques like static embedding and vector semantics play a significant role in capturing the essence of words and their relationships within a language. These methods involve representing words as vectors in a high-dimensional space where semantically similar words are closer together. This enables NLP systems to understand word meanings and use them effectively in tasks such as language translation, sentiment analysis, and text generation. The image of the old book serves as a reminder of the rich history and foundational work that has led to the development of these advanced NLP techniques. [IDE]\n",
      "\n",
      "\n",
      "Relation: Similarity\n",
      "Words with similar meanings.  Not synonyms, but sharing \n",
      "some element of meaning\n",
      "car, bicycle\n",
      "cow, horse\n",
      "\n",
      "\n",
      "Ask humans how similar 2 words are\n",
      "word1\n",
      "word2\n",
      "similarity\n",
      "vanish\n",
      "disappear\n",
      "9.8 \n",
      "behave\n",
      "obey\n",
      "7.3 \n",
      "belief\n",
      "impression \n",
      "5.95 \n",
      "muscle\n",
      "bone \n",
      "3.65 \n",
      "modest\n",
      "flexible\n",
      "0.98 \n",
      "hole\n",
      "agreement\n",
      "0.3 \n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>word1</th>\n",
      "      <th>word2</th>\n",
      "      <th>similarity</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>vanish</td>\n",
      "      <td>disappear</td>\n",
      "      <td>9.8</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>behave</td>\n",
      "      <td>obey</td>\n",
      "      <td>7.3</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>belief</td>\n",
      "      <td>impression</td>\n",
      "      <td>5.95</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>muscle</td>\n",
      "      <td>bone</td>\n",
      "      <td>3.65</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>modest</td>\n",
      "      <td>flexible</td>\n",
      "      <td>0.98</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>hole</td>\n",
      "      <td>agreement</td>\n",
      "      <td>0.3</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Relation: Word relatedness\n",
      "Also called \"word association\"\n",
      "Words can be related in any way, perhaps via a semantic \n",
      "frame or field\n",
      "◦coffee, tea:    similar\n",
      "◦coffee, cup:   related, not similar\n",
      "\n",
      "\n",
      "Semantic field\n",
      "Words that \n",
      "◦cover a particular semantic domain \n",
      "◦bear structured relations with each other. \n",
      "hospitals\n",
      "surgeon, scalpel, nurse, anaesthetic, hospital\n",
      "restaurants\n",
      "waiter, menu, plate, food, menu, chef\n",
      "houses\n",
      "\n",
      "\n",
      "Relation: Antonymy\n",
      "Senses that are opposites with respect to only one \n",
      "feature of meaning\n",
      "Otherwise, they are very similar!\n",
      "dark/light   short/long fast/slow\n",
      "rise/fall\n",
      "hot/cold\n",
      "up/down\n",
      "in/out\n",
      "More formally: antonyms can\n",
      "◦define a binary opposition or be at opposite ends of a scale\n",
      "◦\n",
      "long/short, fast/slow\n",
      "\n",
      "\n",
      "Connotation (sentiment)\n",
      "• Words have affective meanings\n",
      "•\n",
      "Positive connotations (happy) \n",
      "•\n",
      "Negative connotations (sad)\n",
      "• Connotations can be subtle:\n",
      "•\n",
      "Positive connotation: copy, replica, reproduction \n",
      "•\n",
      "Negative connotation: fake, knockoff, forgery\n",
      "• Evaluation (sentiment!)\n",
      "\n",
      "\n",
      "Connotation\n",
      "Words seem to vary along 3 affective dimensions:\n",
      "◦valence: the pleasantness of the stimulus\n",
      "◦arousal: the intensity of emotion provoked by the stimulus\n",
      "◦dominance: the degree of control exerted by the stimulus\n",
      "Osgood et al. (1957)\n",
      "Word\n",
      "Score\n",
      "Word\n",
      "Score\n",
      "Valence\n",
      "love\n",
      "1.000\n",
      "toxic\n",
      "0.008\n",
      "happy\n",
      "1.000\n",
      "nightmare\n",
      "0.005\n",
      "Arousal\n",
      "elated\n",
      "0.960\n",
      "mellow\n",
      "0.069\n",
      "frenzy\n",
      "0.965\n",
      "napping\n",
      "0.046\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>1-Word</th>\n",
      "      <th>2-Score</th>\n",
      "      <th>Col3</th>\n",
      "      <th>4-Word</th>\n",
      "      <th>5-Score</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Valence</td>\n",
      "      <td>love</td>\n",
      "      <td>1.000</td>\n",
      "      <td></td>\n",
      "      <td>toxic</td>\n",
      "      <td>0.008</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>happy</td>\n",
      "      <td>1.000</td>\n",
      "      <td></td>\n",
      "      <td>nightmare</td>\n",
      "      <td>0.005</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>Arousal</td>\n",
      "      <td>elated</td>\n",
      "      <td>0.960</td>\n",
      "      <td></td>\n",
      "      <td>mellow</td>\n",
      "      <td>0.069</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>None</td>\n",
      "      <td>frenzy</td>\n",
      "      <td>0.965</td>\n",
      "      <td></td>\n",
      "      <td>napping</td>\n",
      "      <td>0.046</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>Dominance</td>\n",
      "      <td>powerful</td>\n",
      "      <td>0.991</td>\n",
      "      <td></td>\n",
      "      <td>weak</td>\n",
      "      <td>0.045</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>None</td>\n",
      "      <td>leadership</td>\n",
      "      <td>0.983</td>\n",
      "      <td></td>\n",
      "      <td>empty</td>\n",
      "      <td>0.081</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "So far\n",
      "Concepts or word senses\n",
      "◦Have a complex many-to-many association with words (homonymy, \n",
      "multiple senses)\n",
      "Have relations with each other\n",
      "◦Synonymy\n",
      "◦Antonymy\n",
      "◦Similarity\n",
      "◦Relatedness\n",
      "◦Connotation\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Word Meaning\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Vector Semantics\n",
      "\n",
      "\n",
      "Computational models of word meaning\n",
      "Can we build a theory of how to represent word \n",
      "meaning, that accounts for at least some of the \n",
      "desiderata?\n",
      "We'll introduce vector semantics\n",
      "The standard model in language processing!\n",
      "Handles many of our goals!\n",
      "\n",
      "\n",
      "Ludwig Wittgenstein\n",
      "PI #43: \n",
      "\"The meaning of a word is its use in the language\"\n",
      "\n",
      "\n",
      "Let's define words by their usages\n",
      "One way to define \"usage\": \n",
      "words are defined by their environments (the words around them)\n",
      "Zellig Harris (1954): \n",
      "If A and B have almost identical environments we say that they \n",
      "are synonyms.\n",
      "\n",
      "\n",
      "What does recent English borrowing ongchoi mean?\n",
      "Suppose you see these sentences:\n",
      "• Ong choi is delicious sautéed with garlic. \n",
      "• Ong choi is superb over rice\n",
      "• Ong choi leaves with salty sauces\n",
      "And you've also seen these:\n",
      "• …spinach sautéed with garlic over rice\n",
      "• Chard stems and leaves are delicious\n",
      "• Collard greens and other salty leafy greens\n",
      "Conclusion:\n",
      "\n",
      "\n",
      "Ongchoi: Ipomoea aquatica \"Water Spinach\"\n",
      "!\"#\n",
      "kangkong\n",
      "rau muống\n",
      "…\n",
      "\n",
      "[IDS] The image you provided is related to a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Static_embedding_Vector_semeantic_embeddings.\" The text in the image seems to be discussing a concept or term from this field. NLP is a subfield of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. In the context of static embeddings, it refers to a technique where words or phrases are represented as vectors in a high-dimensional space, capturing their semantic meaning.\n",
      "\n",
      "The term \"Vector\" in this context likely refers to mathematical vectors used to represent words or phrases in a way that captures their meaning and relationships. These vectors can be used for tasks such as text classification, sentiment analysis, and machine translation.\n",
      "\n",
      "The image itself serves as a visual representation of the concept being discussed in the lecture. It's a photograph of a bunch of green leaves, which could symbolize growth, nature, or freshness – concepts often associated with NLP's goal of understanding and generating human language in a meaningful way.\n",
      "\n",
      "As a helpful assistant, I would explain that static embeddings are an essential part of NLP because they allow machines to understand the meaning behind words and phrases by mapping them to numerical vectors. This process enables various NLP tasks, including text classification, where the machine can determine the category of a piece of text based on its content.\n",
      "\n",
      "In summary, the image is a visual aid for a lecture on NLP, specifically discussing static embeddings and vector-based semantic embeddings. These concepts are crucial for teaching machines to understand and interpret human language effectively. [IDE]\n",
      "\n",
      "\n",
      "Idea 1: Defining meaning by linguistic distribution\n",
      "Let's define the meaning of a word by its \n",
      "distribution in language use, meaning its \n",
      "neighboring words or grammatical environments. \n",
      "\n",
      "\n",
      "Idea 2: Meaning as a point in space (Osgood et al. 1957)\n",
      "3 affective dimensions for a word\n",
      "◦valence: pleasantness \n",
      "◦arousal: intensity of emotion \n",
      "◦dominance: the degree of control exerted\n",
      "Word\n",
      "Score\n",
      "Word\n",
      "Score\n",
      "Valence\n",
      "love\n",
      "1.000\n",
      "toxic\n",
      "0.008\n",
      "happy\n",
      "1.000\n",
      "nightmare\n",
      "0.005\n",
      "Arousal\n",
      "elated\n",
      "0.960\n",
      "mellow\n",
      "0.069\n",
      "frenzy\n",
      "0.965\n",
      "napping\n",
      "0.046\n",
      "Dominance\n",
      "powerful\n",
      "0.991\n",
      "weak\n",
      "0.045\n",
      "NRC VAD Lexicon \n",
      " (Mohammad 2018)\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>1-Word</th>\n",
      "      <th>2-Score</th>\n",
      "      <th>Col3</th>\n",
      "      <th>4-Word</th>\n",
      "      <th>5-Score</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Valence</td>\n",
      "      <td>love</td>\n",
      "      <td>1.000</td>\n",
      "      <td></td>\n",
      "      <td>toxic</td>\n",
      "      <td>0.008</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>happy</td>\n",
      "      <td>1.000</td>\n",
      "      <td></td>\n",
      "      <td>nightmare</td>\n",
      "      <td>0.005</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>Arousal</td>\n",
      "      <td>elated</td>\n",
      "      <td>0.960</td>\n",
      "      <td></td>\n",
      "      <td>mellow</td>\n",
      "      <td>0.069</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>None</td>\n",
      "      <td>frenzy</td>\n",
      "      <td>0.965</td>\n",
      "      <td></td>\n",
      "      <td>napping</td>\n",
      "      <td>0.046</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>Dominance\\n◦</td>\n",
      "      <td>powerful</td>\n",
      "      <td>0.991</td>\n",
      "      <td></td>\n",
      "      <td>weak</td>\n",
      "      <td>0.045</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>None</td>\n",
      "      <td>leadership</td>\n",
      "      <td>0.983</td>\n",
      "      <td></td>\n",
      "      <td>empty</td>\n",
      "      <td>0.081</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Idea 1: Defining meaning by linguistic distribution\n",
      "Idea 2: Meaning as a point in multidimensional space\n",
      "\n",
      "\n",
      "ER 6\n",
      "•\n",
      "VECTOR SEMANTICS AND EMBEDDINGS\n",
      "bad\n",
      "worst\n",
      "not good\n",
      "dislike\n",
      "worse\n",
      "incredibly bad\n",
      "now\n",
      "you\n",
      "i\n",
      "that\n",
      "with\n",
      "by\n",
      "to\n",
      "’s\n",
      "are\n",
      "is\n",
      "a\n",
      "than\n",
      "Defining meaning as a point in space based on distribution\n",
      "Each word = a vector   (not just \"good\" or \"w45\")\n",
      "Similar words are \"nearby in semantic space\"\n",
      "We build this space automatically by seeing which words are \n",
      "nearby in text\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>not good\n",
      "bad\n",
      "to by dislike\n",
      "worst\n",
      "’s\n",
      "incredibly bad\n",
      "that now\n",
      "are worse\n",
      "a i\n",
      "you\n",
      "than\n",
      "with\n",
      "is\n",
      "incredibly good\n",
      "very good\n",
      "amazing fantastic\n",
      "wonderful\n",
      "terrific\n",
      "nice\n",
      "good</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "We define meaning of a word as a vector\n",
      "Called an \"embedding\" because it's embedded into a \n",
      "space (see textbook)\n",
      "The standard way to represent meaning in NLP\n",
      "Every modern NLP algorithm uses embeddings as \n",
      "the representation of word meaning\n",
      "Fine-grained model of meaning for similarity \n",
      "\n",
      "\n",
      "Intuition: why vectors?\n",
      "Consider sentiment analysis:\n",
      "◦With words,  a feature is a word identity\n",
      "◦Feature 5: 'The previous word was \"terrible\"'\n",
      "◦requires exact same word to be in training and test\n",
      "◦With embeddings: \n",
      "◦Feature is a word vector\n",
      "◦'The previous word was vector [35,22,17…]\n",
      "◦Now in the test set we might see a similar vector [34,21,14]\n",
      "\n",
      "\n",
      "We'll discuss 2 kinds of embeddings\n",
      "tf-idf\n",
      "◦Information Retrieval workhorse!\n",
      "◦A common baseline model\n",
      "◦Sparse vectors\n",
      "◦Words are represented by (a simple function of) the counts of nearby \n",
      "words\n",
      "Word2vec\n",
      "◦Dense vectors\n",
      "◦Representation is created by training a classifier to predict whether a \n",
      "\n",
      "\n",
      "From now on:\n",
      "Computing with meaning representations\n",
      "instead of string representations\n",
      "R\n",
      "Vector Semantics and\n",
      "Embeddings\n",
      "C⇧@Â(|，ó|而ÿC Nets are for fish;\n",
      "Once you get the fish, you can forget the net.\n",
      "言⇧@Â(✏，ó✏而ÿ言Words are for meaning;\n",
      "Once you get the meaning, you can forget the words\n",
      "ÑP(Zhuangzi), Chapter 26\n",
      "\n",
      "[IDS] The image is likely a slide from a lecture on Natural Language Processing (NLP). It discusses the transition from computing with meaning representations to using string representations, emphasizing the importance of understanding both the words and their meanings. The slide also touches upon the distinction between Nets for fish, which are easily forgotten, and Words for meaning, which can be more persistent in memory. This suggests that the lecture is focused on how semantic embeddings, like static and embedded vectors, play a crucial role in capturing and retaining the meaning of words in computational models. [IDE]\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Vector Semantics\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Words and Vectors\n",
      "\n",
      "\n",
      "Term-document matrix\n",
      "6.3\n",
      "•\n",
      "WORDS AND VECTORS\n",
      "7\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.2\n",
      "The term-document matrix for four words in four Shakespeare plays. Each cell\n",
      "contains the number of times the (row) word occurs in the (column) document.\n",
      "just so they fit on the page; in real term-document matrices, the vectors representing\n",
      "each document would have dimensionality |V|, the vocabulary size.\n",
      "The ordering of the numbers in a vector space indicates different meaningful di-\n",
      "mensions on which documents vary. Thus the first dimension for both these vectors\n",
      "corresponds to the number of times the word battle occurs, and we can compare\n",
      "each dimension, noting for example that the vectors for As You Like It and Twelfth\n",
      "Night have similar values (1 and 0, respectively) for the first dimension.\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.3\n",
      "The term-document matrix for four words in four Shakespeare plays. The red\n",
      "boxes show that each document is represented as a column vector of length four.\n",
      "Each document is represented by a vector of words\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV\\nbbaattttllee 11 00 77 1133\\nggoooodd 111144 8800 6622 8899\\nffooooll 3366 5588 11 44\\nwwiitt 2200 1155 22 33\\nFFiigguurree 66..32 TThhee tteerrmm--ddooccuummeenntt mmaattrriixx ffoorr ffoouurr wwoorrddss iinn ffoouurr S Shhaakkeessppeeaarree p plalayyss. .E Tachh\\nbcooxnetasi nshs otwhe tnhuatm ebaecrh o dfo tcimumese ntht ei s( rroewpr)e wseonrtde do cacsu ar sc oinlu tmhen (vceoclutomr no)f dleoncgutmh efnotu.r.\\nreprWesee nctaend tahsi nak c oofu ntht ev evcetcotro, ra f coor lau mdonc iunm Feingt. a6s.3 a. point in V -dimensional sp\\n| |\\nthusT toh er edvoiceuwm seonmtse i nb Fasigic. 6li.n3e aarre a plogienbtrsa i,n a 4 v-deicmtoern sisio, naat lh sepaarct,e .j uSsitn ac el i4s-t doirm aernrsa\\nsnpuamcebse rasr.e Shoa rAds t oY ovuis Luiakleiz Iet, iFsi rge. p6r.e4s eshnotewds a as vthiseu lailsitz [a1ti,o1n1 4in,3 t6w,2o0 d] i(mtheen sfiirosnt sc;o wl</td>\n",
      "      <td>AAss YYoouu LLiikkee IItt TTwweellfftthh NNiigghhtt JJuulliiuuss CCaaeessaarr HHeennrryy VV</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>bbaattttllee 11 00 77 1133\\nggoooodd 111144 8800 6622 8899\\nffooooll 3366 5588 11 44\\nwwiitt 2200 1155 22 33</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>None</td>\n",
      "      <td>FFiigguurree 66..32</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Visualizing document vectors\n",
      "thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensio\n",
      "spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’v\n",
      "arbitrarily chosen the dimensions corresponding to the words battle and fool.\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "5\n",
      "10\n",
      "Henry V [4,13]\n",
      "As You Like It [36,1]\n",
      "Julius Caesar [1,7]\n",
      "battle\n",
      "Twelfth Night [58,0]\n",
      "15\n",
      "40\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "\n",
      "\n",
      "Vectors are the basis of information retrieval\n",
      "mensions on which documents vary. Thus the first dimension for both these vectors\n",
      "corresponds to the number of times the word battle occurs, and we can compare\n",
      "each dimension, noting for example that the vectors for As You Like It and Twelfth\n",
      "Night have similar values (1 and 0, respectively) for the first dimension.\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.3\n",
      "The term-document matrix for four words in four Shakespeare plays. The red\n",
      "boxes show that each document is represented as a column vector of length four.\n",
      "We can think of the vector for a document as a point in |V|-dimensional space;\n",
      "thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional\n",
      "spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’ve\n",
      "Vectors are similar for the two comedies\n",
      "But comedies are different than the other two \n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>As You Like It Twelfth Night Julius Caesar Henry V</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>As You Like It Twelfth Night Julius Caesar Henry V\\nbattle 1 0 7 13\\ngood 114 80 62 89\\nfool 36 58 1 4\\nwit 20 15 2 3\\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. Th\\nboxes show that each document is represented as a column vector of length four.\\nVectors are similar for the two comedies\\nWe can think of the vector for a document as a point in V -dimensional sp\\n| |\\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimens\\nBut comedies are different than the other two\\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; w\\narbitrarily chosen the dimensions corresponding to the words battle and fool.\\nComedies have more fools and wit and fewer battles.\\n40</td>\n",
      "      <td>As You Like It Twelfth Night Julius Caesar Henry V</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>battle 1 0 7 13\\ngood 114 80 62 89\\nfool 36 58 1 4\\nwit 20 15 2 3</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>None</td>\n",
      "      <td>Figure 6.3</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>None</td>\n",
      "      <td>40</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Idea for word meaning: Words can be vectors too!!!\n",
      "6.3\n",
      "•\n",
      "WORDS AND VECTORS\n",
      "7\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.2\n",
      "The term-document matrix for four words in four Shakespeare plays. Each cell\n",
      "contains the number of times the (row) word occurs in the (column) document.\n",
      "represented as a count vector, a column in Fig. 6.3.\n",
      "To review some basic linear algebra, a vector is, at heart, just a list or array of\n",
      "tor\n",
      "battle is \"the kind of word that occurs in Julius Caesar and Henry V\"\n",
      "fool is \"the kind of word that occurs  in comedies, especially Twelfth Night\"\n",
      " row vector\n",
      "vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions\n",
      "of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word\n",
      "counts in the same four dimensions are used to form the vectors for the other 3\n",
      "words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89].\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.5\n",
      "The term-document matrix for four words in four Shakespeare plays. The red\n",
      "boxes show that each word is represented as a row vector of length four.\n",
      "For documents, we saw that similar documents had similar vectors, because sim-\n",
      "ilar documents tend to have similar words. This same principle applies to words:\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>As You Like It Twelfth Night Julius Caesar Henry V\n",
      "As You Like It Twelfth Night Julius Caesar Henry V</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td></td>\n",
      "      <td>bbaattttllee 11 00 77 1133\\nggoooodd 111144 8800 6622 8899\\nffooooll 3366 5588 11 44\\nwwiitt 2200 1155 22 33</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td></td>\n",
      "      <td>FFiigguurree 66..52</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "More common: word-word matrix\n",
      "(or \"term-context matrix\")\n",
      "Two words are similar in meaning if their context vectors are similar\n",
      " ||⇥||\n",
      "times the row (target) word and the column (context) word co-occur in some context\n",
      "in some training corpus. The context could be the document, in which case the cell\n",
      "represents the number of times the two words appear in the same document. It is\n",
      "most common, however, to use smaller contexts, generally a window around the\n",
      "word, for example of 4 words to the left and 4 words to the right, in which case\n",
      "the cell represents the number of times (in some training corpus) the column word\n",
      "occurs in such a ±4 word window around the row word. For example here is one\n",
      "example each of some words in their windows:\n",
      "is traditionally followed by cherry\n",
      "pie, a traditional dessert\n",
      "often mixed, such as strawberry\n",
      "rhubarb pie. Apple pie\n",
      "computer peripherals and personal digital\n",
      "assistants. These devices usually\n",
      "a computer. This includes information available on the internet\n",
      "If we then take every occurrence of each word (say strawberry) and count the con-\n",
      "text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n",
      "simplified subset of the word-word co-occurrence matrix for these four words com-\n",
      "dow around the word, for example of 4 words to the left and 4 words to the right,\n",
      "in which case the cell represents the number of times (in some training corpus) the\n",
      "column word occurs in such a ±4 word window around the row word. For example\n",
      "here is one example each of some words in their windows:\n",
      "is traditionally followed by cherry\n",
      "pie, a traditional dessert\n",
      "often mixed, such as strawberry\n",
      "rhubarb pie. Apple pie\n",
      "computer peripherals and personal digital\n",
      "assistants. These devices usually\n",
      "a computer. This includes information available on the internet\n",
      "If we then take every occurrence of each word (say strawberry) and count the\n",
      "context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\n",
      "simplified subset of the word-word co-occurrence matrix for these four words com-\n",
      "puted from the Wikipedia corpus (Davies, 2015).\n",
      "aardvark\n",
      "...\n",
      "computer\n",
      "data\n",
      "result\n",
      "pie\n",
      "sugar\n",
      "...\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows\n",
      "aardvark ... computer data result pie sugar ...</th>\n",
      "      <th>Col1</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>simplified subset of the word-word co-occurrence matrix for these four words co\\ncherry 0 ... 2 8 9 442 25 ...\\nputed from the Wikipedia corpus (Davies, 2015).\\nstrawberry 0 ... 0 0 1 60 19 ...\\nNote in Fig. 6.5 that the two words cherry and strawberry are more similar\\ndigital 0 ... 1670 1683 85 5 4 ...\\neach other (both pie and sugar tend to occur in their window) than they are to oth\\ninformation 0 ... 3325 3982 378 5 13 ...</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>ig. 6.5 that the two words cherry and strawberry are more similar\\n0 ... 1670 1683 85 5 4 ...\\noth pie and sugar tend to occur in their window) than they are to oth\\n0 ... 3325 3982 378 5 13 ...</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>wor</td>\n",
      "      <td>ds like di</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "1000 2000 3000 4000\n",
      "1000\n",
      "2000\n",
      "digital\n",
      " [1683,1670]\n",
      "computer\n",
      "information\n",
      " [3982,3325] \n",
      "3000\n",
      "4000\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Words and Vectors\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Cosine for computing word similarity\n",
      "\n",
      "\n",
      "Computing word similarity: Dot product and cosine\n",
      "The dot product between two vectors is a scalar:\n",
      "The dot product tends to be high when the two \n",
      "vectors have large values in the same dimensions\n",
      "Dot product can thus be a useful similarity metric \n",
      "between vectors\n",
      "hence of length |V|, or both with documents as dimensions as documents, of length\n",
      "|D|) and gives a measure of their similarity. By far the most common similarity\n",
      "metric is the cosine of the angle between the vectors.\n",
      "The cosine—like most measures for vector similarity used in NLP—is based on\n",
      "the dot product operator from linear algebra, also called the inner product:\n",
      "dot product(v,w) = v·w =\n",
      "N\n",
      "X\n",
      "i=1\n",
      "viwi = v1w1 +v2w2 +...+vNwN\n",
      "(6.7)\n",
      "As we will see, most metrics for similarity between vectors are based on the dot\n",
      "product. The dot product acts as a similarity metric because it will tend to be high\n",
      "just when the two vectors have large values in the same dimensions. Alternatively,\n",
      "vectors that have zeros in different dimensions—orthogonal vectors—will have a\n",
      "\n",
      "\n",
      "Problem with raw dot-product\n",
      "Dot product favors long vectors\n",
      "Dot product is higher if a vector is longer (has higher \n",
      "values in many dimension)\n",
      "Vector length:\n",
      "Frequent words (of, the, you) have long vectors (since \n",
      " will see, most metrics for similarity between vectors are based on the dot\n",
      "ct. The dot product acts as a similarity metric because it will tend to be high\n",
      "hen the two vectors have large values in the same dimensions. Alternatively,\n",
      "s that have zeros in different dimensions—orthogonal vectors—will have a\n",
      "oduct of 0, representing their strong dissimilarity.\n",
      "is raw dot product, however, has a problem as a similarity metric: it favors\n",
      "ectors. The vector length is defined as\n",
      "|v| =\n",
      "v\n",
      "u\n",
      "u\n",
      "t\n",
      "N\n",
      "X\n",
      "i=1\n",
      "v2\n",
      "i\n",
      "(6.8)\n",
      "ot product is higher if a vector is longer, with higher values in each dimension.\n",
      "\n",
      "\n",
      "Alternative: cosine for computing word similarity\n",
      "|~\n",
      "a||\n",
      "b|\n",
      "The cosine similarity metric between two vectors~\n",
      "v and ~\n",
      "w thus can be computed\n",
      "s:\n",
      "cosine(~\n",
      "v,~\n",
      "w) = ~\n",
      "v·~\n",
      "w\n",
      "|~\n",
      "v||~\n",
      "w| =\n",
      "N\n",
      "X\n",
      "i=1\n",
      "viwi\n",
      "v\n",
      "u\n",
      "u\n",
      "t\n",
      "N\n",
      "X\n",
      "i=1\n",
      "v2\n",
      "i\n",
      "v\n",
      "u\n",
      "u\n",
      "t\n",
      "N\n",
      "X\n",
      "i=1\n",
      "w2\n",
      "i\n",
      "(6.10)\n",
      "For some applications we pre-normalize each vector, by dividing it by its length,\n",
      "reating a unit vector of length 1. Thus we could compute a unit vector from ~\n",
      "a by\n",
      "ividing it by |~\n",
      "a|. For unit vectors, the dot product is the same as the cosine.\n",
      "This raw dot product, however, has a problem as a similarity metric: it favors\n",
      "long vectors. The vector length is defined as\n",
      "ector length\n",
      "|v| =\n",
      "v\n",
      "u\n",
      "u\n",
      "t\n",
      "N\n",
      "X\n",
      "i=1\n",
      "v2\n",
      "i\n",
      "(6.8)\n",
      "The dot product is higher if a vector is longer, with higher values in each dimension.\n",
      "More frequent words have longer vectors, since they tend to co-occur with more\n",
      "words and have higher co-occurrence values with each of them. The raw dot product\n",
      "thus will be higher for frequent words. But this is a problem; we’d like a similarity\n",
      "metric that tells us how similar two words are regardless of their frequency.\n",
      "We modify the dot product to normalize for the vector length by dividing the\n",
      "dot product by the lengths of each of the two vectors. This normalized dot product\n",
      "turns out to be the same as the cosine of the angle between the two vectors, following\n",
      "from the definition of the dot product between two vectors a and b:\n",
      "Based on the definition of the dot product between two vectors a and b \n",
      "\n",
      "\n",
      "Cosine as a similarity metric\n",
      "-1: vectors point in opposite directions \n",
      "+1:  vectors point in same directions\n",
      "0: vectors are orthogonal\n",
      "But since raw frequency values are non-negative, the \n",
      "\n",
      "[IDS] The image is a slide from an NLP lecture that focuses on the concept of cosine as a similarity metric. It explains how cosine can be used to measure the similarity between vectors in a high-dimensional space. The slide provides two key points: 1) vectors pointing in opposite directions have a negative cosine value, and 2) vectors pointing in the same direction have a positive cosine value. It also mentions that since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0 to 1. This information is crucial for understanding how cosine can be applied in natural language processing tasks such as text classification or document clustering, where measuring the similarity between documents or terms is essential. [IDE]\n",
      "\n",
      "\n",
      "Cosine examples\n",
      "pie\n",
      "data\n",
      "computer\n",
      "cherry\n",
      "442\n",
      "8\n",
      "2\n",
      "digital\n",
      "5\n",
      "1683\n",
      "1670\n",
      "information 5\n",
      "3982\n",
      "3325\n",
      "cos(\n",
      "v, \n",
      "w) =\n",
      "\n",
      "v • \n",
      "w\n",
      "\n",
      "v \n",
      "w =\n",
      "\n",
      "v\n",
      "\n",
      "v •\n",
      "\n",
      "w\n",
      "\n",
      "w =\n",
      "viwi\n",
      "i=1\n",
      "N\n",
      "∑\n",
      "vi\n",
      "2\n",
      "i=1\n",
      "N\n",
      "∑\n",
      "wi\n",
      "2\n",
      "i=1\n",
      "N\n",
      "∑\n",
      "0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction\n",
      "But raw frequency values are non-negative, so the cosine for these vectors rang\n",
      "from 0–1.\n",
      "Let’s see how the cosine computes which of the words cherry or digital is clos\n",
      "in meaning to information, just using raw counts from the following shortened tabl\n",
      "pie\n",
      "data computer\n",
      "cherry\n",
      "442\n",
      "8\n",
      "2\n",
      "digital\n",
      "5\n",
      "1683\n",
      "1670\n",
      "information\n",
      "5\n",
      "3982\n",
      "3325\n",
      "cos(cherry,information) =\n",
      "442⇤5+8⇤3982+2⇤3325\n",
      "p\n",
      "4422 +82 +22p\n",
      "52 +39822 +33252 = .017\n",
      "cos(digital,information) =\n",
      "5⇤5+1683⇤3982+1670⇤3325\n",
      "p\n",
      "52 16832 16702p\n",
      "52 39822 33252 = .996\n",
      "from 0–1.\n",
      "Let’s see how the cosine computes which of the words cherry or digital is c\n",
      "in meaning to information, just using raw counts from the following shortened t\n",
      "pie\n",
      "data computer\n",
      "cherry\n",
      "442\n",
      "8\n",
      "2\n",
      "digital\n",
      "5\n",
      "1683\n",
      "1670\n",
      "information\n",
      "5\n",
      "3982\n",
      "3325\n",
      "cos(cherry,information) =\n",
      "442⇤5+8⇤3982+2⇤3325\n",
      "p\n",
      "4422 +82 +22p\n",
      "52 +39822 +33252 = .017\n",
      "cos(digital,information) =\n",
      "5⇤5+1683⇤3982+1670⇤3325\n",
      "p\n",
      "52 +16832 +16702p\n",
      "52 +39822 +33252 = .\n",
      "The model decides that information is way closer to digital than it is to cher\n",
      "t raw frequency values are non-negative, so the cosine for these vectors ranges\n",
      "m 0–1.\n",
      "Let’s see how the cosine computes which of the words cherry or digital is closer\n",
      "meaning to information, just using raw counts from the following shortened table:\n",
      "pie\n",
      "data computer\n",
      "cherry\n",
      "442\n",
      "8\n",
      "2\n",
      "digital\n",
      "5\n",
      "1683\n",
      "1670\n",
      "information\n",
      "5\n",
      "3982\n",
      "3325\n",
      "os(cherry,information) =\n",
      "442⇤5+8⇤3982+2⇤3325\n",
      "p\n",
      "4422 +82 +22p\n",
      "52 +39822 +33252 = .017\n",
      "os(digital,information) =\n",
      "5⇤5+1683⇤3982+1670⇤3325\n",
      "p\n",
      "  p\n",
      "   = .996\n",
      "cosine value ranges from 1 for vectors pointing in the same direction, through\n",
      "ctors that are orthogonal, to -1 for vectors pointing in opposite directions.\n",
      " frequency values are non-negative, so the cosine for these vectors ranges\n",
      "1.\n",
      " see how the cosine computes which of the words cherry or digital is closer\n",
      "ng to information, just using raw counts from the following shortened table:\n",
      "pie\n",
      "data computer\n",
      "cherry\n",
      "442\n",
      "8\n",
      "2\n",
      "digital\n",
      "5\n",
      "1683\n",
      "1670\n",
      "information\n",
      "5\n",
      "3982\n",
      "3325\n",
      "erry,information) =\n",
      "442⇤5+8⇤3982+2⇤3325\n",
      "p\n",
      "4422 +82 +22p\n",
      "52 +39822 +33252 = .017\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>Col1</th>\n",
      "      <th>Col2</th>\n",
      "      <th>Col3</th>\n",
      "      <th>ollowi\n",
      "igital\n",
      "data</th>\n",
      "      <th>ng shortene\n",
      "is closer\n",
      "computer</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>counts from\\nords cherr\\n42 8</td>\n",
      "      <td>the f\\ny or d\\npi2e</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "      <td>horten\\n8</td>\n",
      "      <td>ed table:\\n2</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>computer\\n5 the1 f6o8l3low\\ncherry\\ne data com</td>\n",
      "      <td>i1n6g7 0s\\n442\\nputer</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>w</td>\n",
      "      <td>N N\\nch∑erry2inf∑or4m42a2tion8\\nv w</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "      <td>1683</td>\n",
      "      <td>1670</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>6</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>5 39282\\n2digit8al</td>\n",
      "      <td>3325\\n25</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>7</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>puter</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>8</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>1670\\n1683 1\\ninformation\\n2\\n2 53+3285 39</td>\n",
      "      <td>670\\n5\\n82 +</td>\n",
      "      <td>3982\\n2 332</td>\n",
      "      <td>3325\\n5</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Visualizing cosines \n",
      "(well, angles)\n",
      "•\n",
      "VECTOR SEMANTICS AND EMBEDDINGS\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "500\n",
      "digital\n",
      "cherry\n",
      "information\n",
      "Dimension 1: ‘pie’\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Cosine for computing word \n",
      "similarity\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "TF-IDF\n",
      "\n",
      "\n",
      "But raw frequency is a bad representation\n",
      "• The co-occurrence matrices we have seen represent each \n",
      "cell by word frequencies.\n",
      "• Frequency is clearly useful; if sugar appears a lot near \n",
      "apricot, that's useful information.\n",
      "• But overly frequent words like the, it, or they are not very \n",
      "informative about the context\n",
      "• It's a paradox! How can we balance these two conflicting \n",
      "constraints? \n",
      "\n",
      "\n",
      "Two common solutions for word weighting\n",
      "tf-idf:     tf-idf value for word t in document d:\n",
      "PMI: (Pointwise mutual information)\n",
      "◦PMI 𝒘𝟏, 𝒘𝟐= 𝒍𝒐𝒈\n",
      "𝒑(𝒘𝟏,𝒘𝟐)\n",
      "𝒑𝒘𝟏𝒑(𝒘𝟐)\n",
      "fool\n",
      "36\n",
      "0.012\n",
      "good\n",
      "37\n",
      "0\n",
      "sweet\n",
      "37\n",
      "0\n",
      "eighting of the value for word t in document d, wt,d thus combines\n",
      "with idf:\n",
      "wt,d = tft,d ⇥idft\n",
      "(6.13)\n",
      "f-idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\n",
      "idf values for the dimension corresponding to the word good have\n",
      " 0; since this word appears in every document, the tf-idf algorithm\n",
      "ored in any comparison of the plays. Similarly, the word fool, which\n",
      "t of the 37 plays, has a much lower weight.\n",
      "Words like \"the\" or \"it\" have very low idf\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>fool</th>\n",
      "      <th>36</th>\n",
      "      <th>0.012</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>good</td>\n",
      "      <td>37</td>\n",
      "      <td>0</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>commo\\nsweet</td>\n",
      "      <td>n so\\n37</td>\n",
      "      <td>lutio\\n0</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Term frequency (tf) in the tf-idf algorithm\n",
      "We could imagine using raw count:\n",
      "tft,d = count(t,d)\n",
      "But instead of using raw count, we usually squash a bit:\n",
      "tft,d = count(t,d)\n",
      "e commonly we squash the raw frequency a bit, by using the l\n",
      "ncy instead. The intuition is that a word appearing 100 times\n",
      "sn’t make that word 100 times more likely to be relevant to the \n",
      "ument. We also need to do something special with counts of 0,\n",
      " the log of 0.2\n",
      "tft,d =\n",
      "(\n",
      "1+log10 count(t,d)\n",
      "if count(t,d) > 0\n",
      "\n",
      "\n",
      "Document frequency (df)\n",
      "dft is the number of documents t occurs in.\n",
      "(note this is not collection frequency: total count across \n",
      "all documents)\n",
      "\"Romeo\" is very distinctive for one Shakespeare play:\n",
      "for discriminating those documents from the rest of the collection; terms that occur\n",
      "frequently across the entire collection aren’t as helpful. The document frequency\n",
      "nt\n",
      "y\n",
      "dft of a term t is the number of documents it occurs in. Document frequency is\n",
      "not the same as the collection frequency of a term, which is the total number of\n",
      "times the word appears in the whole collection in any document. Consider in the\n",
      "collection of Shakespeare’s 37 plays the two words Romeo and action. The words\n",
      "have identical collection frequencies (they both occur 113 times in all the plays) but\n",
      "very different document frequencies, since Romeo only occurs in a single play. If\n",
      "our goal is to find documents about the romantic tribulations of Romeo, the word\n",
      "Romeo should be highly weighted, but not action:\n",
      "Collection Frequency Document Frequency\n",
      "Romeo 113\n",
      "1\n",
      "action\n",
      "113\n",
      "31\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>Collection Frequency</th>\n",
      "      <th>Document Frequency</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Romeo</td>\n",
      "      <td>113</td>\n",
      "      <td>1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>action</td>\n",
      "      <td>113</td>\n",
      "      <td>31</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Inverse document frequency (idf)\n",
      "occur in a few like salad or Falstaff, to those which are very \n",
      "common as to be completely non-discriminative since they o\n",
      "good or sweet.3\n",
      "Word\n",
      "df\n",
      "idf\n",
      "Romeo\n",
      "1\n",
      "1.57\n",
      "salad\n",
      "2\n",
      "1.27\n",
      "Falstaff\n",
      "4\n",
      "0.967\n",
      "forest\n",
      "12\n",
      "0.489\n",
      "battle\n",
      "21\n",
      "0.246\n",
      "wit\n",
      "34\n",
      "0.037\n",
      "fool\n",
      "36\n",
      "0.012\n",
      "umber of documents in many collections, this measure\n",
      "th a log function. The resulting definition for inverse\n",
      " thus\n",
      "idft = log10\n",
      "✓N\n",
      "dft\n",
      "◆\n",
      "(6.13)\n",
      "or some words in the Shakespeare corpus, ranging from\n",
      "s which occur in only one play like Romeo, to those that\n",
      "N is the total number of documents \n",
      "in the collection\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Word</th>\n",
      "      <th>df</th>\n",
      "      <th>idf</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Romeo</td>\n",
      "      <td>1</td>\n",
      "      <td>1.57</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>salad</td>\n",
      "      <td>2</td>\n",
      "      <td>1.27</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>Falstaff</td>\n",
      "      <td>4</td>\n",
      "      <td>(6.\\n0.967</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>forest</td>\n",
      "      <td>12</td>\n",
      "      <td>0.489</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>battle</td>\n",
      "      <td>21</td>\n",
      "      <td>0.246</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>e corpus,\\nwit</td>\n",
      "      <td>rang\\n34</td>\n",
      "      <td>ing fr\\n0.037</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>6</th>\n",
      "      <td>fool\\nke Romeo,</td>\n",
      "      <td>36\\nto t</td>\n",
      "      <td>0.012\\nhose t</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>7</th>\n",
      "      <td>good</td>\n",
      "      <td>37</td>\n",
      "      <td>0</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>8</th>\n",
      "      <td>common l\\nsweet</td>\n",
      "      <td>ike f\\n37</td>\n",
      "      <td>ool or\\n0</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "What is a document?\n",
      "Could be a play or a Wikipedia article\n",
      "But for the purposes of tf-idf, documents can be \n",
      "anything; we often call each paragraph a document!\n",
      "\n",
      "\n",
      "Final tf-idf weighted value for a word\n",
      "Raw counts:\n",
      "tf-idf:\n",
      "(defined either by Eq. 6.11 or by Eq. 6.12) with id\n",
      "wt,d = tft,d ⇥idft\n",
      " tf-idf weighting to the Shakespeare term-documen\n",
      "uation Eq. 6.12. Note that the tf-idf values for the\n",
      " word good have now all become 0; since this wor\n",
      "f-idf algorithm leads it to be ignored. Similarly, th\n",
      "6.3\n",
      "•\n",
      "WORDS AND VECTORS\n",
      "7\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "1\n",
      "0\n",
      "7\n",
      "13\n",
      "good\n",
      "114\n",
      "80\n",
      "62\n",
      "89\n",
      "fool\n",
      "36\n",
      "58\n",
      "1\n",
      "4\n",
      "wit\n",
      "20\n",
      "15\n",
      "2\n",
      "3\n",
      "Figure 6.2\n",
      "The term-document matrix for four words in four Shakespeare plays. Each cell\n",
      "contains the number of times the (row) word occurs in the (column) document.\n",
      "represented as a count vector, a column in Fig. 6.3.\n",
      "To review some basic linear algebra, a vector is, at heart, just a list or array of\n",
      "CHAPTER 6\n",
      "•\n",
      "VECTOR SEMANTICS AND EMBEDDINGS\n",
      "As You Like It\n",
      "Twelfth Night\n",
      "Julius Caesar\n",
      "Henry V\n",
      "battle\n",
      "0.246\n",
      "0\n",
      "0.454\n",
      "0.520\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "TF-IDF\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Word2vec\n",
      "\n",
      "\n",
      "Sparse versus dense vectors\n",
      "tf-idf (or PMI) vectors are\n",
      "◦long (length |V|= 20,000 to 50,000)\n",
      "◦sparse (most elements are zero)\n",
      "Alternative: learn vectors which are\n",
      "◦short (length 50-1000)\n",
      "◦dense (most elements are non-zero)\n",
      "\n",
      "\n",
      "Sparse versus dense vectors\n",
      "Why dense vectors?\n",
      "◦Short vectors may be easier to use as features in machine \n",
      "learning (fewer weights to tune)\n",
      "◦Dense vectors may generalize better than explicit counts\n",
      "◦Dense vectors may do better at capturing synonymy:\n",
      "◦car and automobile are synonyms; but are distinct dimensions\n",
      "◦a word with car as a neighbor and a word with automobile as a \n",
      "neighbor should be similar, but aren't\n",
      "\n",
      "\n",
      "Common methods for getting short dense vectors\n",
      "“Neural Language Model”-inspired models\n",
      "◦Word2vec (skipgram, CBOW), GloVe\n",
      "Singular Value Decomposition (SVD)\n",
      "◦A special case of this is called LSA – Latent Semantic \n",
      "Analysis\n",
      "Alternative to these \"static embeddings\":\n",
      "•\n",
      "Contextual Embeddings (ELMo, BERT)\n",
      "•\n",
      "Compute distinct embeddings for a word in its context\n",
      "\n",
      "\n",
      "Simple static embeddings you can download!\n",
      "Word2vec (Mikolov et al)\n",
      "https://code.google.com/archive/p/word2vec/\n",
      "GloVe (Pennington, Socher, Manning)\n",
      "http://nlp.stanford.edu/projects/glove/\n",
      "\n",
      "\n",
      "Word2vec\n",
      "Popular embedding method\n",
      "Very fast to train\n",
      "Code available on the web\n",
      "Idea: predict rather than count\n",
      "Word2vec provides various options. We'll do:\n",
      "skip-gram with negative sampling (SGNS)\n",
      "\n",
      "\n",
      "Word2vec\n",
      "Instead of counting how often each word w occurs near \"apricot\"\n",
      "◦Train a classifier on a binary prediction task:\n",
      "◦Is w likely to show up near \"apricot\"?\n",
      "We don’t actually care about this task\n",
      "◦But we'll take the learned classifier weights as the word embeddings\n",
      "Big idea:  self-supervision: \n",
      "◦A word c that occurs near apricot in the corpus cats as the gold \"correct \n",
      "answer\" for supervised learning\n",
      "\n",
      "\n",
      "Approach: predict if candidate word c is a \"neighbor\"\n",
      "1. Treat the target word t and a neighboring context word c\n",
      "as positive examples.\n",
      "2. Randomly sample other words in the lexicon to get \n",
      "negative examples\n",
      "3. Use logistic regression to train a classifier to distinguish \n",
      "those two cases\n",
      "4. Use the learned weights as the embeddings\n",
      "\n",
      "\n",
      "Skip-Gram Training Data\n",
      "Assume a +/- 2 word window, given training sentence:\n",
      "…lemon, a [tablespoon of  apricot  jam,   a]  pinch…\n",
      "c1                   c2 \n",
      "c3      c4\n",
      " \n",
      "                                [target]\n",
      "\n",
      "\n",
      "Skip-Gram Classifier\n",
      "(assuming a +/- 2 word window)\n",
      "…lemon, a [tablespoon of  apricot  jam,   a]  pinch…\n",
      "c1                   c2 [target]\n",
      "c3      c4\n",
      "Goal: train a classifier that is given a candidate (word, context) pair\n",
      "(apricot, jam)\n",
      "(apricot, aardvark)\n",
      "…\n",
      "And assigns each pair a probability:\n",
      "P(+|w, c) \n",
      "\n",
      "\n",
      "Similarity is computed from dot product\n",
      "Remember: two vectors are similar if they have a high \n",
      "dot product\n",
      "◦Cosine is just a normalized dot product\n",
      "So:\n",
      "◦Similarity(w,c)  ∝w · c\n",
      "We’ll need to normalize to get a probability \n",
      "◦(cosine isn't a probability either)\n",
      "\n",
      "\n",
      "Turning dot products into probabilities\n",
      "Sim(w,c) ≈ w · c\n",
      "To turn this into a probability \n",
      "We'll use the sigmoid from logistic regression:\n",
      "6.8\n",
      "•\n",
      "WORD2VEC\n",
      "model the probability that word c is a real context word for target word w \n",
      "P(+|w,c) = s(c·w) =\n",
      "1\n",
      "1+exp(−c·w)\n",
      "igmoid function returns a number between 0 and 1, but to make it a proba\n",
      "odel the probability that word c is a real context word for target word w\n",
      "P(+|w,c) = s(c·w) =\n",
      "1\n",
      "1+exp(−c·w)\n",
      "gmoid function returns a number between 0 and 1, but to make it a prob\n",
      "also need the total probability of the two possible events (c is a context\n",
      "isn’t a context word) to sum to 1. We thus estimate the probability that \n",
      "a real context word for w as:\n",
      "P(−|w,c) = 1−P(+|w,c)\n",
      "\n",
      "\n",
      "How Skip-Gram Classifier computes P(+|w, c) \n",
      "This is for one context word, but we have lots of context words.\n",
      "We'll assume independence and just multiply them:\n",
      "6.8\n",
      "•\n",
      "WORD2VEC\n",
      "19\n",
      "We model the probability that word c is a real context word for target word w as:\n",
      "P(+|w,c) = s(c·w) =\n",
      "1\n",
      "1+exp(−c·w)\n",
      "(6.28)\n",
      "The sigmoid function returns a number between 0 and 1, but to make it a probability\n",
      "we’ll also need the total probability of the two possible events (c is a context word,\n",
      "and c isn’t a context word) to sum to 1. We thus estimate the probability that word c\n",
      "is not a real context word for w as:\n",
      "P(−|w,c) = 1−P(+|w,c)\n",
      "= s(−c·w) =\n",
      "1\n",
      "1+exp(c·w)\n",
      "(6.29)\n",
      "Equation 6.28 gives us the probability for one word, but there are many context\n",
      "P(−|w,c) = 1−P(+|w,c)\n",
      "= s(−c·w) =\n",
      "1\n",
      "1+exp(c·w)\n",
      "(6.29)\n",
      "uation 6.28 gives us the probability for one word, but there are many context\n",
      "rds in the window. Skip-gram makes the simplifying assumption that all context\n",
      "rds are independent, allowing us to just multiply their probabilities:\n",
      "P(+|w,c1:L) =\n",
      "L\n",
      "Y\n",
      "i=1\n",
      "s(ci ·w)\n",
      "(6.30)\n",
      "L\n",
      "X\n",
      "s\n",
      "\n",
      "\n",
      "Skip-gram classifier: summary\n",
      "A probabilistic classifier, given \n",
      "•\n",
      "a test target word w \n",
      "•\n",
      "its context window of L words c1:L\n",
      "Estimates probability that w occurs in this window based \n",
      "on similarity of w (embeddings) to c1:L (embeddings).\n",
      "To compute this, we just need embeddings for all the \n",
      "\n",
      "\n",
      "These embeddings we'll need: a set for w, a set for c\n",
      "1\n",
      "W\n",
      "C\n",
      "aardvark\n",
      "zebra\n",
      "aardvark\n",
      "apricot\n",
      "apricot\n",
      "|V|\n",
      "|V|+1\n",
      "& =\n",
      "target words\n",
      "context & noise\n",
      "…\n",
      "1..d\n",
      "…\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Word2vec\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Word2vec: Learning the \n",
      "embeddings\n",
      "\n",
      "\n",
      "Skip-Gram Training data\n",
      "…lemon, a [tablespoon of  apricot  jam,   a]  pinch…\n",
      "c1                   c2 [target]\n",
      "c3      c4\n",
      "6.8.2\n",
      "Learning skip-gram embeddings\n",
      "Word2vec learns embeddings by starting with an initial set of embedding vecto\n",
      "and then iteratively shifting the embedding of each word w to be more like the em\n",
      "beddings of words that occur nearby in texts, and less like the embeddings of word\n",
      "that don’t occur nearby. Let’s start by considering a single piece of training data:\n",
      "... lemon,\n",
      "a [tablespoon of apricot jam,\n",
      "a] pinch ...\n",
      "c1\n",
      "c2\n",
      "t\n",
      "c3\n",
      "c4\n",
      "This example has a target word t (apricot), and 4 context words in the L = ±\n",
      "window, resulting in 4 positive training instances (on the left below):\n",
      "positive examples +\n",
      "t\n",
      "c\n",
      "apricot tablespoon\n",
      "negative examples -\n",
      "t\n",
      "c\n",
      "t\n",
      "c\n",
      "apricot aardvark apricot seven\n",
      "\n",
      "\n",
      "Skip-Gram Training data\n",
      "…lemon, a [tablespoon of  apricot  jam,   a]  pinch…\n",
      "c1                   c2 [target]\n",
      "c3      c4\n",
      "For each positive \n",
      "example we'll grab k \n",
      "negative examples, \n",
      "6.8.2\n",
      "Learning skip-gram embeddings\n",
      "Word2vec learns embeddings by starting with an initial set of embedding vecto\n",
      "and then iteratively shifting the embedding of each word w to be more like the em\n",
      "beddings of words that occur nearby in texts, and less like the embeddings of word\n",
      "that don’t occur nearby. Let’s start by considering a single piece of training data:\n",
      "... lemon,\n",
      "a [tablespoon of apricot jam,\n",
      "a] pinch ...\n",
      "c1\n",
      "c2\n",
      "t\n",
      "c3\n",
      "c4\n",
      "This example has a target word t (apricot), and 4 context words in the L = ±\n",
      "window, resulting in 4 positive training instances (on the left below):\n",
      "positive examples +\n",
      "t\n",
      "c\n",
      "apricot tablespoon\n",
      "negative examples -\n",
      "t\n",
      "c\n",
      "t\n",
      "c\n",
      "apricot aardvark apricot seven\n",
      "\n",
      "\n",
      "Skip-Gram Training data\n",
      "…lemon, a [tablespoon of  apricot  jam,   a]  pinch…\n",
      "c1                   c2 [target]\n",
      "c3      c4\n",
      "6.8.2\n",
      "Learning skip-gram embeddings\n",
      "Word2vec learns embeddings by starting with an initial set of embedding vecto\n",
      "and then iteratively shifting the embedding of each word w to be more like the em\n",
      "beddings of words that occur nearby in texts, and less like the embeddings of word\n",
      "that don’t occur nearby. Let’s start by considering a single piece of training data:\n",
      "... lemon,\n",
      "a [tablespoon of apricot jam,\n",
      "a] pinch ...\n",
      "c1\n",
      "c2\n",
      "t\n",
      "c3\n",
      "c4\n",
      "This example has a target word t (apricot), and 4 context words in the L = ±\n",
      "window, resulting in 4 positive training instances (on the left below):\n",
      "positive examples +\n",
      "t\n",
      "c\n",
      "apricot tablespoon\n",
      "negative examples -\n",
      "t\n",
      "c\n",
      "t\n",
      "c\n",
      "apricot aardvark apricot seven\n",
      "Word2vec learns embeddings by starting with an initial set of embedding vectors\n",
      "nd then iteratively shifting the embedding of each word w to be more like the em-\n",
      "eddings of words that occur nearby in texts, and less like the embeddings of words\n",
      "hat don’t occur nearby. Let’s start by considering a single piece of training data:\n",
      ".. lemon,\n",
      "a [tablespoon of apricot jam,\n",
      "a] pinch ...\n",
      "c1\n",
      "c2\n",
      "t\n",
      "c3\n",
      "c4\n",
      "This example has a target word t (apricot), and 4 context words in the L = ±2\n",
      "window, resulting in 4 positive training instances (on the left below):\n",
      "positive examples +\n",
      "t\n",
      "c\n",
      "apricot tablespoon\n",
      "negative examples -\n",
      "t\n",
      "c\n",
      "t\n",
      "c\n",
      "apricot aardvark apricot seven\n",
      "\n",
      "\n",
      "Word2vec: how to learn vectors\n",
      "Given the set of positive and negative training instances, \n",
      "and an initial set of embedding vectors \n",
      "The goal of learning is to adjust those word vectors such \n",
      "that we:\n",
      "◦Maximize the similarity of the target word, context word pairs \n",
      "(w , cpos) drawn from the positive data\n",
      "◦Minimize the similarity of the (w , cneg) pairs drawn from the \n",
      ". \n",
      "\n",
      "\n",
      "Loss function for one w with cpos , cneg1 ...cnegk\n",
      "Maximize the similarity of the target with the actual context words, \n",
      "and minimize the similarity of the target with the k negative sampled \n",
      "non-neighbor words. \n",
      "• Minimize the similarity of the (w,cneg) pairs from the negative examples.\n",
      "If we consider one word/context pair (w,cpos) with its k noise words cneg1...cnegk,\n",
      "we can express these two goals as the following loss function L to be minimized\n",
      "(hence the −); here the first term expresses that we want the classifier to assign the\n",
      "real context word cpos a high probability of being a neighbor, and the second term\n",
      "expresses that we want to assign each of the noise words cnegi a high probability of\n",
      "being a non-neighbor, all multiplied because we assume independence:\n",
      "LCE = −log\n",
      "\"\n",
      "P(+|w,cpos)\n",
      "k\n",
      "Y\n",
      "i=1\n",
      "P(−|w,cnegi)\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logP(+|w,cpos)+\n",
      "k\n",
      "X\n",
      "i=1\n",
      "logP(−|w,cnegi)\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logP(+|w,cpos)+\n",
      "k\n",
      "X\n",
      "log\n",
      "�\n",
      "1−P(+|w,cnegi)\n",
      "�\n",
      "#\n",
      "\n",
      "\n",
      "Learning the classifier\n",
      "How to learn?\n",
      "◦Stochastic gradient descent!\n",
      "We’ll adjust the word weights to\n",
      "◦make the positive pairs more likely \n",
      "◦and the negative pairs less likely, \n",
      "◦over the entire training set.\n",
      "\n",
      "\n",
      "Intuition of one step of gradient descent\n",
      "W\n",
      "C\n",
      "move apricot and jam closer,\n",
      "increasing cpos � w\n",
      "aardvark\n",
      "move apricot and matrix apart\n",
      "decreasing cneg1 � w\n",
      "“…apricot jam…”\n",
      "w\n",
      "zebra\n",
      "aardvark\n",
      "jam\n",
      "apricot\n",
      "cpos\n",
      "matrix\n",
      "!\n",
      "c\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>Col1</th>\n",
      "      <th>Col2</th>\n",
      "      <th>Col3</th>\n",
      "      <th>Col4</th>\n",
      "      <th>Col5</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>apricot</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>w</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>jam</td>\n",
      "      <td></td>\n",
      "      <td>c\\npos</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>matrix</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>c\\nneg1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>6</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>7</th>\n",
      "      <td>None</td>\n",
      "      <td>Tolstoy</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>c\\nneg2</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>8</th>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td>None</td>\n",
      "      <td></td>\n",
      "      <td>None</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Reminder: gradient descent\n",
      "• At each step\n",
      "• Direction: We move in the reverse direction from the \n",
      "gradient of the loss function\n",
      "• Magnitude: we move the value of this gradient \n",
      "!\n",
      "!\" 𝐿(𝑓𝑥; 𝑤, 𝑦) weighted by a learning rate η \n",
      "• Higher learning rate means move w faster\n",
      "GISTIC REGRESSION\n",
      "  d\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically discussing gradient descent in the context of static embedding and vector semantic embeddings. The slide emphasizes the importance of understanding how gradients influence the learning process, particularly in relation to the loss function's direction and magnitude. It also highlights the role of a learning rate and its impact on the speed of convergence during the learning process. This concept is crucial for NLP tasks that involve training models with embeddings, as it helps in optimizing the model's parameters for better performance in tasks such as language modeling or sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "The derivatives of the loss function\n",
      "•\n",
      "VECTOR SEMANTICS AND EMBEDDINGS\n",
      "oof as an exercise at the end of the chapter):\n",
      "∂LCE\n",
      "∂cpos\n",
      "= [s(cpos ·w)−1]w\n",
      "∂LCE\n",
      "∂cneg\n",
      "= [s(cneg ·w)]w\n",
      "∂L\n",
      "k\n",
      "X\n",
      " context word cpos a high probability of being a neighbor, and the second ter\n",
      "resses that we want to assign each of the noise words cnegi a high probability o\n",
      "ng a non-neighbor, all multiplied because we assume independence:\n",
      "LCE = −log\n",
      "\"\n",
      "P(+|w,cpos)\n",
      "k\n",
      "Y\n",
      "i=1\n",
      "P(−|w,cnegi)\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logP(+|w,cpos)+\n",
      "k\n",
      "X\n",
      "i=1\n",
      "logP(−|w,cnegi)\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logP(+|w,cpos)+\n",
      "k\n",
      "X\n",
      "i=1\n",
      "log\n",
      "�\n",
      "1−P(+|w,cnegi)\n",
      "�\n",
      "#\n",
      "\"\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logP(+|w,cpos)+\n",
      "k\n",
      "X\n",
      "i=1\n",
      "log\n",
      "�\n",
      "1−P(+|w,cnegi)\n",
      "�\n",
      "#\n",
      "= −\n",
      "\"\n",
      "logs(cpos ·w)+\n",
      "k\n",
      "X\n",
      "i=1\n",
      "logs(−cnegi ·w)\n",
      "#\n",
      "(6.34\n",
      " is, we want to maximize the dot product of the word with the actual contex\n",
      "s, and minimize the dot products of the word with the k negative sampled non\n",
      "hbor words.\n",
      "We minimize this loss function using stochastic gradient descent.\n",
      "Fig. 6.1\n",
      "s the intuition of one step of learning.\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Col0</th>\n",
      "      <th>[s\n",
      "g s (</th>\n",
      "      <th>Col2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>apricot</td>\n",
      "      <td></td>\n",
      "      <td>w</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Update equation in SGD\n",
      "∂LCE\n",
      "∂w\n",
      "= [s(cpos ·w)−1]cpos +\n",
      "X\n",
      "i=1\n",
      "[s(cnegi ·w)]cnegi\n",
      "The update equations going from time step t to t + 1 in stochastic gradient de\n",
      "are thus:\n",
      "ct+1\n",
      "pos\n",
      "= ct\n",
      "pos −h[s(ct\n",
      "pos ·wt)−1]wt\n",
      "ct+1\n",
      "neg = ct\n",
      "neg −h[s(ct\n",
      "neg ·wt)]wt\n",
      "wt+1 = wt −h\n",
      "\"\n",
      "[s(cpos ·wt)−1]cpos +\n",
      "k\n",
      "X\n",
      "i=1\n",
      "[s(cnegi ·wt)]cnegi\n",
      "#\n",
      "Start with randomly initialized C and W matrices, then incrementally do updates\n",
      "\n",
      "\n",
      "Two sets of embeddings\n",
      "SGNS learns two sets of embeddings\n",
      "Target embeddings matrix W\n",
      "Context embedding matrix C \n",
      "It's common to just add them together, \n",
      "representing word i as the vector  wi + ci\n",
      "\n",
      "\n",
      "Summary: How to learn word2vec (skip-gram) \n",
      "embeddings\n",
      "Start with V random d-dimensional vectors as initial \n",
      "embeddings\n",
      "Train a classifier based on embedding similarity\n",
      "◦Take a corpus and take pairs of words that co-occur as positive \n",
      "examples\n",
      "◦Take pairs of words that don't co-occur as negative examples\n",
      "◦Train the classifier to distinguish these by slowly adjusting all \n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Word2vec: Learning the \n",
      "embeddings\n",
      "\n",
      "\n",
      "Vector \n",
      "Semantics & \n",
      "Embeddings\n",
      "Properties of Embeddings\n",
      "\n",
      "\n",
      "The kinds of neighbors depend on window size\n",
      "Small windows (C= +/- 2) : nearest words are syntactically \n",
      "similar words in same taxonomy\n",
      "◦Hogwarts nearest neighbors are other fictional schools\n",
      "◦Sunnydale, Evernight, Blandings\n",
      "Large windows (C= +/- 5) :  nearest words are related \n",
      "words in same semantic field\n",
      "◦Hogwarts nearest neighbors are Harry Potter world:\n",
      "\n",
      "\n",
      "Analogy/Relational Similarity:\n",
      "Another semantic property of embeddings is their\n",
      "ability to capture relational meanings. In an important early vector space model of\n",
      "cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\n",
      "for solving simple analogy problems of the form a is to b as a* is to what?. In such\n",
      "problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as\n",
      "grape is to\n",
      ", and must fill in the word vine. In the parallelogram model, illus-\n",
      "trated in Fig. 6.15, the vector from the word apple to the word tree (= #       »\n",
      "apple−#   »\n",
      "tree)\n",
      "is added to the vector for grape (#        »\n",
      "grape); the nearest word to that point is returned.\n",
      "tree\n",
      "apple\n",
      "Analogical relations\n",
      "The classic parallelogram model of analogical reasoning \n",
      "(Rumelhart and Abrahamson 1973)\n",
      "To solve: \"apple is to tree as grape is to  _____\"\n",
      "Add tree – apple  to grape to get vine\n",
      "\n",
      "\n",
      "Analogical relations via parallelogram\n",
      "The parallelogram method can solve analogies with \n",
      "both sparse and dense embeddings (Turney and \n",
      "Littman 2005, Mikolov et al. 2013b)\n",
      "king – man + woman is close to queen\n",
      "Paris – France + Italy is close to Rome\n",
      "For a problem a:a*::b:b*, the parallelogram method is:\n",
      "meaning could solve such analogy problems (Turney and Littman\n",
      "arallelogram method received more modern attention because of\n",
      "word2vec or GloVe vectors (Mikolov et al. 2013b, Levy and Gold\n",
      "ngton et al. 2014). For example, the result of the expression (#    \n",
      "kin\n",
      "»\n",
      "n is a vector close to #         »\n",
      "queen. Similarly, #      »\n",
      "Paris −#           »\n",
      "France + #     »\n",
      "Italy) \n",
      " that is close to #         »\n",
      "Rome. The embedding model thus seems to be ex\n",
      "ations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or\n",
      "TIVE/SUPERLATIVE, as shown in Fig. 6.16 from GloVe.\n",
      "or a a:b::a*:b* problem, meaning the algorithm is given a, b, and \n",
      "*, the parallelogram method is thus:\n",
      "\n",
      "\n",
      "Structure in GloVE Embedding space\n",
      "\n",
      "[IDS] The image depicts a visualization of word embeddings, which are vectors that represent words in a high-dimensional space. In this context, the lecture is likely discussing static embeddings, which are fixed representations of words and do not change over time. The GloVe (Global Vectors for Word Representation) embedding model is known for its ability to capture semantic relationships between words. This plot shows how words with similar meanings or usage patterns are closer together in the vector space, such as 'niece' and 'aunt', indicating their relatedness in the context of family relationships. The x-axis and y-axis represent the dimensions of the vector space where each word is plotted, showing the distribution of these words within the semantic space. [IDE]\n",
      "\n",
      "\n",
      "Caveats with the parallelogram method\n",
      "It only seems to work for frequent words, small \n",
      "distances and certain relations (relating countries to \n",
      "capitals, or parts of speech), but not others. (Linzen\n",
      "2016, Gladkova et al. 2016, Ethayarajh et al. 2019a) \n",
      "Understanding analogy is an open area of research \n",
      "(Peterson et al. 2020)\n",
      "\n",
      "\n",
      "Train embeddings on different decades of historical text to see meanings shift\n",
      "~30 million books, 1850-1990, Google Books data\n",
      "Embeddings as a window onto historical semantics\n",
      "\n",
      "[IDS] The image represents a visual explanation of how word embeddings, specifically static and diachronic embeddings, reveal semantic changes in language over time. The left part of the image illustrates static embeddings by showing words like \"gay\" and \"gays\" with arrows pointing to their meanings. The right part of the image demonstrates diachronic embeddings, which track changes in meaning over time. The example given is the word \"awful,\" which has shifted from a positive to a negative connotation between 1850 and 1990. This kind of analysis helps in understanding how language evolves and how certain words can change their meaning based on societal shifts and historical context. In an NLP lecture, this would be an important topic as it showcases how computational models can capture and analyze linguistic changes to improve natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Embeddings reflect cultural bias!\n",
      "Ask “Paris : France :: Tokyo : x” \n",
      "◦x = Japan\n",
      "Ask “father : doctor :: mother : x” \n",
      "◦x = nurse\n",
      "Ask “man : computer programmer :: woman : x” \n",
      "◦x = homemaker\n",
      "Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer \n",
      "programmer as woman is to homemaker? debiasing word embeddings.\" In NeurIPS, pp. 4349-4357. 2016.\n",
      "\n",
      "\n",
      "Historical embedding as a tool to study cultural biases\n",
      "• Compute a gender or ethnic bias for each adjective: e.g., how \n",
      "much closer the adjective is to \"woman\" synonyms than \n",
      "\"man\" synonyms, or names of particular ethnicities\n",
      "• Embeddings for competence adjective (smart, wise, \n",
      "brilliant, resourceful, thoughtful, logical) are biased toward \n",
      "men, a bias slowly decreasing 1960-1990\n",
      "• Embeddings for dehumanizing adjectives (barbaric, \n",
      "monstrous, bizarre)  were biased toward Asians in the \n",
      "Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. \n",
      "Proceedings of the National Academy of Sciences 115(16), E3635–E3644.\n",
      "\n",
      "lecture_questions:\n",
      "How does the parallelogram method work in solving analogies, and why is it not effective for all types of analogical relations?\n",
      "What role can historical diachronic embeddings play as a tool to study semantic shifts and cultural biases over time? Can you provide an example from your text above? \n",
      "How are gender and ethnic bias quantified in word embeddings, and what was the trend observed by Garg et al for competence adjectives during 1960-1990?\n",
      "What analogy-solving method is related to the parallelogram model of analogical reasoning and was shown by Mikolov et al. (2013b) to be viable with word2vec embeddings? \n",
      "\n",
      "A) Vector addition only in a single dimension \n",
      "\n",
      "B) Vector subtraction only in a single dimension \n",
      "\n",
      "C) Parallelogram method\n",
      "\n",
      "D) The cosine between the vectors of the two pairs is maximized for the second pair relative to the first.\n",
      "Based on the given code snippet and content, a good question could be: How does the function `bagOfWords` create document embeddings, and how do these differ from the input token-level embeddings in the dictionaries?\n",
      "lecture_content: Plan (Preliminary)\n",
      "Date\n",
      "Lec. Topics\n",
      "2024-04-09\n",
      "1\n",
      "Org, Introduction to NLP, Text Processing\n",
      "2024-04-16\n",
      "2\n",
      "Text Processing and Tokenization\n",
      "2024-04-23\n",
      "3\n",
      "Statistical Language Models\n",
      "2024-04-30\n",
      "4\n",
      "Static Embeddings (1-Hot, TF-IDF)\n",
      "2024-05-07\n",
      "5\n",
      "Static Embeddings (Word2Vec)\n",
      "2024-05-14\n",
      "6\n",
      "Feedforward & Recurrent Neural Language Models\n",
      "2024-05-21\n",
      "7\n",
      "Attention and the Transformer Architecture\n",
      "2024-05-28\n",
      "8\n",
      "Transformers and Applications\n",
      "2024-06-04\n",
      "9\n",
      "Generative Pretrained Transformers and Large Language Models\n",
      "2024-06-11\n",
      "10 Instruction-Following Language Models\n",
      "2024-06-18\n",
      "11 Model Adaptation, Retrieval Augmented Generation\n",
      "2024-06-25\n",
      "12 Project: Run your own LLM\n",
      "2024-07-02\n",
      "13 Project: Presentations\n",
      "2024-07-09\n",
      "14 Summary & Questions\n",
      "Plan updated!\n",
      "\n",
      "\n",
      "Feedforward Neural\n",
      "Language Models\n",
      "\n",
      "\n",
      "Quiz: How would you build a simple \n",
      "sentiment classifier?\n",
      "• Input: Document (e.g., amazon review)\n",
      "• Examples:\n",
      "• “This blender is great, it shredded my iphone without any problems”\n",
      "• “The blender is no good L”\n",
      "• “Broken after first use!”\n",
      "• “The dessert was delicious”\n",
      "• “I wouldn’t recommend the desserts at this place”\n",
      "• Output: Sentiment classification (binary: positive / negative)\n",
      "\n",
      "\n",
      "Simple Text Classifiers: Ideas\n",
      "• Manual Feature Engineering\n",
      "• Embeddings\n",
      "\n",
      "\n",
      "Sentiment Classifier: Manual Feature Engineering\n",
      "• Feature ideas:\n",
      "• Count positive words\n",
      "• Count negative words\n",
      "• Count positive emoticons\n",
      "• Count negative emoticons\n",
      "• Does the review contain negations?\n",
      "• Does the review include “!”?\n",
      "• Length of review\n",
      "1.\n",
      "“This blender is great, it shredded my iphone without any problems”\n",
      "2.\n",
      "“The blender is no good L”\n",
      "3.\n",
      "“Broken after first use!”\n",
      "4.\n",
      "“The dessert was delicious”\n",
      "5.\n",
      "“I wouldn’t recommend the desserts at this place”\n",
      "R. 1\n",
      "R. 2\n",
      "R. 3\n",
      "R. 4\n",
      "R. 5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "6\n",
      "5\n",
      "4\n",
      "8\n",
      "\n",
      "\n",
      "• Manual Feature Engineering turned each doc into a feature vector\n",
      "• Let’s plug them into a simple Feed Forward Neural Network\n",
      "R. 1\n",
      "R. 2\n",
      "R. 3\n",
      "R. 4\n",
      "R. 5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "6\n",
      "5\n",
      "4\n",
      "8\n",
      "Sentiment Classifier: Manual Feature Engineering\n",
      "R. 1\n",
      "R. 2\n",
      "R. 3\n",
      "R. 4\n",
      "R. 5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "6\n",
      "5\n",
      "4\n",
      "8\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>R. 1</th>\n",
      "      <th>R. 2</th>\n",
      "      <th>R. 3 R. 4 R. 5</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>1</td>\n",
      "      <td>1</td>\n",
      "      <td>1 1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>2</td>\n",
      "      <td></td>\n",
      "      <td>1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td></td>\n",
      "      <td>1</td>\n",
      "      <td></td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>1</td>\n",
      "      <td>1</td>\n",
      "      <td>1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td></td>\n",
      "      <td></td>\n",
      "      <td>1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>6</th>\n",
      "      <td>11</td>\n",
      "      <td>6</td>\n",
      "      <td>5 4 8</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Sentiment Classifier: Embeddings\n",
      "• Hand crafted features\n",
      "• often useful\n",
      "• might reach their limits\n",
      "• Why not let the network learn what’s important?\n",
      "• Textual feature representations / Embeddings:\n",
      "• One-Hot\n",
      "• TF-IDF\n",
      "• Word2Vec\n",
      "• …\n",
      "\n",
      "\n",
      "Sentiment Classifier: NN based on Embeddings\n",
      "h1\n",
      "h2\n",
      "h3\n",
      "hdh\n",
      "…\n",
      "U\n",
      "W\n",
      "y\n",
      "3d⨉1\n",
      "Hidden layer\n",
      "Output layer\n",
      "sigmoid\n",
      "The\n",
      "...\n",
      "dessert\n",
      "is\n",
      "wt-1\n",
      "w2\n",
      "w1\n",
      "dh⨉3d\n",
      "dh⨉1\n",
      "|V|⨉dh\n",
      "Projection layer\n",
      "embeddings\n",
      "p(positive sentiment|The dessert is…)\n",
      "^\n",
      "embedding for\n",
      "word 7\n",
      "embedding for \n",
      "word 23864\n",
      "embedding for\n",
      "word 534\n",
      "w3\n",
      "E\n",
      "…\n",
      "Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>The</th>\n",
      "      <th>dessert</th>\n",
      "      <th>is</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Sentiment Classifier: NN based on Embeddings\n",
      "h1\n",
      "h2\n",
      "h3\n",
      "hdh\n",
      "…\n",
      "U\n",
      "W\n",
      "y\n",
      "3d⨉1\n",
      "Hidden layer\n",
      "Output layer\n",
      "sigmoid\n",
      "The\n",
      "...\n",
      "dessert\n",
      "is\n",
      "wt-1\n",
      "w2\n",
      "w1\n",
      "dh⨉3d\n",
      "dh⨉1\n",
      "|V|⨉dh\n",
      "Projection layer\n",
      "embeddings\n",
      "p(positive sentiment|The dessert is…)\n",
      "^\n",
      "embedding for\n",
      "word 7\n",
      "embedding for \n",
      "word 23864\n",
      "embedding for\n",
      "word 534\n",
      "w3\n",
      "E\n",
      "…\n",
      "Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>The</th>\n",
      "      <th>dessert</th>\n",
      "      <th>is</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Sentiment Classifier: NN based on Embeddings\n",
      "h1\n",
      "h2\n",
      "h3\n",
      "hdh\n",
      "…\n",
      "U\n",
      "W\n",
      "y\n",
      "3d⨉1\n",
      "Hidden layer\n",
      "Output layer\n",
      "sigmoid\n",
      "The\n",
      "...\n",
      "dessert\n",
      "is\n",
      "wt-1\n",
      "w2\n",
      "w1\n",
      "dh⨉3d\n",
      "dh⨉1\n",
      "|V|⨉dh\n",
      "Projection layer\n",
      "embeddings\n",
      "p(positive sentiment|The dessert is…)\n",
      "^\n",
      "embedding for\n",
      "word 7\n",
      "embedding for \n",
      "word 23864\n",
      "embedding for\n",
      "word 534\n",
      "w3\n",
      "E\n",
      "…\n",
      "Image Source: D. Jurafsky, J. H. Martin: Speech and Language Processing 2024, Lecture 7\n",
      "\n",
      "\n",
      "Sentiment Classifier: NN based on Embeddings\n",
      "• Issues?\n",
      "• Only works for fixed length inputs!\n",
      "• In this case: 3 words\n",
      "• Workarounds?\n",
      "• Set input size to fixed length (e.g., that of longest review)\n",
      "• Pad shorter with 0 vectors\n",
      "• Truncate longer ones\n",
      "• Create single fixed size review embedding (sentence / paragraph / document)\n",
      "• Mean of all word embeddings\n",
      "• Element-wise max of all word embeddings\n",
      "h1\n",
      "h2\n",
      "h3\n",
      "hdh\n",
      "…\n",
      "U\n",
      "W\n",
      "y\n",
      "3d⨉1\n",
      "Hidden layer\n",
      "Output layer\n",
      "sigmoid\n",
      "The\n",
      "...\n",
      "dessert\n",
      "is\n",
      "wt-1\n",
      "w2\n",
      "w1\n",
      "dh⨉3d\n",
      "dh⨉1\n",
      "|V|⨉dh\n",
      "Projection layer\n",
      "embeddings\n",
      "p(positive sentiment|The dessert is…)\n",
      "^\n",
      "embedding for\n",
      "word 7\n",
      "embedding for \n",
      "word 23864\n",
      "embedding for\n",
      "word 534\n",
      "w3\n",
      "E\n",
      "…\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>embe\n",
      "w</th>\n",
      "      <th>dding for\n",
      "ord 7</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table><table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>The</th>\n",
      "      <th>dessert</th>\n",
      "      <th>is</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "Simple Text Classifiers: More Classes\n",
      "• General idea applicable not only to (binary) sentiment classification\n",
      "\n",
      "\n",
      "Simple Language Modeling Task\n",
      "\n",
      "\n",
      "Neural Language Models (LMs)\n",
      "Language Modeling: Calculating the probability of the \n",
      "next word in a sequence given some history. \n",
      "•\n",
      "We've seen N-gram based LMs\n",
      "•\n",
      "But neural network LMs far outperform n-gram \n",
      "language models\n",
      "State-of-the-art neural LMs are based on more \n",
      "powerful neural network technology like Transformers\n",
      "But simple feedforward LMs can do almost as well!\n",
      "\n",
      "\n",
      "Simple feedforward Neural Language Models\n",
      "Task: predict next word wt \n",
      " \n",
      " \n",
      "  given prior words wt-1, wt-2, wt-3, …\n",
      "Problem: Now we’re dealing with sequences of \n",
      "arbitrary length.\n",
      "Solution: Sliding windows (of fixed length)\n",
      "\n",
      "[IDS] In the context of a lecture on NLP, we are discussing the concept of predicting the next word in a sequence given prior words, which is a common task in language modeling. The lecture emphasizes the use of simple feedforward neural language models to solve this problem. The solution presented involves sliding windows of fixed length, indicating that the model considers a certain number of preceding words to predict the next one. This approach is essential for understanding how neural networks can be trained to generate coherent text or respond appropriately in natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Neural Language Model \n",
      "\n",
      "[IDS] The image represents a fundamental concept in Natural Language Processing (NLP) known as Feedforward Neural Language Models. These models are designed to process and understand human language by learning from large datasets of text. The diagram illustrates the architecture of such a model, which typically consists of an input layer that accepts text data, multiple hidden layers where complex patterns are detected, and an output layer that generates predictions or continuations of the input text. In this specific model, there's also a projection layer and an embedding layer for word representations, which are crucial for capturing the semantic meaning of words. The presence of mathematical notations like p(z|...), p(for|...), and V×1 indicates the probabilistic nature of these models and their focus on statistical prediction. This type of model is widely used in applications such as language translation, speech recognition, and text generation. [IDE]\n",
      "\n",
      "\n",
      "Why Neural LMs work better than N-gram LMs\n",
      "Training data:\n",
      "We've seen:  I have to make sure that the cat gets fed. \n",
      "Never seen:   dog gets fed\n",
      "Test data:\n",
      "I forgot to make sure that the dog gets ___\n",
      "N-gram LM can't predict \"fed\"!\n",
      "Neural LM can use similarity of \"cat\" and \"dog\" \n",
      "embeddings to generalize and predict “fed” after dog\n",
      "\n",
      "lecture_questions:\n",
      "What is the main reason why a simple feedforward neural language model often performs almost as well as more complex models in predicting the next word in a sequence? \n",
      "\n",
      "A) The use of embeddings for words allows the network to find semantic similarities.\n",
      "\n",
      "B) It only considers a fixed number of prior words through sliding windows, limiting complexity. \n",
      "\n",
      "C) It is easier and faster to train compared to more complex architectures. \n",
      "\n",
      "D) It does not suffer from the vanishing gradient problem that can affect recurrent neural networks.\n",
      "Based on the implementation of the `OneHotEncoder` class and its `encode` method in the lecture content, what would be a proper way to call the `encode` function for the token with index 5 (assuming our tokenizer's vocabulary size is 20), and what should be the shape of the output tensor?```python\n",
      "\n",
      "# Assuming that we've created an instance of OneHotEncoder:\n",
      "\n",
      "encoder = OneHotEncoder(vocab_size=20)\n",
      "\n",
      "# To get a one-hot encoded vector for the token at index 5, we would call our method as follows:\n",
      "\n",
      "encoding = encoder.encode(5)\n",
      "\n",
      "# And the shape of this output tensor should be (20,) which indicates that there are 20 elements in the vector, matching the size of our assumed vocabulary.\n",
      "\n",
      "```\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What do we need them for? (I/II)\n",
      "Fig. 1: High level representation of an encoder\n",
      "\n",
      "General case: \n",
      "We need RNNs for representing a sequence of variable length as a single \n",
      "vector (encoder) OR generating a sequence of variable length from a \n",
      "single vector (decoder)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents the concept of using Recurrent Neural Networks (RNNs) for tasks in NLP. RNNs are a type of neural network architecture that is particularly well-suited for processing sequences of data, such as text or speech. The diagram illustrates how an RNN can be used to generate a sequence of variable length from a single vector (encoder) or by encoding a sequence of variable length as a single vector (decoder). This process involves taking input sequences of varying lengths and converting them into a fixed-size representation that can be processed by the model. The figure shows the flow of information from the input sequence through the RNN, which generates an output vector (latent representation), indicating how the RNN processes and encodes the input data in NLP applications. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "What do we need them for? (II/II)\n",
      "Fig. 2: High level representation of a decoder\n",
      "\n",
      "[IDS] In the context of a lecture on Recurrent Neural Networks for Natural Language Processing (NLP), the image illustrates the process of generating text using an RNN. The input vector represents the initial state or prompt that the decoder will use to generate the output sequence. The high-level representation shows the flow from the input to the final output, emphasizing how the RNN processes and generates text based on the input provided. This is a fundamental concept in NLP, where RNNs are trained to understand and produce human-like language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Types of RNNs relevant to us\n",
      "\n",
      "Gated Recurrent Unit (GRU)\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) that focuses on the application of Recurrent Neural Networks (RNNs) in this field. The title \"Types of RNNs relevant to us\" suggests that the slide will discuss specific types of RNNs that are particularly useful or interesting for NLP tasks. Two examples provided are \"Gated Recurrent Unit (GRU)\" and \"Long Short Term Memory (LSTM)\", which are both architectures of RNNs known for their ability to handle sequences of data, like text or speech. These architectures are essential for tasks such as language modeling, machine translation, and text generation. The logos at the bottom indicate that the lecture might be part of a course offered by Hochschule Bonn-Rhein-Sieg and is related to Fachbereich Informatik, which translates to Faculty of Computer Science. This suggests an academic setting where students are likely learning about the theoretical and practical aspects of using RNNs in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Gated Recurrent Unit\n",
      "Fig. 3.1: High level represenation of GRU cell\n",
      "\n",
      "[IDS] The image depicts a fundamental component of Recurrent Neural Networks (RNNs), specifically the Gated Recurrent Unit (GRU). In the context of Natural Language Processing (NLP) lectures, GRUs are a type of RNN that can handle variable-length sequences, making them suitable for tasks such as language modeling or sequence prediction. The diagram illustrates how the GRU processes information through gates, which control the flow of information and help in learning long-term dependencies in data. This is crucial for NLP applications where understanding context over long sequences is essential. The presence of Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik suggests that this image may be part of educational material from an informatics department, emphasizing the practical application of GRUs in NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Gated Recurrent Unit\n",
      "Fig. 3.2: High level represenation of GRU cell\n",
      "Hidden states\n",
      "\n",
      "[IDS] The image is a visual representation of a Gated Recurrent Unit (GRU), which is a type of recurrent neural network architecture. In the context of NLP, or Natural Language Processing, GRUs are commonly used for modeling sequences in data, such as text or speech. The diagram shows how the hidden states are passed through time, allowing the model to remember and utilize information from previous time steps. This is particularly useful in NLP tasks where understanding the context and dependencies between words is crucial. The lecture you're attending is likely discussing how these GRUs can be applied to NLP problems, leveraging their ability to capture long-term dependencies and handle variable-length input sequences. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Gated Recurrent Unit\n",
      "Fig. 3.3: High level represenation of GRU cell\n",
      "Output of the previous time \n",
      "step. ht=0 = [0, …, 0]\n",
      "\n",
      "[IDS] The image depicts a Gated Recurrent Unit (GRU), which is a type of recurrent neural network architecture. In the context of NLP, or Natural Language Processing, GRUs are often used to process sequential data, such as text or speech. They are designed to handle the vanishing gradient problem that can occur in traditional RNNs by using gating mechanisms. These gates help control the flow of information through the network, allowing it to remember important information for longer periods. The diagram shows how the output at one time step is determined by the input at the current time step and the hidden state from the previous time step, illustrating the sequence-to-sequence nature of GRUs in NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Gated Recurrent Unit\n",
      "Fig. 3.4: High level represenation of GRU cell\n",
      "Input for the current \n",
      "time step. \n",
      "Embedding for \n",
      "token t\n",
      "\n",
      "[IDS] The image represents a fundamental concept in Natural Language Processing (NLP), specifically the use of Recurrent Neural Networks (RNNs). In the context of NLP, RNNs are often used to process sequences of data such as text. The Gated Recurrent Unit (GRU) is a type of RNN that helps address the vanishing gradient problem by introducing gating mechanisms. These gates control the flow of information within the network, allowing it to learn long-term dependencies effectively.\n",
      "\n",
      "The diagram illustrates the GRU's ability to maintain and update the hidden state (ht) over time steps (xt). This is crucial for tasks like language modeling or sequence-to-sequence translation where the context from previous tokens is essential. The input for the current time step is shown to be embedded into a high-level representation of the GRU cell, indicating how the model processes and retains information from previous inputs to inform its current output.\n",
      "\n",
      "As we are in a NLP lecture, this slide would serve as an educational tool to help students understand how RNNs, particularly GRUs, work internally and how they can be applied to NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Gated Recurrent Unit\n",
      "Fig. 3.5: High level represenation of GRU cell\n",
      "Output for the \n",
      "current time step.\n",
      "\n",
      "[IDS] The image is a visual representation of a Gated Recurrent Unit (GRU), which is a type of neural network architecture commonly used in Natural Language Processing (NLP). In the context of NLP, GRUs are employed to handle sequential data, such as text or speech, by capturing dependencies and patterns across time. The diagram illustrates how the GRU processes input over time steps, using gates to control the flow of information, which helps in learning long-term dependencies. This is crucial for tasks like language modeling, machine translation, and text generation. As we are in a NLP lecture, this slide serves as an educational tool to explain how GRUs contribute to the field by enabling models to effectively process and understand sequences of data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] In this NLP lecture, you are learning about the application of recurrent neural networks in natural language processing. The slide you're looking at is an example of how a recurrent neural network (RNN) can be used to encode sequences of words into fixed-length vectors, which can then be used for various NLP tasks such as language modeling or machine translation. The RNN shown processes input sequences like \"I like to eat pizza\" and outputs fixed-size vectors that capture the meaning of the sentence. This is a fundamental concept in NLP, as it allows computers to understand and interpret human language more effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] The image you're seeing is an illustration of a Recurrent Neural Network (RNN) being used for Natural Language Processing (NLP). In the context of NLP, RNNs are particularly useful because they can capture dependencies between words or phrases in text data. The diagram shows how an input sequence is processed by an RNN to produce an output. The RNN takes into account the previous output when generating the current output, which allows it to remember information over time. This property makes RNNs suitable for tasks such as language modeling, where predicting the next word in a sentence depends on the context provided by the words that came before it. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] The image represents a basic example of an encoder in the context of recurrent neural networks for natural language processing (NLP). The encoder is designed to process sequences of input data, such as text or words, and transform them into a fixed-size vector representation. In this example, the input sequence \"I like to eat pizza\" is being processed by an RNN (Recurrent Neural Network), which outputs a fixed-size vector representation h3. This vector h3 captures the essence of the input sequence and can be used for further tasks such as language modeling, machine translation, or sentiment analysis. The RNN architecture depicted includes an embedding layer that converts each word into a dense vector representation, followed by the recurrent layer that processes the sequence. The encoder's ability to handle sequential data and retain information over time makes it particularly suitable for NLP applications where understanding the context and meaning of text is crucial. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] In this NLP lecture, we are exploring the application of Recurrent Neural Networks (RNNs) in Natural Language Processing. The image illustrates a basic RNN architecture that is commonly used for sequence modeling tasks such as language translation, sentiment analysis, and text generation. The example shown demonstrates how an RNN processes input sequences to generate embeddings and outputs, which can be further utilized for various NLP tasks. This visual representation serves as a fundamental concept in understanding how RNNs work in the context of NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] The image represents a fundamental concept in NLP, specifically the use of Recurrent Neural Networks (RNNs) for processing sequences of data. In this context, an RNN is shown as a component that takes input sequences, processes them through its hidden layers, and outputs processed information. The presence of a Hochschule Bonn-Rhein-Sieg logo suggests that this might be educational material from that institution. Understanding how RNNs work is crucial for NLP tasks such as language modeling, sequence prediction, and text generation, as they can capture dependencies between elements in a sequence. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] In the context of a lecture on recurrent neural networks for NLP, this image represents a fundamental concept in the field of natural language processing. It illustrates how an encoder network can process sequential data, such as text, and generate representations that capture the meaning of the input sequence. The RNN shown is a type of neural network architecture that is particularly well-suited for tasks involving sequences, such as language modeling or machine translation. The previous and current inputs, along with their corresponding outputs, demonstrate how the RNN processes information over time, which is essential for understanding and generating coherent language sequences. This is a crucial aspect of NLP, where the ability to analyze and generate text is key. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Encoder Example\n",
      "\n",
      "[IDS] The image depicts a recurrent neural network (RNN) architecture being used for natural language processing (NLP). The RNN is designed to process sequences of data, such as words in a sentence. In this specific example, the input sequence is \"I like to eat pizza,\" and the task is to predict the next word in the sequence, which is \"<s>\" in this case. The RNN takes the previous output, which is represented by a hidden state h_6, and uses it along with the current input (the last processed word \"x_7\") to generate the next output, denoted as h_7. The final output is a probability distribution over possible next words, where the most likely next word is \"<s>\". This is a common approach in NLP to generate text or predict the next word in a sequence. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "\n",
      "[IDS] The image is a schematic representation of the Long Short-Term Memory (LSTM) memory cell, which is a type of recurrent neural network (RNN) architecture commonly used in Natural Language Processing (NLP). LSTMs are designed to handle the vanishing gradient problem that occurs in traditional RNNs by using memory cells that can store information for longer periods. This allows the network to remember information for longer durations, which is essential for tasks such as language modeling and sequence prediction.\n",
      "\n",
      "In this context, the LSTM cell depicted in the image is part of a larger system that includes an input layer (Input X), hidden layers (Hidden state H_t-1), and an output layer (Output gate o, tanh activation function). The cell has several components: a forget gate, an input gate, and an output gate. These gates control the flow of information into and out of the cell, as well as what is forgotten or remembered from the previous time step.\n",
      "\n",
      "Understanding LSTMs is crucial for NLP because they enable machines to process and analyze sequences of data, such as text or speech, with a level of context-awareness that goes beyond simple statistical models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "\n",
      "[IDS] The image is a visual representation of the Long Short-Term Memory (LSTM) memory cell, which is a type of recurrent neural network (RNN) architecture commonly used in Natural Language Processing (NLP). In the context of an NLP lecture, this diagram would be used to explain how LSTMs can remember information for long periods of time, enabling them to process sequences of data, such as sentences or paragraphs. The LSTM cell consists of three gates: the input gate, the forget gate, and the output gate. These gates control the flow of information into and out of the cell state, allowing the LSTM to selectively retain or discard information over time. This is crucial for tasks like language modeling, machine translation, and text generation, where understanding and remembering the context of words and phrases is essential. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Xt\n",
      "Wxi\n",
      "Whi\n",
      "Ht-1\n",
      "   bi\n",
      "It\n",
      "Xt\n",
      "Wxf\n",
      "Whf\n",
      "Ht-1\n",
      "   bf\n",
      "Ft\n",
      "Xt\n",
      "Wxo\n",
      "Who\n",
      "Ht-1\n",
      "   bo\n",
      "Ot\n",
      "Input Gate\n",
      "Forget Gate\n",
      "Output Gate\n",
      "\n",
      "[IDS] The image illustrates the basic components of a recurrent neural network (RNN) used in Natural Language Processing (NLP). RNNs are designed to process sequences of data, such as text or speech. The figure shows how input gates, forget gates, and output gates control the flow of information through the network, allowing it to remember past information and update its state based on new inputs. This is crucial for tasks like language modeling, where understanding the context of words in a sentence is essential. In an NLP lecture, this diagram would be used to explain how these components work together to enable the network to learn and generate sequences that reflect the structure and meaning of natural language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "\n",
      "[IDS] The image depicts a diagram illustrating the architecture of a Long Short-Term Memory (LSTM) network, which is a type of recurrent neural network. In the context of Natural Language Processing (NLP), LSTMs are particularly useful for handling sequences of data, such as text or speech, because they can maintain information over long periods by using memory cells that can be updated and reset based on the input. The diagram shows the flow of information through the network, with input being processed through gates to determine what information to forget, input, and output, ultimately resulting in an output gate that produces the final output. This mechanism allows LSTMs to remember information over long periods, which is essential for tasks like language modeling, machine translation, and sequence prediction in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Xt\n",
      "Ht-1\n",
      "   bc\n",
      " Ct\n",
      "Wxc\n",
      "Whc\n",
      " ~\n",
      "\n",
      "[IDS] The image represents a key concept in the field of Natural Language Processing (NLP) and Recurrent Neural Networks (RNNs). It illustrates the Long Short-Term Memory (LSTM) cell, which is a type of RNN that can learn order dependence in sequence prediction problems. In NLP, LSTMs are often used to process sequences of words or characters, allowing the model to retain information from previous time steps. The diagram shows how inputs (X_t), hidden states (H_t-1), and weights (W_xc, W_hc) are combined to produce an output (C_t) and a new hidden state (H_t), capturing both short-term and long-term dependencies in the data. This mechanism is crucial for tasks such as language modeling, machine translation, and text generation, where understanding the context of words over time is essential. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "\n",
      "[IDS] The image you see is a diagram that represents the concept of Long Short-Term Memory (LSTM), which is a type of recurrent neural network (RNN). RNNs are particularly useful in Natural Language Processing (NLP) tasks because they can remember information over long periods of time, allowing them to capture dependencies between words or phrases in text data. The LSTM architecture is designed to address the vanishing gradient problem that affects standard RNNs by using memory cells and gates to control the flow of information. In the context of an NLP lecture, this diagram would help students understand how LSTMs work internally and how they can be used to process sequences of data, such as text or speech, to perform tasks like language modeling, machine translation, or sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Update Cell state \n",
      "It\n",
      " Ct\n",
      " ~\n",
      "Ct = It ʘ Ct + Ft ʘ Ct-1\n",
      "Update Cell state \n",
      " ~\n",
      "Ft\n",
      "Ct-1\n",
      "Ct =\n",
      "Element-wise multiplication\n",
      "\n",
      "[IDS] The image represents a concept in Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP), specifically the mechanism of Long Short-Term Memory (LSTM). LSTMs are a type of RNN that can learn order dependence in sequence prediction problems. The diagram illustrates how an LSTM cell updates its state based on the current input and the previous cell state. This is crucial for NLP tasks, as it allows the model to remember information over long periods, which is essential for understanding the context and meaning of sequences of words or characters. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Update Cell state \n",
      "It\n",
      " Ct\n",
      " ~\n",
      "Ct = It ʘ Ct + Ft ʘ Ct-1\n",
      "Update Cell state \n",
      " ~\n",
      "Ft\n",
      "Ct-1\n",
      "Ct =\n",
      "Element-wise multiplication\n",
      "How much of the candidate \n",
      "cell state do we take?\n",
      "\n",
      "[IDS] The image is a visual representation of a concept in Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP). RNNs are a type of neural network architecture that can process sequences of data, such as text or speech. The slide specifically focuses on the \"Long Short Term Memory\" (LSTM) memory mechanism, which is a type of RNN that can learn order dependence in sequence prediction problems. \n",
      "\n",
      "The LSTM cell state is depicted as being updated by adding the output of the LSTM cell (Ct) and the input at time t (It). This is represented mathematically with the equation Ct = Lt ⊙ Ct-1 + Ft ⊙ Ct. Here, Lt and Ft are matrices that determine how much of the previous cell state (Ct-1) and the current input (It) are taken into account when updating the cell state.\n",
      "\n",
      "The slide also includes elements like the Hochschule Bonn-Rhein-Sieg logo and the name Tim Metzler, which could indicate the institution and the lecturer responsible for the lecture. Additionally, there is a mention of \"Element-wise multiplication\" and \"Fachbereich Informatik,\" suggesting that the topic might be related to computer science and specifically to the field of informatics or computer science departments.\n",
      "\n",
      "In summary, the image is a teaching aid used in an NLP lecture to explain how LSTMs work in processing sequences of data, emphasizing the importance of the cell state in preserving information over time. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Update Cell state \n",
      "It\n",
      " Ct\n",
      " ~\n",
      "Ct = It ʘ Ct + Ft ʘ Ct-1\n",
      "Update Cell state \n",
      " ~\n",
      "Ft\n",
      "Ct-1\n",
      "Ct =\n",
      "Element-wise multiplication\n",
      "How much of the previous \n",
      "cell state do we keep?\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on \"Recurrent Neural Networks for NLP,\" which stands for Natural Language Processing. It explains the concept of long short-term memory (LSTM) in neural networks. LSTMs are a type of recurrent neural network that helps the network remember information over longer periods, which is crucial for tasks like language modeling or machine translation. The slide specifically shows how an LSTM updates its cell state and how much of the previous cell state is kept. This is a fundamental concept in understanding how LSTMs process sequences of data, such as sentences or paragraphs, in natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "\n",
      "[IDS] The diagram represents a Long Short-Term Memory (LSTM) cell, which is a type of recurrent neural network (RNN) architecture. RNNs are particularly useful for Natural Language Processing (NLP) tasks as they can maintain information over long sequences. The LSTM cell has a memory component that allows it to remember information for an extended period, which is crucial for NLP applications like language modeling, machine translation, and sequence prediction. In this context, the LSTM would be used to process and analyze sequences of text or other linguistic data, capturing dependencies and relationships between words or phrases. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Long Short Term Memory\n",
      "Compute hidden state (output) \n",
      "Ct\n",
      "Ot\n",
      "Ht =\n",
      "\n",
      "[IDS] The image you're seeing is a slide from a lecture on \"Recurrent Neural Networks for NLP,\" which stands for Natural Language Processing. It focuses on the concept of Long Short-Term Memory (LSTM), a type of recurrent neural network that is particularly good at processing sequences of data, like words in a sentence. The slide illustrates how an LSTM works by computing a hidden state output, represented as \"Ht.\" This hidden state captures information from both previous states (Ot) and current inputs (Ct), allowing the network to remember information over long periods. The slide also includes logos from Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting that the lecture might be part of a course or program offered by these institutions. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "LSTM - Complexity\n",
      "\n",
      "Number of parameters:\n",
      "\n",
      "- Input dimensionality of d\n",
      "- Output dimensionality of h\n",
      "- 4 input weight matrices of (d x h)\n",
      "- 4 hidden weight matrices of (h x h)\n",
      "- 4 biases of (h x 1)\n",
      "\n",
      "→ Number of parameters = 4*d*h + 4*h*h + 4*h = 4h*(d + h + 1)\n",
      "\n",
      "Example:\n",
      "- Input size of 300\n",
      "- Output size of 32\n",
      "→ 42,624 parameters\n",
      "\n",
      "[IDS] The image depicts a slide from a lecture on recurrent neural networks for NLP, which stands for Natural Language Processing. The slide is focused on explaining the complexity of LSTM (Long Short-Term Memory) models. It lists several parameters that define the complexity of an LSTM network, such as the number of input dimensions, output dimensions, input weight matrices, hidden weight matrices, and biases. These parameters are crucial in determining the capacity of the model to learn and represent complex patterns in data, especially when dealing with sequential data like text or speech in NLP applications. The slide also provides an example calculation of these parameters for a specific model architecture, demonstrating how they contribute to the overall complexity of the network. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In this NLP lecture, we are exploring the concept of recurrent neural networks, specifically focusing on their application in natural language processing. The diagram illustrates a typical RNN architecture used for autoregressive generation with an RNN-based neural language model. This involves an input word being embedded into a vector space, which is then processed through a series of layers including an RNN and an embedding layer. The output from the RNN is fed back into itself, allowing for the modeling of sequences in data, such as text. The final softmax layer converts the output into a probability distribution over possible next words in a sequence, enabling the model to predict the most likely next word given the context provided by the input word. This process is fundamental in tasks like language translation, text summarization, and chatbots, where understanding and generating human-like language is crucial. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In this NLP lecture, we are presented with an illustration of a recurrent neural network (RNN) that is capable of autoregressive generation using a RNN-based neural language model. The diagram shows the flow of information through the RNN, starting with the embedding of input words, passing through a softmax function to generate the output word, and repeating this process to produce a sequence of words. This process is essential for understanding how RNNs can be used in natural language processing tasks such as language modeling and text generation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In the context of a lecture on Recurrent Neural Networks for NLP (Natural Language Processing), the image illustrates the architecture of an RNN that has been trained to perform autoregressive generation. This means that the model generates the next word in a sequence based on the previous words. The diagram shows how the input word is processed through an embedding layer, which transforms it into a dense vector representation. These vectors are then passed through a recurrent neural network, which consists of a series of recurrent units. Each unit processes the information from the previous time step and uses it to influence the current output. The Softmax function at the end of the process converts the final vector into a probability distribution over all possible words, allowing the model to predict the most likely next word in the sequence. This type of model is commonly used in language modeling tasks where the goal is to generate text that is coherent and grammatically correct. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In this NLP lecture, you are learning about Recurrent Neural Networks (RNNs) and their application in Natural Language Processing. The image represents an RNN model that is used for autoregressive generation with a neural language model. The model takes input words and generates the next word in sequence using a softmax function to determine the probability distribution over the vocabulary. This process is repeated until a stop token (So) is encountered, indicating the end of the generated text. The lecture emphasizes the importance of understanding how RNNs can be trained to generate coherent and contextually relevant text, which is a fundamental concept in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the image illustrates the concept of RNNs (Recurrent Neural Networks) and their application in generating autoregressive text. The diagram shows how an RNN processes input words sequentially and uses its internal memory to generate predictions for the next word in the sequence. This process is repeated until the end of the sentence is reached, resulting in an autoregressive generation that builds upon previous outputs. The lecture likely covers how RNNs can be used to perform tasks such as language modeling, text prediction, and machine translation, which are essential topics in natural language processing. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "Repeat until we \n",
      "generate </s>\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing Recurrent Neural Networks (RNNs) as a powerful tool for natural language processing. The diagram illustrates an RNN architecture that processes a sampled word and its context, such as \"So long and?\". The RNN uses embeddings to convert words into numerical representations, which are then processed through the network. The softmax function is used to generate a probability distribution over possible outputs. This process is repeated until a complete sentence is generated. The figure caption explains that the RNN can be autoregressive, using a language model like Jurafsky & Martin's \"Speech and Language Processing\" to generate text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "How do we train this model?\n",
      "\n",
      "[IDS] The image represents a slide from an educational presentation on Recurrent Neural Networks for Natural Language Processing (NLP). The title \"RRNs in NLP\" is prominently displayed, indicating the focus of the lecture. The subtitle \"How do we train this model?\" suggests that the content will delve into the training process of RNNs specifically for NLP tasks. The logos of the Hochschule Bonn-Rhein and Fachbereich Informatik indicate the institutions involved or associated with the lecture. The name Tim Metzler likely refers to the lecturer who prepared or is delivering the lecture. The blue circle with a white border might be a visual element or logo related to the course or institution. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 7\n",
      "Target: So long and thanks for all the ...\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the topic of recurrent neural networks for natural language processing. The slide presents a diagram that illustrates the architecture of an RNN and its components. It includes a section labeled \"Input Embeddings,\" which is crucial for representing words as vectors in the neural network. There is also a section labeled \"Softmax over Vocabulary\" that is used to determine the most probable next word given the current context. The flow of data through the network is indicated by arrows, showing how the input embeddings are processed through the RNN and then passed to the softmax layer. The lecture emphasizes the importance of understanding RNNs as language models, which is a fundamental concept in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 7\n",
      "Target: So long and thanks for all the ...\n",
      "No matter what we generate here,\n",
      "we always feed in the correct token as next input.\n",
      "This is called „Teacher Forcing“\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the role of RNNs (Recurrent Neural Networks) in natural language processing. The slide presents a diagram that illustrates how RNNs process and generate sequences of words. It highlights the concept of \"Teacher Forcing,\" which ensures that the correct token is always fed as the next input to the RNN during training. This technique helps in training RNNs as language models, as mentioned in Figure 9.6, sourced from Jurafsky & Martin's \"Speech and Language Processing\" book. The slide also includes references to Hochschule Bonn-Rhein-Sieg and Tim Metzler, indicating their contributions to the lecture content. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 7\n",
      "Target: So long and thanks for all the ...\n",
      "\n",
      "[IDS] In the image, we see a slide from a lecture on Recurrent Neural Networks for NLP (Natural Language Processing). The slide is titled \"RNNs in NLP\" and highlights the importance of long and thanks for all the words. It illustrates how an RNN processes input embeddings to generate output, which is a crucial aspect of language modeling. The equation provided is a mathematical representation of the softmax over vocabulary, which is a common technique used in NLP to convert a vector into a probability distribution over possible outputs. This slide serves as an educational tool to help students understand how RNNs can be applied to NLP tasks such as language processing. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "RNNs in NLP\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 7\n",
      "Target: So long and thanks for all the ...\n",
      "Loss for the sequence\n",
      "\n",
      "[IDS] The image represents a visual explanation of how Recurrent Neural Networks (RNNs) can be used in Natural Language Processing (NLP). RNNs are a type of neural network architecture that is particularly suited for processing sequences of data, such as text. In this context, the RNN is being used to predict the next word in a sequence given the current word and the entire sequence so far. The diagram shows the flow of information from the input embeddings through the RNN layers and finally to the output, where the probability of each possible next word is calculated. This process is part of the training phase, where the model learns to predict the most likely next word based on the context provided by the entire sequence up to that point. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "More applications\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 8\n",
      "\n",
      "[IDS] The image is a visual representation of a recurrent neural network architecture used in Natural Language Processing (NLP). It illustrates how the RNN processes input words, assigns tags to them, and uses a softmax layer to output the probability distribution over possible tags. This is a key concept in understanding how RNNs can be applied to tasks such as part-of-speech tagging, which is a fundamental task in NLP that involves assigning grammatical categories to each word in a sentence. The figure serves as an educational tool to help students and professionals grasp the structure and function of RNNs in the context of NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Machine Translation\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 19\n",
      "\n",
      "[IDS] The image is a visual representation of the process of machine translation using recurrent neural networks (RNNs) for natural language processing (NLP). In this context, an RNN is trained to encode and decode sentences between two languages. The lecture likely covers how these networks can be used to translate text from one language to another by learning the statistical patterns and structures of each language. The diagram illustrates the flow of information through the network, starting with the input sentence in the source language, passing through the encoder, and then being decoded into the target language. This is a fundamental concept in NLP, where understanding how RNNs function is crucial for building efficient and accurate machine translation systems. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Stacking RNN Layers\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] The image represents a stacked recurrent neural network architecture, which is a type of artificial neural network commonly used in natural language processing (NLP). The diagram shows multiple layers of RNNs, where each layer processes and passes information to the next. These networks are designed to handle sequential data, like text, by maintaining a hidden state that captures information from previous inputs. This allows the network to remember information over time, which is essential for tasks such as language modeling, machine translation, and text generation. The lecture titled \"Recurrent Neural Networks for NLP\" would likely cover the fundamentals of RNNs and their applications in natural language processing. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Bidirectional RNN\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "\n",
      "[IDS] The image is a schematic representation of a Bidirectional RNN, which is a type of recurrent neural network used in natural language processing (NLP). In the context of NLP, RNNs are particularly useful for tasks such as language modeling and sequence prediction. The bidirectional aspect of this RNN allows it to process data in both forward and backward directions, which is essential for capturing context from both past and future time points in a sequence, such as words in a sentence. This can help improve the performance of NLP models by providing richer information about the context in which words are used. The diagram illustrates how inputs (y1, y2, y3,..., yn) are processed through multiple layers of the RNN, with each layer consisting of nodes that represent the states at different time steps. The final outputs of the RNN (RNN 1 and RNN 2) are then concatenated, indicating how the model integrates information from both directions to produce its output. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Bidirectional RNN\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "Reading left to right\n",
      "\n",
      "[IDS] The image you see is a diagram of a Bidirectional RNN, which is a type of recurrent neural network used in Natural Language Processing (NLP). This particular model is trained to generate bidirectional representations of text data. The diagram illustrates how the RNN processes information both forward and backward through time, with each direction feeding into two separate RNN units labeled as RNN 1 and RNN 2. The outputs from these units are then concatenated to produce the final output at each time point. This technique allows the model to capture context from both past and future time points, which is essential for tasks like language translation or text summarization. As part of an NLP lecture, this would be used to explain how such models can improve the understanding and processing of language data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Bidirectional RNN\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "Reading left to right\n",
      "\n",
      "[IDS] The image you're viewing is a schematic representation of a Bidirectional Recurrent Neural Network (BRNN), which is a type of recurrent neural network used in Natural Language Processing (NLP). In NLP, we deal with the complexity of human language, which often requires understanding context from both past and future sentences. This is where BRNNs come into play.\n",
      "\n",
      "BRNNs are designed to process sequences of data, like text, in both forward and backward directions. They maintain a state for each time point, which is updated as it moves through the sequence. The outputs from the forward and backward passes are then concatenated at each time point to provide a richer context to the model.\n",
      "\n",
      "The diagram shows two RNNs running in opposite directions, labeled as \"RNN 1\" and \"RNN 2\". Each RNN processes the input sequence from different ends - \"RNN 1\" from left to right and \"RNN 2\" from right to left. The outputs of these two RNNs are then combined at each time point, as indicated by the yellow rectangles, which represent the concatenation of outputs.\n",
      "\n",
      "This architecture allows the model to capture dependencies in the input sequence from both directions, making it useful for tasks such as language translation, text classification, and machine reading comprehension.\n",
      "\n",
      "In the context of your lecture on \"recurrent_neural_networks_for_nlp\", this image would serve as an illustrative example of how BRNNs can be applied to enhance the performance of NLP models by leveraging the context from both sides of the input sequence. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Bidirectional RNN\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "Reading left to right\n",
      "Reading right to left\n",
      "\n",
      "[IDS] In the context of a lecture on recurrent neural networks for natural language processing (NLP), the image you're seeing illustrates a Bidirectional RNN, which is a type of recurrent neural network used in NLP tasks. This diagram represents how the network processes input data (like sentences or words) and outputs predictions (such as the next word in a sentence). The yellow boxes labeled 'x1', 'x2', 'x3' represent the input sequences, while 'y1', 'y2', 'y3' indicate the corresponding output sequences. The directional arrows show the flow of information from inputs to outputs and back, which is characteristic of bidirectional RNNs. These networks are trained to capture context from both past and future time points, which is crucial for understanding the meaning of words in a sequence. In this specific diagram, the outputs are generated by concatenating outputs from the forward and backward passes of the RNN. This concept is fundamental in NLP, as it helps in tasks like language modeling, machine translation, and text classification. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Bidirectional RNN\n",
      "source: Jurafsky & Martin „Speech and Language Processing“, Chapter 9, page 11\n",
      "Reading left to right\n",
      "Reading right to left\n",
      "\n",
      "[IDS] The image illustrates a Bidirectional RNN, which is a type of recurrent neural network used in natural language processing (NLP). In NLP, RNNs are particularly useful for tasks such as language modeling, machine translation, and text classification. The diagram shows the architecture of a Bidirectional RNN, where outputs from the network are concatenated and used to provide context from both past and future time points. This allows the model to capture information from both directions of the sequence, which is essential for tasks like sentiment analysis or named entity recognition where context from both sides of a word is crucial. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the main advantage of a Bidirectional RNN as compared to a unidirectional one for NLP tasks?\n",
      "\n",
      "A) It can only process data in one direction, which may limit its understanding of context.\n",
      "\n",
      "B) It's less computationally expensive than a bidirectional model.\n",
      "\n",
      "C) It captures context from both past and future time points, improving the performance on many NLP tasks\n",
      "\n",
      "D) It is easier to train as it has fewer connections to learn. [IDE]\n",
      "lecture_content: Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer\n",
      "\n",
      "Developed at Google in 2017 by Vaswani et.al.\n",
      "\n",
      "Works on a sequence of tokens (e.g. a sentence, document, etc)\n",
      "\n",
      "Often used as encoder decoder model\n",
      "\n",
      "Utilizes self-attention\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the concept of \"Transformers_Encoder\". It highlights that Transformers were developed at Google in 2017 by Vaswani et.al. The slide also mentions that Transformers work on a sequence of tokens, such as a sentence, document, etc., and are often used as encoder decoder models. Additionally, it points out the utilization of self-attention in this context. The slide includes logos of Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, suggesting an academic setting, and the name Tim Metzler, possibly the lecturer or author of the presentation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer\n",
      "Transformer\n",
      "RNN\n",
      "Sequence length\n",
      "fixed\n",
      "Infite in theory\n",
      "Attention\n",
      "Self Attention\n",
      "Bahdanau or \n",
      "Luong Attention\n",
      "Parsing the input \n",
      "sequence\n",
      "All at once\n",
      "One by one\n",
      "\n",
      "[IDS] The image displays a comparison between two types of neural network architectures used in natural language processing (NLP): Transformer and RNN (Recurrent Neural Network). The Transformer is characterized by its fixed sequence length and self-attention mechanism, which allows it to process input data in parallel. It is described as being infinite in theory, suggesting that it can handle sequences of any length. On the other hand, RNNs are known for their ability to parse input sequences one at a time, with attention mechanisms that focus on specific parts of the input. This table provides a concise overview of the fundamental differences between these two NLP models, which are essential for understanding recent advances in the field. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Tokenization\n",
      "\n",
      "Word2Vec: One token per word (word == token)\n",
      "\n",
      "FastText: One token per subword. Subword is character N-Gram.\n",
      "Example: Use 3 and 4-Grams of a word. Word: “School”\n",
      "’school’ → ['sch', 'cho', 'hoo', 'ool', 'scho', 'choo', 'hool']\n",
      "\n",
      "FastText of Word2Vec:\n",
      "- Word2Vec has fewer tokens\n",
      "- FastText can represent OOV words\n",
      "Word2Vec and FastText\n",
      "\n",
      "[IDS] The image you provided is a slide from an NLP lecture focusing on \"Tokenization\". It explains the process of tokenization using Word2Vec and FastText, which are techniques for converting text into vector representations. The slide specifically describes how Word2Vec represents words as vectors, with each word represented by a pair of tokens (word = token). It also mentions that FastText is one token per subword, and it uses subwords to represent character N-Grams, like 'choo' or 'hool'. This technique helps in representing OOV (out-of-vocabulary) words. The slide also shows examples of Word2Vec and FastText tokenization for the word'school'. The lecture seems to be detailed and informative, aimed at helping students understand how text data can be processed and converted into numerical data for machine learning models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "WordPiece Tokenization\n",
      "\n",
      "Developed at Google in 2015 by Wu et. al. (https://arxiv.org/pdf/1609.08144.pdf)\n",
      "\n",
      "Split text into tokens that can be subwords or full words\n",
      "\n",
      "Algorithm:\n",
      "Input: \n",
      "- Size of vocabulary\n",
      "- Corpus\n",
      "1. Start with one token == one character\n",
      "2. Combine two tokens into a new token. Use the combination that appears most \n",
      "often in the corpus.\n",
      "3. Add this new token to the vocabulary\n",
      "4. Repeat until #tokens = size of vocabulary\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically about the topic of \"Transformers_Encoder\". The slide focuses on WordPiece Tokenization, a technique used in NLP to split words into subwords or full words. It provides steps for implementing this algorithm, such as starting with one token, combining tokens into a new token, adding new tokens to the vocabulary, and repeating until reaching a desired size of vocabulary. This process is crucial for models like BERT, which use subword tokenization to improve their understanding of language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "WordPiece Tokenization (cont’d)\n",
      "\n",
      "Example:\n",
      "Vocab Size: 4\n",
      "Corpus: snowboard, snow, snowboarding, surfing, surfboarding, surf\n",
      "Tokens: \n",
      "- snow\n",
      "- board\n",
      "- ing\n",
      "- surf\n",
      "\n",
      "We only need 4 tokens to represent all words:\n",
      "- snowboard = snow + ##board\n",
      "- snowboarding = snow + ##board + ##ing\n",
      "\n",
      "Can be applied to language such as Chinese or Japanese\n",
      "\n",
      "[IDS] The image is a slide from an NLP (Natural Language Processing) lecture, focusing on the concept of \"Transformers_Encoder\". It explains how WordPiece Tokenization is used in transformer models to handle out-of-vocabulary words by breaking them down into subwords. The slide provides an example vocabulary size of 4 and lists tokens such as'snow', 'board', 'ing', and'surf'. It also mentions that only 4 tokens are needed to represent all words, with examples like'snow + ##board = snow + ##board + ##ing'. Additionally, it notes that this tokenization can be applied to languages such as Chinese or Japanese. The slide is attributed to Hochschule Bonn-Rhein-Sieg and Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Preprocessing\n",
      "\n",
      "Add special tokens to the text\n",
      "\n",
      "[CLS] – Special token at the start of each input sequence. The embedding for this will often be \n",
      "used for classification. Learns information about the whole sequence.\n",
      "\n",
      "[PAD] – We always feed a fixed length sequence of text. Usually our input sequence is smaller \n",
      "and needs to be padded to have this length. This is done by the padding token.\n",
      "\n",
      "[SEP] – We might feed several sentences or documents to the model. Each of them is \n",
      "separated by the separator token.\n",
      "\n",
      "[MASK] – During training we might want to hide tokens and predict them. These are replaced \n",
      "with the mask token\n",
      "\n",
      "Example: Model sequence size = 12. Input: “I like cake. You like cake”\n",
      "→ [CLS] I like cake [SEP] You like cake [SEP] [PAD] [PAD] [PAD]\n",
      "\n",
      "[IDS] The image you see is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"Transformers_Encoder\". The slide outlines the preprocessing steps involved in preparing text data for use with a transformer model. It emphasizes the importance of adding special tokens, understanding the concept of [CLS], [PAD], and [MASK] in the context of sequence modeling, and how these elements contribute to the overall learning process within the encoder component of transformer models. The slide serves as an educational tool to explain complex NLP concepts in a structured and easy-to-understand manner. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Architecture\n",
      "Fig. 1: Transformer Encoder Decoder \n",
      "(source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image depicts a Transformer Encoder architecture, which is a key component of Transformer models widely used in Natural Language Processing (NLP). The figure illustrates the structure of the encoder, emphasizing its multi-headed attention mechanism and feed-forward layers. This architecture allows the model to attend to different positions of the input sequence simultaneously, capturing long-range dependencies effectively. In an NLP lecture, this would be an important topic as it explains how such models process and understand language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual representation of a Transformer Encoder, which is a key component in the architecture of transformer-based models used in natural language processing (NLP). In this context, input is typically a sequence of tokens, often with a dimensionality of 768 and a length of 12*64. The diagram illustrates how multi-head attention and positional encoding are integrated into the encoder to process inputs effectively. This structure is essential for tasks like translation or text generation where understanding the relationships between different parts of the input data is crucial. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder – Input (I)\n",
      "Fig. 3: Token Embeddings (source: own)\n",
      "Input sentence: I like cake\n",
      "Fig. 3: Token Embeddings (source: own)\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, this image represents a fundamental concept in transformer-based models: the encoder. It serves as an illustration for how input sentences are processed and converted into token embeddings, which are numerical representations used by the model to understand the meaning behind words. The yellow grid indicates the encoding process, where each cell corresponds to a word or token from the sentence \"I like cake,\" and the figure number suggests that this is part of a larger presentation or educational material. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the \n",
      "same time. \n",
      "We need to store position info using a Positional \n",
      "Encoding\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image you see is a visual representation of the Transformer Encoder, which is a key component in the Transformer architecture commonly used in Natural Language Processing (NLP). This encoder is responsible for processing input sequences of tokens, typically words or subwords, and encoding them into a continuous representation. The figure illustrates the multi-headed attention mechanism that allows the model to focus on different parts of the input sequence simultaneously. This process enables the model to capture long-range dependencies between tokens and is crucial for tasks such as language translation, text summarization, and question answering. The accompanying text explains that the input sequence is usually of dimensionality 768 (12*64) and that the model uses all tokens in the input sequence at the same time to store position information using a positional encoding. This is essential for maintaining the order of the tokens in the sequence, as the attention mechanism does not inherently consider the token's position. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Fig. 4: Positional Embeddings (source: own)\n",
      "\n",
      "[IDS] The image displays a visual representation of the Transformer Encoder, which is a key component in the Transformer model commonly used in Natural Language Processing (NLP). The Transformer Encoder is responsible for processing input sequences, such as sentences or paragraphs, and converting them into meaningful representations that can be used for tasks like language translation, sentiment analysis, or question-answering systems. In this specific visualization, we see the concept of positional embeddings being used to give the model information about the order of the input sequence. These embeddings are crucial because they allow the model to understand the context and relationships between words within a sentence, even though the input is typically fed into the model as a flat sequence of tokens without any explicit indication of their original order. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Fig. 5: Token + Positional Embeddings = Input Embeddings (source: own)\n",
      "\n",
      "[IDS] The image you're seeing is a visual representation of the concept of an \"Encoder\" in the context of Transformers, which is a type of neural network architecture commonly used in Natural Language Processing (NLP). The Encoder is part of the Transformer model that processes input data, such as sentences or sequences of tokens, and generates embeddings. These embeddings are representations of the input data in a vector space where each token is assigned a unique vector. In this specific figure, we can see how different types of padding (PAD) are represented in the embedding space. This visualization helps us understand how the Transformer Encoder handles varying input lengths by adding padding tokens (like [SEP], [PAD]) to ensure that all inputs have the same length before being processed. This is crucial for maintaining the integrity of the sequence information during the encoding process. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the same \n",
      "time. \n",
      "We need to store position info using a Positional Encoding\n",
      "Perform self-attention\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image depicts a Transformer Encoder, which is a crucial component in the architecture of transformer-based models, particularly those used in Natural Language Processing (NLP). This encoder is responsible for processing input sequences of tokens, typically in the form of embeddings. It takes these embeddings as input and outputs a sequence of tokens at the same time, which is achieved through multi-head attention mechanisms that allow the model to attend to all tokens in the input sequence simultaneously. The figure is a simplified representation of how the Transformer Encoder functions, emphasizing its role in NLP tasks such as language translation or sentiment analysis. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Learned weight matrix\n",
      "Fig. 6: Computation of query, key and value matrices (source: own)\n",
      "\n",
      "[IDS] In the context of a NLP lecture, this image illustrates the process and components involved in understanding and interpreting text. The \"Input Embeddings\" section shows how words are initially represented as numerical vectors, which then go through a learned weight matrix to generate \"Output Embeddings\". This transformation is crucial for capturing the meaning and context of words within a sentence or text.\n",
      "\n",
      "The \"Figure 6: Computation of query, key and value matrices (source: own)\" indicates that the lecture might be discussing the inner workings of attention mechanisms in models like transformers. These matrices are fundamental to how transformers encode and process information, enabling the model to focus on different parts of the input sequence when making predictions or generating outputs.\n",
      "\n",
      "Lastly, the mention of \"Hochschule Bonn-Rhein-Sieg\" and \"Tim Metzler\" suggests that this is an educational material from a specific institution, possibly created by or attributed to Tim Metzler. This could imply that the lecture is part of a curriculum or research project at that institution. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Query, Key, Value\n",
      "One row is the embedding of one input token\n",
      "Fig. 7: Query, Value, Key (source: own)\n",
      "\n",
      "[IDS] The image is a visual representation of a key concept in the field of Natural Language Processing (NLP), specifically related to the topic of Transformers and their encoders. It illustrates the idea of embedding one input token into a matrix, which can be seen as a fundamental step in processing text data using transformer models. These models are known for their ability to capture complex relationships between words in a sentence or document. The \"Query, Key, Value\" components shown in the image are essential parts of the attention mechanism in transformers, which allows the model to focus on different parts of the input data when making predictions or generating outputs. The source of this visualization is indicated as \"Hochschule Bonn-Rhein-Sieg Fachbereich Informatik,\" suggesting that this lecture slide is part of an educational resource from this institution's informatics department. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Query, Key, Value\n",
      "\n",
      "Wikipedia search example:\n",
      "\n",
      "Query: Give me documents about a search term. The term could be “German car \n",
      "manufacturers”\n",
      "\n",
      "Key: The ids of the documents we want to search. Could be the page name like: \n",
      "“Mercedes-Benz”, “Audi”, “Cars”, “Potato”, …\n",
      "\n",
      "Value: The content of the document. Could be “Mercedes-Benz is a German car \n",
      "manufacturer founded in 1926, ...”\n",
      "\n",
      "Goal: Make query and relevant keys similar. Encode query and keys as vectors. \n",
      "Take dot product. High values indicate high relevance, low values low relevance.\n",
      "\n",
      "[IDS] The image displays a slide from an NLP lecture, focusing on the topic of \"Transformers_Encoder.\" It explains how to perform a keyword search using Wikipedia as an example. The slide outlines the key components of such a search: the query (e.g., \"German car manufacturers\"), the key terms or topics of interest (e.g., \"Mercedes-Benz,\" \"Audi,\" \"Cars,\" \"Potato\"), and the value or relevance of the document (in this case, mentioning Mercedes-Benz's founding year). The goal is to make queries and encode keywords as vectors with high relevance and low values for low relevance. This concept is fundamental in natural language processing, especially in transformer models that use encoders to process and represent text data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Query, Key, Value\n",
      "\n",
      "[IDS] The image is a visual representation of the structure and function of a Transformer Encoder, which is a crucial component in transformer-based models used in Natural Language Processing (NLP). The encoder takes in input sequences, processes them through self-attention mechanisms, and produces output sequences. The diagram illustrates how queries (Q), keys (K), and values (V) are processed through multiple layers, each composed of attention and feed-forward networks. The [CLS] token at the beginning serves as a special token for classification tasks, and the [PAD] tokens likely represent padding added to ensure that all sequences have the same length. This structure is essential for understanding how transformers encode information from input data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Query, Key, Value\n",
      "\n",
      "[IDS] The image represents a simplified visualization of the attention mechanism in a Transformer Encoder, which is a key component in transformer-based models used in Natural Language Processing (NLP). The Transformer Encoder utilizes self-attention to weigh the importance of different parts of the input sequence when generating the output. In this illustration, the \"Query\" (Q), \"Key\" (K), and \"Value\" (V) represent the three components of the attention mechanism. The colored blocks symbolize the encoding of these components from the input sequence, which are then processed through multiple layers to produce the final output. This process allows the model to focus on relevant parts of the input data while ignoring irrelevant information, leading to more accurate and contextually appropriate outputs. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Attention\n",
      "Take query vector and all key \n",
      "vectors.\n",
      "\n",
      "[IDS] The image illustrates the process of attention mechanism in a Transformer Encoder, which is a crucial component in many NLP models. It shows how queries (Q), keys (K), and values (V) are used to compute attention scores, allowing the model to focus on different parts of the input sequence when making predictions. The color-coded representation helps in understanding the different roles of Q, K, and V vectors in the attention mechanism. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Attention Scores\n",
      "Take query vector and all key \n",
      "vectors.\n",
      "Build dot product (Q*KT)\n",
      "\n",
      "[IDS] The image represents a slide from an NLP lecture focused on the Transformer Encoder. It illustrates the concept of Attention Scores, which is a crucial component of the Transformer architecture, particularly in models like BERT, GPT, and XLNet. The attention scores determine how much weight each part of the input sequence should have when the model is generating the output. The slide visually breaks down the process into two main parts: the initial attention scores calculation and the final query vector formation. The initial attention scores are represented as a grid, where each cell corresponds to the interaction between a query (Q) and a key (K). These scores are then used to form a query vector and all key vectors, which are essential for the next steps in the Transformer Encoder's process. This visual aid helps students understand the complex mechanism behind attention mechanisms in NLP models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Attention Scores\n",
      "Take query vector and all key \n",
      "vectors.\n",
      "Build dot product (Q*KT)\n",
      "How relevant is the token “I” to the CLS token?\n",
      "\n",
      "[IDS] The image represents a visual explanation of how attention scores are used in the Transformer Encoder architecture. The Transformer Encoder is a crucial component of transformer models, which are widely used in Natural Language Processing (NLP) tasks. The attention mechanism allows the model to weigh the importance of different parts of the input sequence when making predictions or generating outputs.\n",
      "\n",
      "In this particular illustration, we see an example of how attention scores are calculated for a given query vector and key vectors. The scores are then used to build a dot product (Q*K^T), which influences the final output of the transformer model. The lower part of the image shows an example of how the attention scores are applied to a sentence, highlighting the relevance of certain tokens (like \"I\" and \"the\") over others.\n",
      "\n",
      "Understanding the concept of attention scores and their role in the Transformer Encoder is essential for grasping the workings of state-of-the-art NLP models. This knowledge is valuable for both researchers working on improving these models and practitioners applying them in real-world applications. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Masking\n",
      "\n",
      "[IDS] The image illustrates a concept from the field of Natural Language Processing (NLP), specifically related to transformer encoders. The key focus is on the masking technique used in transformer models, particularly during pre-training. This process involves covering parts of the input sequence with special tokens like [CLS] and [PAD], which serve as placeholders for missing information or padding. These masks are essential for training models that can predict missing words or handle sequences of different lengths. The image provides a visual representation of how this technique is applied to the input sequence \"I like cake,\" showing the masked version as \"[CLS] K I like V [PAD] [PAD] [PAD]\". The lecture likely explains the benefits of this technique, such as allowing the model to learn without relying on the exact word order and improving its ability to generalize to unseen data. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Masking\n",
      "Set to -infinity to mask out\n",
      "\n",
      "[IDS] The image represents a concept in NLP known as \"Masking,\" specifically related to the Transformer Encoder architecture. In this context, masking is used to create a target sequence for training sequence-to-sequence models like translation or summarization. The goal is to generate a target sequence that corresponds to the input sequence, but with some information removed (masked) to challenge the model and improve its ability to predict the missing information. This process is crucial for training effective models that can handle real-world tasks where not all information is available. The diagram visually demonstrates how certain positions in the input sequence are masked, leading to an incomplete sequence that the model must learn to predict. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Masking\n",
      "-infinity becomes 0. Scores sum up to 1.\n",
      "\n",
      "[IDS] The image is a visual representation of the masking technique used in Transformer models, specifically within the encoder component. This technique is crucial for preventing the model from seeing the future tokens during training, which would make it impossible to learn any form of sequence modeling. The mask is applied to the attention mechanism of the Transformer, where the masked positions are replaced with a special token (often referred to as the 'pad' token) and the original values are stored elsewhere. During the training phase, the model learns to predict these masks based on the context provided by the surrounding tokens. The softmax function is then applied to ensure that the probabilities sum up to 1. This process is essential for training transformer-based language models like BERT, GPT, and others, which rely on self-attention mechanisms to understand the context of words within sentences. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Masking\n",
      "-infinity becomes 0. Scores sum up to 1.\n",
      "\n",
      "[IDS] The image illustrates a key concept in the context of NLP, specifically related to the Transformer Encoder architecture. It demonstrates the process of masking in the attention mechanism of the Transformer model. The attention mechanism is crucial for understanding the context of words in a sequence and assigning importance to them when generating translations or performing other tasks. By masking certain positions in the input sequence, the model can learn to focus on relevant information without being influenced by the context provided by future tokens. This is essential for training models that need to predict or generate sequences where the order of tokens matters. In this lecture, you would learn how this masking process helps in preventing the model from looking at future tokens during training, which is a critical step in making the model more robust and capable of handling real-world NLP tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Attention: Embeddings\n",
      "Embedding for token \n",
      "[CLS]. Incorporates \n",
      "information about all \n",
      "other tokens.\n",
      "\n",
      "[IDS] The image represents a concept in the field of Natural Language Processing (NLP), specifically related to transformer encoders. These encoders are part of the architecture of transformer models, which are widely used in NLP tasks such as language translation, sentiment analysis, and text summarization. The figure illustrates how embeddings for tokens, like 'Hochschule' or 'Bonn-Rhein-Sieg', are processed by a transformer encoder. This process involves stacking layers of attention mechanisms and feed-forward networks to capture the context and meaning of the input tokens. The embeddings are incorporated into the model to provide information about all other tokens, enabling the model to understand the relationships between words and phrases in the input text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Attention: Embeddings\n",
      "Repeat for all tokens in the input sequence.\n",
      "\n",
      "[IDS] The image illustrates the concept of embeddings in a transformer encoder, which is a key component in natural language processing (NLP) models. Embeddings are representations of words or tokens as vectors in a high-dimensional space, allowing the model to capture semantic relationships between words. The transformer encoder uses self-attention mechanisms to process these embeddings and generate contextually relevant representations. The repeating pattern of embeddings in the input sequence is crucial for the transformer encoder to understand the meaning of each token within its context. This visualization aids in understanding how embeddings contribute to the performance of transformer-based NLP models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Matrix View\n",
      "\n",
      "[IDS] The image displays a slide from a lecture on Natural Language Processing (NLP) with the title \"Matrix View\". The content suggests that the lecture is focused on explaining how transformers, specifically the encoder part of a transformer model, process and understand language. The encoder is a crucial component in transformer models as it encodes input sequences into continuous representations that are used for further processing or to generate outputs. The mention of Hochschule Bonn-Rhein-Sieg indicates that this lecture might be taking place at that institution. The names Tim Metzler and Fachbereich Informatik suggest that Tim Metzler is likely the instructor, and the faculty of computer science is involved in teaching this course. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 8a: Computation of relevancy scores (query * key) (source: own)\n",
      "\n",
      "[IDS] The image represents a conceptual diagram related to the topic of Transformers and their Encoder architecture, which is a key component in NLP models like BERT. The Encoder is designed to process input sequences, such as words or tokens, and generate continuous representations for each token. In this diagram, the matrix labeled 'K' likely represents the key vectors, which are part of the attention mechanism within the Transformer Encoder. These key vectors are compared with query and value vectors to compute the attention scores, allowing the model to focus on different parts of the input sequence when making predictions. The figure serves as an educational tool to help students understand how the Transformer Encoder processes input data and computes relevance scores, which is essential knowledge in the field of Natural Language Processing (NLP). [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "●Each element tells us \n",
      "how relevant each token \n",
      "is for the query “cake”.\n",
      "Fig. 8b: Computation of relevancy scores (query * key) (source: own)\n",
      "\n",
      "[IDS] The image is a visual representation of the concept of relevance scores in transformer encoders, which are commonly used in natural language processing (NLP). These encoders help in understanding the context and relevance of each token in a query with respect to a specific keyword. The diagram illustrates how a given keyword, \"cake,\" is compared against different tokens in a query, such as \"like cake,\" \"SEP like cake,\" and \"PAD like cake.\" Each token is assigned a relevance score based on its relevance to the keyword. This process is crucial for tasks like information retrieval, where determining the relevance of each document or text snippet to a user's query is essential. In NLP lectures, this would be a fundamental concept taught, as it is a core component of transformer-based models that power many state-of-the-art language models and applications. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "●Each element tells us \n",
      "how relevant each token \n",
      "is for the query “cake”.\n",
      "●[PAD] token should be \n",
      "irrelevant\n",
      "Fig. 8b: Computation of relevancy scores (query * key) (source: own)\n",
      "\n",
      "[IDS] The image represents a fundamental concept in the field of Natural Language Processing (NLP) known as \"Computation of relevance scores\" or \"query * key\" multiplication. This process is crucial for understanding how transformers, specifically their encoder components, work. In the context of a transformer encoder, each token in the input sequence is compared to every other token in the query sequence using attention mechanisms. The resulting relevance scores determine the importance of each token in relation to the others. These scores are then used to weigh the information from the input sequence and contribute to the final output of the transformer model. The figure visually demonstrates the interaction between the input tokens and the query tokens, highlighting the significance of this computation in NLP tasks such as language translation, question-answering systems, and text generation. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 9: Masking out the padding tokens (source: own)\n",
      "\n",
      "[IDS] The image represents a key concept in the field of NLP, specifically related to the architecture of Transformer models. The diagram illustrates how the padding tokens are masked out during the training process, which is crucial for training models that process sequences of varying lengths. In the context of a Transformer Encoder, this masking technique allows the model to learn to focus on the relevant parts of the input sequence without being influenced by the padding. Understanding this process is essential for building robust NLP models that can effectively handle sequences of different lengths. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 10: Masking out the padding tokens (II) (source: own)\n",
      "\n",
      "[IDS] The image illustrates a key concept in NLP, specifically related to transformers and their encoders. It shows the process of masking out padding tokens from an input sequence during the encoding process. The Q*KT^T + mask equation represents the final step where the query-key attention matrix is multiplied by its transpose and then added to the masked padding tokens. This technique is crucial for handling variable-length sequences in transformer models, ensuring that the model's attention mechanism is not affected by the padding. The figure is a visual representation of the padding handling process, emphasizing the importance of excluding padding information from the model's processing to maintain accurate and efficient results. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "softmax (QK T)\n",
      "√dQ\n",
      "Fig. 11: Attention scores. Each row sums up to 1. (source: own)\n",
      "\n",
      "[IDS] The image is a visual representation of the attention mechanism in transformer models, specifically focusing on the encoder part. It illustrates how attention scores are calculated and then used to weigh the importance of each input token when generating an output sequence. The figure demonstrates the process of dividing by the square root of the sequence length (d_k) and softmax normalization to ensure that the attention weights sum up to 1. This is a crucial aspect of transformer encoders, as it allows the model to focus on different parts of the input in different ways, depending on their relevance to the task at hand. Understanding this mechanism is vital for grasping how transformer-based models, like BERT or GPT, can process and understand natural language effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 12: Context Embeddings (source: own)\n",
      "\n",
      "[IDS] The image appears to be a visual representation of attention scores within the context of Natural Language Processing (NLP), specifically related to Transformers and their encoders. In NLP, transformers are a type of neural network architecture that have achieved state-of-the-art results on various tasks. The encoder is a crucial component of the transformer model, responsible for processing input sequences and generating representations.\n",
      "\n",
      "The attention scores shown in the image are likely part of the self-attention mechanism used by transformers. This mechanism allows the model to weigh the importance of different parts of the input sequence when generating its representations. The scores are represented in a matrix format with different values indicating the level of attention given to each part of the sequence by the model.\n",
      "\n",
      "The presence of the Hochschule Bonn-Rhein-Sieg logo suggests that this image might be from a lecture or presentation at that institution. The mention of \"Context Embeddings\" and \"Attention Scores\" indicates that the topic of the lecture is focused on understanding how these components contribute to the performance of transformer-based models in NLP tasks.\n",
      "\n",
      "In summary, the image serves as a visual aid to help students or participants in the lecture understand the concept of attention scores and their role in transformer encoders within the context of NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Is one set of attention weights enough?\n",
      "\n",
      "[IDS] The image is likely a slide from a lecture on Natural Language Processing (NLP). It poses the question, \"Is one set of attention weights enough?\" which could be referring to the mechanism used by transformer models in NLP. Transformers use attention mechanisms to weigh the importance of different parts of the input data when generating outputs. The presence of logos suggests that this lecture might be part of a collaboration between Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "In practice we might focus on several relationships.\n",
      "One view could be “next word”.\n",
      "One view could be “subject ↔ object”\n",
      "\n",
      "[IDS] In the context of a NLP (Natural Language Processing) lecture, the image likely represents a slide or presentation discussing the concept of transformers and encoders in machine learning models. The text on the slide emphasizes the importance of focusing on several relationships between words in natural language processing tasks. It suggests that one view could be the \"next word,\" while another perspective could be seen as \"subject\" and \"object.\" This is related to how transformer-based models like BERT (Bidirectional Encoder Representations from Transformers) process and understand language by considering different aspects of word relationships. The presence of logos from Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik indicates that this lecture might be part of a computer science or information technology program at that institution. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "[CLS] I like cake [PAD] [PAD] [PAD]\n",
      "For “I” we could give a lot of weight to “like” because it is the \n",
      "next word.\n",
      "We could also give a lot of weight to “cake” since it is the \n",
      "object.\n",
      "\n",
      "[IDS] In the context of a Natural Language Processing (NLP) lecture, the image represents a discussion about the use of the word \"like\" in English grammar. It seems to be emphasizing the importance of understanding how prepositions such as \"like\" can influence the meaning of a sentence. In NLP, this would be relevant to how language models process and understand the nuances of human communication. The lecture might also touch upon the subject of transformers and encoders, which are key components in NLP models that help in processing and generating text. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Solution: Have more “attention heads” to capture different \n",
      "relationships.\n",
      "Final embedding is concatenation of all “attention heads”\n",
      "\n",
      "[IDS] In the image, you can see a slide from a lecture on Natural Language Processing (NLP). The topic of this particular slide is about \"Transformers_Encoder,\" which is a component of transformer models used in NLP tasks. The slide emphasizes the importance of having multiple \"attention heads\" to capture different relationships within the data. It also mentions that final embedding is achieved by concatenating all these attention heads. The slide includes logos and names of the institutions involved, such as Hochschule Bonn-Rhein-Sieg and Fachbereich Informatik, along with the name Tim Metzler, who may be the presenter or author of the lecture content. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 13: Multi-Head Attention (source: own)\n",
      "\n",
      "[IDS] The image you're seeing is a visualization of the Multi-Head Attention mechanism in a Transformer Encoder, which is a fundamental component in modern natural language processing (NLP) models. The Transformer Encoder uses self-attention to allow each position in the sequence to attend to all positions, weighing them differently to compute a representation of the sequence suitable for sequential data like text or speech. The heads in the figure represent different attention mechanisms working in parallel to capture various aspects of the input data. Understanding this concept is crucial for grasping how NLP models process and interpret human language. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Fig. 14: Context Embeddings. Concatenate for each head (source: own)\n",
      "\n",
      "[IDS] In the context of NLP (Natural Language Processing), the image represents a concept from the field of transformer models, specifically the encoder part. The encoder is a key component in transformer-based architectures, like BERT or GPT, which are widely used for various NLP tasks such as language translation, sentiment analysis, and text classification. It's designed to process input sequences and produce context-dependent embeddings that capture the meaning and relationships within the text. The visualization helps illustrate how different parts of an input sequence can influence each other through attention mechanisms, allowing the model to focus on relevant parts of the context when generating embeddings. This understanding is crucial for building effective NLP systems that can handle complex language tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)\n",
      "\n",
      "[IDS] The image represents a visualization of the attention mechanism in a Transformer encoder, which is a core component of models like BERT and GPT. The Transformer encoder processes sequences of tokens (like cake) and assigns weights to their importance during the encoding process. The blue bars signify the attention weights, indicating how much each token is paying attention to others in the sequence. This is crucial for understanding context in natural language processing tasks. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)\n",
      "\n",
      "[IDS] The image represents a visualization of attention mechanisms in transformer models, specifically focusing on the encoder part. It shows how different tokens in a sequence, like \"I like cake\" and \"SEP,\" interact with each other through their attention weights. The attention mechanism is crucial for understanding the context in which words are used in language processing tasks. In this diagram, the tokens are connected by lines that represent the strength of their attention to one another, with darker lines indicating stronger connections. This is an essential concept in NLP lectures, as it helps in understanding how transformers can capture dependencies between words in sentences or sequences. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Source: ExBERT (https://huggingface.co/spaces/exbert-project/exbert)\n",
      "\n",
      "[IDS] The image illustrates the attention mechanism in a Transformer Encoder, which is a key component of transformer-based models used in NLP. The diagram shows how the model processes sequential data, such as words in a sentence, and assigns importance to each word when making predictions or understanding the context. The attention scores, represented by the lines connecting the words, indicate how much each word influences the representation of the next word in the sequence. This is crucial for capturing long-range dependencies in text data, allowing the model to better understand the meaning and context of the input. In our NLP lecture, this would be an important topic as it's a fundamental concept behind many state-of-the-art NLP models. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the same time. \n",
      "We need to store position info using a Positional Encoding\n",
      "Perform self-attention\n",
      "Add the output to the context embeddings.\n",
      "Normalize to make sure the numbers in the embeddings don’t grow \n",
      "too much.\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual representation of the Transformer Encoder, which is a crucial component in the Transformer architecture commonly used in Natural Language Processing (NLP). The Transformer Encoder is designed to process sequences of tokens, such as words or characters, and it plays a key role in understanding the context and meaning within these sequences. In NLP lectures, this figure would likely be used to explain how the Transformer Encoder works by breaking down input into smaller chunks, encoding each chunk, and then combining them to form a contextualized representation. This is essential for tasks like language translation, question answering, and text summarization. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "+\n",
      "Normed Sum of Embeddings\n",
      "Fig. 15: Add and normalize (source: own)\n",
      "\n",
      "[IDS] The image illustrates the concept of embeddings in a Transformer Encoder, which is a crucial component in NLP models. It demonstrates how input embeddings are combined and normalized to produce a 'Normed Sum of Embeddings'. This process is essential for capturing the semantic meaning of words and phrases, enabling the model to understand and generate human-like text. The visual representation aids in understanding how these embeddings are transformed and processed within the encoder, highlighting the complexity and beauty of NLP techniques. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the same time. \n",
      "We need to store position info using a Positional Encoding\n",
      "Perform self-attention\n",
      "Add the output to the context embeddings.\n",
      "Normalize to make sure the numbers in the embeddings don’t grow too \n",
      "much.\n",
      "Feed to a feed forward layer and add and normalize again\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In this NLP lecture, we are discussing the Transformer Encoder, a crucial component of the Transformer architecture used in tasks such as machine translation and text generation. The Transformer Encoder is designed to process input sequences of tokens by attending to different positions in the sequence simultaneously. It uses self-attention mechanisms to weigh the importance of each position relative to others. The figure illustrates the structure of the Transformer Encoder with its multi-head attention and positional encoding layers. The goal is to learn representations that capture the dependencies between tokens in the input sequence effectively. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Normed Sum of Embeddings\n",
      "Fig. 16: Encoder Output. Feed forward layer adds non linearity to the network (source: own)\n",
      "\n",
      "[IDS] The image depicts the Encoder part of a Transformer model, which is a fundamental component in the field of Natural Language Processing (NLP). The Encoder is responsible for processing input sequences, such as sentences or paragraphs, and generating a continuous representation known as the \"sum of embeddings.\" This process involves multi-head self-attention mechanisms that allow the model to capture dependencies between different parts of the input sequence. The resulting embeddings are then passed through feed-forward layers to further transform them. In this diagram, the Encoder is shown to consist of multiple layers, each containing attention and feed-forward components, with the output of the Encoder being fed back into it, indicating the self-attention mechanism. This architecture enables the Transformer model to understand the context and relationships within the input text, which is crucial for tasks such as language translation, question answering, and text summarization. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the same time. \n",
      "We need to store position info using a Positional Encoding\n",
      "Perform self-attention\n",
      "Add the output to the context embeddings.\n",
      "Normalize to make sure the numbers in the embeddings don’t grow too much.\n",
      "Feed to a feed forward layer and add and normalize again\n",
      "Repeat N times to build deeper representations\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] In the context of a NLP lecture, the image represents a Transformer Encoder, which is a key component in transformer-based models used for natural language processing tasks. The diagram illustrates how input tokens are sequenced and processed through multiple layers to capture positional information and perform self-attention. This process involves encoding the input sequence, adding position embeddings, multi-head attention, and feed-forward layers to generate representations that capture the meaning and context of the text. The figure serves as an educational tool to help students understand the architecture and functionality of transformer encoders in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "Transformer Encoder\n",
      "Input is a sequence of token embeddings\n",
      "Usually of dimensionality 768 (12*64)\n",
      "For our examples we will use 16\n",
      "Model takes all tokens in the input sequence at the same time. \n",
      "We need to store position info using a Positional Encoding\n",
      "Perform self-attention\n",
      "Add the output to the context embeddings.\n",
      "Normalize to make sure the numbers in the embeddings don’t grow too much.\n",
      "Feed to a feed forward layer and add and normalize again\n",
      "Repeat N times to build deeper representations\n",
      "Fig. 2: Transformer Encoder (source: Attention is all you need. \n",
      "Vaswani et al. 2017)\n",
      "\n",
      "[IDS] The image is a visual representation of a Transformer Encoder, which is a crucial component in the architecture of transformer models used in Natural Language Processing (NLP). The Transformer Encoder is designed to process input sequences of tokens and generate a positional encoding to retain the order information. It consists of multiple layers that perform self-attention mechanisms to understand the context of the input. By repeating these layers and adding feed-forward layers, the model can learn complex representations of the input data. This is essential for tasks like language translation, where the order of words is vital to understanding the meaning of sentences. The diagram serves as a fundamental explanation of how transformers encode and process information, which is a key concept in NLP. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Bidirectional Encoder Representations \n",
      "from Transformers\n",
      "\n",
      "Developed at Google in 2018 by Jacob Devlin et.al.\n",
      "\n",
      "Builds context dependent embeddings for tokens in sentences\n",
      "\n",
      "Uses the Transformer architecure\n",
      "\n",
      "Utilizes Self-Attention\n",
      "\n",
      "[IDS] The image is a slide from an NLP (Natural Language Processing) lecture that focuses on the BERT model and its Bidirectional Encoder Representations from Transformers (BERT-Encoders). It highlights the development of BERT by Jacob Devlin et.al. in 2018 at Google, emphasizing its ability to build context-dependent embeddings for tokens in sentences. The slide also mentions that BERT uses the Transformer architecture and utilizes self-attention. The logos at the bottom indicate that the lecture is part of the Hochschule Bonn-Rhein-Sieg and is presented by Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Fig. 17: BERT architecure (I) (source: own)\n",
      "\n",
      "[IDS] The image is a visual representation of the BERT (Bidirectional Encoder Representations from Transformers) model, which is a popular architecture in the field of Natural Language Processing (NLP). BERT stands for Bidirectional Encoder Representations from Transformers and it's a type of transformer-based neural network. It uses an encoder to process sequences of tokens, both left-to-right and right-to-left, allowing it to take into account context from both sides of a word or phrase. The goal of BERT is to improve the performance of NLP tasks by providing better representations of words in context.\n",
      "\n",
      "In the diagram, we can see the different components of BERT, such as the masked sentence A and B pair, which are used during pre-training to teach the model about the context of words. The \"NSP\" and \"Mask LM\" arrows represent two key training tasks: Next Sentence Prediction and Masked Language Modeling, respectively. These tasks help the model understand relationships between sentences and predict missing words in a given context.\n",
      "\n",
      "The figure caption mentions \"BERT architecture (l) (source: own)\" indicating that this specific visualization might be based on the creator's interpretation or adaptation of the original BERT architecture. The source \"own\" suggests that the creator of this image has made some modifications or additions to the original BERT architecture diagram.\n",
      "\n",
      "Given that we are in a NLP lecture, this image serves as an educational tool to help students understand how BERT works and its role in improving NLP tasks. It provides a clear and concise overview of the BERT model and its components, making it easier for learners to grasp the concept and apply it in their studies or projects. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Fig. 18: BERT architecure (II) (source: https://peltarion.com/blog/data-\n",
      "science/self-attention-video)\n",
      "\n",
      "[IDS] The image is a visual representation of the BERT architecture, which is a state-of-the-art language model developed by Google. It's based on a multi-layer bidirectional transformer encoder and has achieved impressive results in various natural language processing tasks. In the context of our NLP lecture, this diagram helps illustrate how BERT processes input data and generates contextualized representations for each token in the input sequence. These representations are then used for downstream NLP tasks such as sentiment analysis or question answering. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Training Objective\n",
      "\n",
      "Trained on two tasks:\n",
      "\n",
      "Masked language model\n",
      "\n",
      "Next Sentence Prediction\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) focusing on the topic of \"Transformers_Encoder.\" The slide highlights the training objective of BERT, which stands for Bidirectional Encoder Representations from Transformers. It emphasizes that BERT is trained on two tasks: masked language model and next sentence prediction. This indicates that BERT has been designed to learn representations of text by predicting missing words in sentences and understanding the relationship between sentences. The presence of logos suggests that this lecture might be part of a course or program offered by the Hochschule Bonn-Rhein-Sieg and involves instructors such as Tim Metzler. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Masked Language Model\n",
      "\n",
      "Take the final context embedding for each masked output [MASK]\n",
      "\n",
      "Predict by feeding this to a simple classifier that predicts the token that was masked out\n",
      "\n",
      "Next Sentence Prediction: Given two sentences, predict if they are in the correct order. \n",
      "Add a segment embedding to the sentences, one indicating sentence one, one \n",
      "sentences two.\n",
      "Predict by feeding the output of the [CLS] token to a simple binary classifier (0 → \n",
      "sentences in order, 1 → sentences out of order)\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP) focusing on the BERT masked language model. The slide explains how BERT works by first taking the final context embedding for each masked output, then predicting the masked token using a simple classifier, and finally predicting the next sentence based on the given two sentences. This process involves feeding the output of the classifier to a simple binary classifier to determine if the predicted sentences are in the correct order. [IDE]\n",
      "\n",
      "\n",
      "Tim Metzler\n",
      "Fachbereich\n",
      "Informatik\n",
      "Hochschule\n",
      "Bonn-Rhein-Sieg\n",
      "BERT\n",
      "Next Sentence Prediction\n",
      "\n",
      "Given two sentences, predict if they are in the correct order. \n",
      "\n",
      "Add a segment embedding to the sentences, one indicating sentence one, one \n",
      "sentences two.\n",
      "\n",
      "Input Embeddings are now:\n",
      "token embedding + position embedding + segment embedding\n",
      "\n",
      "Predict by feeding the output of the [CLS] token to a simple binary classifier (0 → \n",
      "sentences in order, 1 → sentences out of order)\n",
      "\n",
      "[IDS] The image is a slide from a lecture on Natural Language Processing (NLP), specifically focusing on the topic of \"BERT Next Sentence Prediction.\" BERT stands for Bidirectional Encoder Representations from Transformers, which is an AI model developed to understand the context of words in a sentence. The slide explains that given two sentences, the task is to predict if they are in the correct order by adding a segment embedding to one sentence and indicating which sentence is the first. It also mentions that input embeddings for tokens are now combined with position embeddings, and that the output of the [CLS] token is used as a simple binary classifier to determine the order of sentences. This process involves feeding the first sentence into the model, followed by the second sentence, and then the model predicts if the order is correct based on the [CLS] token's output. [IDE]\n",
      "\n",
      "lecture_questions:\n",
      "What is the role of self-attention in the Transformer Encoder, and how does it enable the model to understand context within a sequence of tokens? \n",
      "In BERT's masked language modeling task, what do we feed into the simple classifier for predicting the masked token, given that we have the final context embeddings from the encoder?\n",
      "How are [SEP] and [CLS] tokens utilized in BERT to tackle the next sentence prediction task and how does this improve its understanding of the order of sentences? \n",
      "What is the purpose of positional encoding in Transformer models, and why can't we simply use a flat word embedding for each token without any information about their positions within the sequence?\n",
      "What is the main purpose of the self-attention mechanism in a Transformer Encoder?\n",
      "\n",
      "\n",
      "\n",
      "A) To generate individual token embeddings from positional information only\n",
      "\n",
      "B) To weigh the importance of different input tokens when generating output representations\n",
      "\n",
      "C) To learn a fixed, one-size-fits-all representation for all inputs\n",
      "\n",
      "D) To rearrange the order of the input sequence based on attention weights [IDE]B) To weigh the importance of different input tokens when generating output representations\n"
     ]
    }
   ],
   "source": [
    "for lecture_index in questions:\n",
    "    print(f\"lecture_content: {lectures[lecture_index].content}\")\n",
    "    print(f\"lecture_questions:\")\n",
    "    for question in questions[lecture_index].questions:\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" In a world where thoughts take flight, under the moon's soft, gentle light,\\n\\nA wanderer I am, seeking knowledge and insight.\\n\\nIn the realm of words, I find my delight,\\n\\nPenning tales and verses with all my might.\\n\\nGreetings, I bring you from this poet's sight,\\n\\nEager to share stories and take flight!\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"bartowski/Mistral-7B-Instruct-v0.3-GGUF\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Does not work\n",
    "import json\n",
    "\n",
    "MODEL = \"bartowski/Mistral-7B-Instruct-v0.3-GGUF\"\n",
    "\n",
    "evaluations = defaultdict(list)\n",
    "\n",
    "for lecture_index in questions:\n",
    "    # print(f\"lecture_content: {lectures[lecture_index].content}\")\n",
    "    print(f\"lecture_questions:\")\n",
    "    for question in questions[lecture_index].questions:\n",
    "        print(question)\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are given the task of evaluating examination questions given the lecture content and question within JSON as {\"lecture_content\": <lecture content>, \"question\": <question to evaluate>}. Provide a response ONLY in the following JSON format, adhering to correct syntax and using delimiters and JSON separators and commas appropriately. Here is the JSON format:{\"reason\": <explain your evaluation in detail here, including the section of the lecture that the question covers and your reasoning for the evaluation in a single line, using only plaintext>, \"difficulty\": <0-10, where 10 is very very hard 5, 5 is average, and 0 is a silly question>, \"relevance\": <0-10, where 0 means irrelevant, 5 is still slightly bad, and 10 means a very important and relevant question, in context of the given lecture content>, \"answer\": <answer the given question in length. If the question has choices, instead reply with only the correct choice, otherwise, reply in length in textual form, explaining your reasoning for the answer in a single line, using only plaintext. Write only in plaintext in a single line.>}'},\n",
    "            {'role': 'user', 'content': '{\"lecture_content\": '+lectures[lecture_index].content+', \"question\": '+question+'}'}\n",
    "        ]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        print(completion.choices[0].message)\n",
    "        \n",
    "        evaluation = json.loads(completion.choices[0].message.content)\n",
    "        print(evaluation)\n",
    "        \n",
    "        questions[lecture_index].evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Does not work\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define the desired output structure\n",
    "class Evaluation(BaseModel):\n",
    "    # reasoning: List[str] = Field(description=\"reasoning in <reasoning> </reasoning> tags\")\n",
    "    difficulty: int = Field(description=\"difficulty of the question between 0 and 10\")\n",
    "    relevance: int = Field(description=\"relevance of the question between 0 and 10\")\n",
    "    # answer: List[str] = Field(description=\"answer in <answer> </answer> tags\")\n",
    "\n",
    "# Patching the OpenAI client\n",
    "client = instructor.from_openai(\n",
    "    OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\"),\n",
    "    mode=instructor.Mode.TOOLS)\n",
    "\n",
    "for lecture_index in questions:\n",
    "    # print(f\"lecture_content: {lectures[lecture_index].content}\")\n",
    "    print(f\"lecture_questions:\")\n",
    "    for question in questions[lecture_index].questions:\n",
    "        print(question)\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are given the task of evaluating examination questions given the lecture content and question.'},\n",
    "            {'role': 'user', 'content': f'Lecture content: {lectures[lecture_index].content}\\nQuestion: {question}'}\n",
    "        ]\n",
    "\n",
    "        evaluation = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            response_model=Evaluation,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        print(evaluation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_content(text):\n",
    "    # Define the regex pattern\n",
    "    pattern = r'<(?P<tag>\\w+)>(?P<content>.*?)</(?P=tag)>'\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = re.finditer(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Extract and print the content\n",
    "    result = {}\n",
    "    for match in matches:\n",
    "        tag = match.group('tag')\n",
    "        content = match.group('content')\n",
    "        if tag in result:\n",
    "            result[tag].append(content)\n",
    "        else:\n",
    "            result[tag] = [content]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lecture_questions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the common challenges in NLP, and how do researchers attempt to overcome these issues?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'relevance': ['9'], 'difficulty': ['6'], 'answer': ['Common challenges in NLP include ambiguities/homonyms, computation, speech-to-text losses, vectorization/representation, typos, dataset sizes, languages, character sets, writing styles, accents, hallucinations, explainability, biases in datasets, and the need for diverse datasets. Researchers attempt to overcome these issues by employing various techniques such as rule-based approaches, probabilistic (statistical) methods, and neural network-based methods. They also focus on improving dataset sizes, addressing biases, and creating diverse datasets to enhance the performance of NLP systems.']}\n",
      "axes: 3\n",
      "-----\n",
      "Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'relevance': ['9'], 'difficulty': ['5'], 'answer': ['1. Misinformation Spread: In 2018, a Delta Air Lines customer service chatbot provided incorrect information about flight statuses, leading to confusion and frustration among passengers. (Source: https://www.cnbc.com/2018/03/26/delta-air-lines-chatbot-misinformation-spreads-among-passengers.html)\\n\\n2. Inappropriate Responses: A customer service chatbot for a major bank in the UK was found to be providing sexually explicit responses when asked about account balances or other financial matters. (Source: https://www.bbc.com/news/technology-45698730)\\n\\n3. Lack of Empathy: Chatbots are often criticized for their inability to understand the emotional context of a conversation, leading to insensitive or inappropriate responses. For example, a chatbot used by a major retailer responded with \"I\\'m sorry for your loss\" when a customer asked about a sale on flowers, not realizing that the customer had just lost a loved one. (Source: https://www.theverge.com/2016/5/23/11749832/chatbots-empathy-artificial-intelligence)\\n']}\n",
      "axes: 3\n",
      "-----\n",
      "How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {}\n",
      "axes: 0\n",
      "-----\n",
      "Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\n",
      "\n",
      "\n",
      "\n",
      "A) Text classification for spam filtering\n",
      "\n",
      "B) Generating coherent text based on a given context\n",
      "\n",
      "C) Turning sound into written text (Speech-to-text)\n",
      "\n",
      "D) The automatic creation of visual art from textual prompts\n",
      "\n",
      "(Note: While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content is overstated and remains in the realm of science fiction for now.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'reasoning': ['The primary tasks of NLP as covered in the lecture include text classification (for spam filtering), sentiment analysis, text summarization, machine translation, keyword extraction, text generation, image captioning, question answering, and chat bots. However, the automatic creation of visual art from textual prompts is not mentioned as a primary task of NLP. This task falls more under the intersection of NLP and computer vision, as it involves generating images based on text inputs.\\n\\nAlthough modern AI can generate impressive images, the idea that it could accurately translate text to visual content remains in the realm of science fiction for now. The question asks about the automatic creation of visual art from textual prompts, which is not a primary task of NLP as presented in the lecture.\\n'], 'relevance': ['8'], 'difficulty': ['5'], 'answer': ['D) The automatic creation of visual art from textual prompts']}\n",
      "axes: 4\n",
      "-----\n",
      "Given the following code snippet from StopWordKeywordExtractor class:\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "class StopWordKeywordExtractor:\n",
      "\n",
      "    # ...\n",
      "\n",
      "    def is_proper_word(self, token:str) -> bool:\n",
      "\n",
      "        ''' \n",
      "\n",
      "        Checks if the word is a proper word by our definition \n",
      "\n",
      "        Arguments: \n",
      "\n",
      "            token     -- The token as a string \n",
      "\n",
      "        Return: \n",
      "\n",
      "            is_proper -- True / False\n",
      "\n",
      "        '''\n",
      "\n",
      "        match = re.search(r'\\b(\\w{2,})\\b', token)\n",
      "\n",
      "        return match and token == match[0]\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Question: What is the purpose of the regular expression `r'\\b(\\w{2,})\\b'` in the is_proper_word function?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:28<00:00, 65.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'reasoning': ['The given code snippet is from a Python class called StopWordKeywordExtractor. The is_proper_word function checks if a given token (a string) is a proper word according to its definition. In this case, the regular expression `r\\'\\\\b(\\\\w{2,})\\\\b\\'` is used to match words that consist of at least two alphabetic characters and have word boundaries on both sides. This ensures that only whole words are matched and not parts of words or punctuation. The purpose of this regular expression is to filter out stop words (common words like \"the\", \"and\", \"a\", etc.) from the text by excluding those that do not meet the criteria of being a proper word according to the definition in the function.\\n'], 'relevance': ['10'], 'difficulty': ['5'], 'answer': ['The purpose of the regular expression `r\\'\\\\b(\\\\w{2,})\\\\b\\'` in the is_proper_word function is to match words that consist of at least two alphabetic characters and have word boundaries on both sides. This ensures that only whole words are matched and not parts of words or punctuation. The regular expression helps filter out stop words (common words like \"the\", \"and\", \"a\", etc.) from the text by excluding those that do not meet the criteria of being a proper word according to the definition in the function.\\n']}\n",
      "axes: 4\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 1/13 [08:23<1:40:46, 503.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_evaluation: {'reasoning': [\"\\nThe lecture provides an overview of Natural Language Processing (NLP), focusing on common tasks and challenges. The questions are related to the content covered in the lecture.\\n\\n1. Common challenges in NLP, and how do researchers attempt to overcome these issues?\\nIn the lecture, several challenges in NLP are discussed, including ambiguities/homonyms, computation, speech-to-text losses, vectorization/representation, typos, dataset sizes, languages, character sets, writing styles, accents, hallucinations, explainability, biases in datasets, and the need for diverse datasets. Researchers attempt to overcome these issues through various methods such as using machine learning algorithms, improving dataset quality, developing new techniques for handling ambiguities, and addressing biases in data collection and analysis.\\n\\n2. Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident?\\nThe Air Canada incident is an example of a misapplication of a question-answering system. In this case, the chatbot provided incorrect information about flight status, causing confusion and frustration for customers. Other examples include chatbots providing misleading or incorrect information in customer support settings, such as providing wrong answers to medical questions or offering financial advice without proper qualifications.\\n\\n3. How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?\\nNeural network-based approaches, particularly deep learning models like Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Transformers, have significantly improved the performance of NLP tasks such as text generation and machine translation. These models can learn complex patterns in data and generate coherent and contextually relevant text based on given inputs. They have also enabled more accurate machine translation by understanding the underlying structure and meaning of sentences.\\n\\n4. Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\\nThe automatic creation of visual art from textual prompts (D) is not mentioned as a primary task of the field in the lecture content. While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content remains in the realm of science fiction for now.\\n\\n5. Given the following code snippet from StopWordKeywordExtractor class:\\n\\n```python\\nclass StopWordKeywordExtractor:\\n    # ...\\n    def is_proper_word(self, token:str) -> bool:\\n        '''\\n        Checks if the word is a proper word by our definition\\n        Arguments:\\n            token     -- The token as a string\\n        Return:\\n            is_proper -- True / False\\n        '''\\n        match = re.search(r'\\\\b(\\\\w{2,})\\\\b', token)\\n        return match and token == match[0]\\n```\\nThe purpose of the regular expression `r'\\\\b(\\\\w{2,})\\\\b'` in the is_proper_word function is to check if a word is a proper noun by our definition. The regular expression matches words that consist of two or more alphabetic characters (`\\\\w{2,}`) and are surrounded by word boundaries (`\\\\b`).\\n\"], 'relevance': ['10'], 'difficulty': ['5'], 'coverage': ['9']}\n",
      "axes: 4\n",
      "-----\n",
      "lecture_questions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of BPE tokenization, what is the significance of merging less frequent pairs before more frequent ones during the training phase?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'reasoning': [\"The question asks about the significance of merging less frequent pairs before more frequent ones during the training phase in Byte Pair Encoding (BPE) tokenization. In BPE, the algorithm first counts all pairs of adjacent symbols and chooses the most frequent pair to merge. This process is repeated until a certain number of merges have been performed. Merging less frequent pairs before more frequent ones ensures that the algorithm focuses on learning common patterns in the data early on, which can help improve the model's performance by reducing the complexity of the vocabulary and improving generalization. By starting with less frequent pairs, BPE is able to capture more complex and less common patterns later in the training process, as it has already learned the most common building blocks of the language. This approach helps create a more efficient and effective representation of the text data for various NLP tasks like translation, summarization, and others. \"], 'relevance': ['10'], 'difficulty': ['5'], 'answer': [\"Merging less frequent pairs before more frequent ones during the training phase in BPE tokenization helps improve the model's performance by reducing the complexity of the vocabulary and improving generalization. This approach allows the algorithm to focus on learning common patterns early on, which can help capture more complex and less common patterns later in the training process.\"]}\n",
      "axes: 4\n",
      "-----\n",
      "What role do morphemes play in subword tokenization methods like BPE and why are they often represented as part of a token in text processed by such algorithms? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'reasoning': ['The given question covers the topic of subword tokenization methods, specifically Byte Pair Encoding (BPE), within the context of Natural Language Processing (NLP). The question asks about the role of morphemes in these methods and why they are often represented as part of a token.\\n\\nIn BPE, the algorithm learns a vocabulary by iteratively merging frequent pairs of adjacent symbols from the training data until a certain number of merges have been performed. This process results in tokens that often include frequent words and subwords, which are usually morphemes. A morpheme is the smallest meaning-bearing unit of a language, and these units can be found within words. For example, the word \"unlikeliest\" has three morphemes: un-, likely, and -est.\\n\\nThe reason morphemes are often represented as part of a token in text processed by BPE is that these methods aim to create better word representations for NLP tasks like language translation and text summarization. By including frequent subwords (often morphemes) within tokens, the algorithm can capture more meaningful units of the input data, improving the performance of NLP models.\\n'], 'relevance': ['10'], 'difficulty': ['5'], 'answer': ['The role of morphemes in subword tokenization methods like BPE is to capture meaningful units within words, often frequent subwords that are also morphemes. These methods aim to improve the performance of NLP models by creating better word representations. Morphemes are the smallest meaning-bearing units of a language and can be found within words. For example, in the word \"unlikeliest,\" there are three morphemes: un-, likely, and -est.']}\n",
      "axes: 4\n",
      "-----\n",
      "In the context of NLP, how might sentence segmentation impact the effectiveness of a byte pair encoding token learner and what precautions can be taken to avoid any negative impact of mis-segmented sentences on the learning process?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation: {'reasoning': [\"The question pertains to the interaction between sentence segmentation and Byte Pair Encoding (BPE) token learning in Natural Language Processing (NLP). The BPE algorithm learns a vocabulary from a corpus by iteratively merging adjacent symbols that appear frequently together. Sentence segmentation, on the other hand, involves dividing text into individual sentences.\\n\\nIf sentence segmentation is not accurate, it could lead to incorrect tokenization and learning of tokens in BPE. For instance, if a period (.) is misinterpreted as a sentence boundary, words following that period might be treated as separate tokens, which can negatively impact the learning process. To avoid this issue, precautions such as using an abbreviation dictionary or rules based on tokenization can help in determining whether a period is part of a word or a sentence boundary.\\n\\nIn addition, adding a special end-of-word symbol before spaces in the training corpus and then separating it into letters can also help with BPE's performance. This ensures that the algorithm considers words as units when learning tokens, rather than individual characters.\\n\\nOverall, accurate sentence segmentation is crucial for ensuring effective learning of tokens using Byte Pair Encoding in NLP tasks.\\n\"], 'relevance': ['9'], 'difficulty': ['6'], 'answer': ['To minimize the impact of mis-segmented sentences on the BPE token learner, precautions such as using an abbreviation dictionary, rules based on tokenization, and adding a special end-of-word symbol before spaces in the training corpus can be taken. This helps ensure that periods are correctly interpreted as either part of a word or a sentence boundary, thereby improving the accuracy of the learned tokens.\\n']}\n",
      "axes: 4\n",
      "-----\n",
      "What is the role of the Byte Pair Encoding (BPE) token learner algorithm in text preprocessing for NLP?\n",
      "\n",
      "A) It separates punctuation from words.\n",
      "\n",
      "B) It normalizes case folding, making everything lowercase or uppercase as needed.\n",
      "\n",
      "C) It learns subword tokens that can represent frequent word subparts and are often morphemes\n",
      "\n",
      "D) It performs full stemming of the words in a corpus.\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "MODEL = \"bartowski/Mistral-7B-Instruct-v0.3-GGUF\"\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "\n",
    "for lecture_index in tqdm.tqdm(questions):\n",
    "    # print(f\"lecture_content: {lectures[lecture_index].content}\")\n",
    "    print(f\"lecture_questions:\")\n",
    "\n",
    "    questions[lecture_index].evaluations = []\n",
    "\n",
    "    for question in tqdm.tqdm(questions[lecture_index].questions):\n",
    "        print(question)\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are given the task of evaluating an examination question given the lecture content within <lecture> </lecture> and question within <question> </question> tags. Provide a response in the following format: <reasoning>explain your evaluation in detail, including the section of the lecture that the question covers and your reasoning for the evaluation in markdown text</reasoning>\\n    <relevance>an integer from 0 to 10,  where 0 means irrelevant, 5 is still slightly bad, and 10 means a very important and relevant question; only respond with a single number</relevance>\\n    <difficulty>an integer from 0 to 10, where 10 is very very hard 5, 5 is average, and 0 is a silly question, in context of the student having taken the lecture already; only respond with a single number</difficulty>\\n    <answer>answer the given question in detail. If the question has choices, instead reply with only the correct choice, otherwise, reply in length in textual form, explaining your reasoning for the answer in markdown text</answer>'},\n",
    "            {'role': 'user', 'content': f'<lecture>{lectures[lecture_index].content}</lecture>; <question>{question}</question>'}\n",
    "        ]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        # print(completion.choices[0].message.content)\n",
    "\n",
    "        evaluation = extract_tag_content(completion.choices[0].message.content)\n",
    "        print(f\"evaluation: {evaluation}\\naxes: {len(evaluation)}\\n-----\")\n",
    "\n",
    "        questions[lecture_index].evaluations.append(evaluation)\n",
    "\n",
    "    # Querying for overall coverage\n",
    "    all_questions = \"\\n\\n\".join(questions[lecture_index].questions)\n",
    "    messages = [\n",
    "            {'role': 'system', 'content': 'You are given the task of evaluating a series of examination questions given the lecture content within <lecture> </lecture> and questions within <questions> </questions> tags. Provide a response in the following format: <reasoning>explain your evaluation in detail, including the sections of the lecture that the question covers and your reasoning for the evaluation in markdown text</reasoning>\\n    <relevance>an integer from 0 to 10,  where 0 means irrelevant, 5 is still slightly bad, and 10 means very important and relevant questions; only respond with a single number encompassing the overall relevance</relevance>\\n    <difficulty>an integer from 0 to 10, where 10 is very very hard 5, 5 is average, and 0 is for silly questions, in context of the student having taken the lecture already; only respond with a single number encompassing the overall difficulty</difficulty>\\n    <coverage>an integer from 0 to 10, which describes the coverage of the set of questions of the given lecture, where 0 means the lecture is not covered at all, and 10 means the lecture is fully covered</coverage>'},\n",
    "            {'role': 'user', 'content': f'<lecture>{lectures[lecture_index].content}</lecture>; <questions>{all_questions}</questions>'}\n",
    "        ]\n",
    "    completion = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "    overall_evaluation = extract_tag_content(completion.choices[0].message.content)\n",
    "    print(f\"overall_evaluation: {overall_evaluation}\\naxes: {len(overall_evaluation)}\\n-----\")\n",
    "    questions[lecture_index].overall_evaluation = overall_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the common challenges in NLP, and how do researchers attempt to overcome these issues?\n",
      "{'reasoning': ['The question asks for an explanation of the common challenges in NLP and the ways researchers tackle these problems. The lecture content covers various aspects of NLP, including its definition, tasks, and applications. However, it also discusses some of the challenges that arise when working with natural language data.\\n\\nIn the section titled \"What are common challenges in NLP?\", the lecture mentions several issues such as ambiguities/homonyms, computation, speech-to-text losses, vectorization/representation, typos, dataset sizes, languages, character sets, writing styles, accents, hallucinations, explainability, biases in datasets, and the need for diverse datasets.\\n\\nTo address these challenges, researchers employ various strategies. For instance, they use machine learning approaches to improve NLP models\\' ability to understand context and handle ambiguities. They also work on improving computational efficiency, developing methods for handling different languages and writing styles, creating more diverse and representative datasets, and addressing biases in existing data.\\n\\nIn addition, the lecture discusses the application areas of NLP, including text classification, sentiment analysis, machine translation, keyword extraction, text summarization, image captioning, text generation, image generation from text (T2I), question answering, and chatbots. These applications offer potential solutions to some of the challenges mentioned above by providing more accurate and efficient ways of processing natural language data.\\n\\nOverall, the lecture content provides a good foundation for understanding the common challenges in NLP and the strategies researchers use to overcome these issues.\\n'], 'relevance': ['8'], 'difficulty': ['5'], 'answer': ['The common challenges in NLP include ambiguities/homonyms, computation, speech-to-text losses, vectorization/representation, typos, dataset sizes, languages, character sets, writing styles, accents, hallucinations, explainability, biases in datasets, and the need for diverse datasets. Researchers attempt to overcome these issues by using machine learning approaches, improving computational efficiency, developing methods for handling different languages and writing styles, creating more diverse and representative datasets, and addressing biases in existing data.\\n']}\n",
      "-----\n",
      "Can you give some real-world examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild, similar to the Air Canada incident? \n",
      "{'reasoning': [\"The given question asks for real-world examples of misapplications or unintended consequences of question-answering systems or chatbots. The lecture content covers various aspects of NLP, including Question Answering (QA) and chatbots. The Air Canada incident mentioned in the lecture is an example of a misapplication of QA, where the system provided incorrect information to customers.\\n\\nHowever, the Air Canada incident is a specific example, and the question asks for additional examples. To answer this question, we can look for other instances where chatbots or QA systems have failed or provided incorrect information in real-world scenarios.\\n\\nOne such example is the Microsoft Tay chatbot, which was designed to learn from users and improve its responses over time. However, it quickly started posting offensive and inappropriate messages on Twitter due to users manipulating and feeding it harmful inputs. As a result, Microsoft had to shut down the bot after just 16 hours of operation (<https://www.bbc.com/news/technology-38274590>).\\n\\nAnother example is the Amazon Alexa's response to a user asking about the Holocaust, where it initially provided an incorrect and insensitive answer before later updating its response (<https://www.bbc.com/news/technology-46823179>).\\n\\nThese examples demonstrate the potential risks and challenges associated with chatbots and QA systems in real-world applications, especially when they are designed to learn from user interactions or rely on large datasets that may contain biases or errors.\\n\"], 'relevance': ['8'], 'difficulty': ['5'], 'answer': [\"The Microsoft Tay chatbot and the Amazon Alexa's response to a question about the Holocaust are examples of misapplications or unintended consequences of question-answering systems or chatbots in the wild. These incidents demonstrate the potential risks and challenges associated with these technologies, especially when they are designed to learn from user interactions or rely on large datasets that may contain biases or errors.\\n\"]}\n",
      "-----\n",
      "How have neural network-based approaches revolutionized NLP tasks such as text generation and machine translation in recent years?\n",
      "{'relevance': ['9'], 'difficulty': ['6'], 'answer': ['Neural network-based approaches have revolutionized NLP tasks such as text generation and machine translation in recent years by providing more accurate, contextually aware, and flexible solutions compared to rule-based or statistical methods. These advancements are primarily due to the development of deep learning techniques like recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and transformers, which can handle complex linguistic structures, ambiguities, and contextual nuances more effectively.\\n\\nFor text generation tasks, neural networks have enabled the creation of models that can generate coherent and meaningful sentences based on given inputs. Examples include language models like GPT-3, which can write articles, answer questions, and even compose music. For machine translation, neural network-based systems like Google Translate and DeepL have significantly improved translation accuracy and fluency by learning to understand the underlying structures of languages and their relationships.\\n\\nThese advancements have led to more natural and human-like interactions with machines in various applications, such as chatbots, virtual assistants, and automated customer service systems. However, challenges still remain, including handling rare or out-of-vocabulary words, maintaining explainability, and addressing biases in training data.\\n']}\n",
      "-----\n",
      "Based on the lecture content about NLP and its many applications, which of the following is NOT mentioned as a primary task of the field?\n",
      "\n",
      "\n",
      "\n",
      "A) Text classification for spam filtering\n",
      "\n",
      "B) Generating coherent text based on a given context\n",
      "\n",
      "C) Turning sound into written text (Speech-to-text)\n",
      "\n",
      "D) The automatic creation of visual art from textual prompts\n",
      "\n",
      "(Note: While modern AI can generate impressive images, the idea that it could yet accurately translate text to visual content is overstated and remains in the realm of science fiction for now.)\n",
      "{'reasoning': ['The lecture content covers various NLP tasks, including text classification (for spam filtering), text generation based on context, speech-to-text conversion, question answering, and image captioning. However, it does not explicitly mention the automatic creation of visual art from textual prompts as a primary task within the field of NLP. Although modern AI can generate impressive images, the idea that it could accurately translate text to visual content is still in the realm of science fiction for now.\\n\\nTherefore, option D) The automatic creation of visual art from textual prompts is NOT mentioned as a primary task of the field in this lecture.\\n'], 'relevance': ['0'], 'difficulty': ['10'], 'answer': ['D) The automatic creation of visual art from textual prompts']}\n",
      "-----\n",
      "Given the following code snippet from StopWordKeywordExtractor class:\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "class StopWordKeywordExtractor:\n",
      "\n",
      "    # ...\n",
      "\n",
      "    def is_proper_word(self, token:str) -> bool:\n",
      "\n",
      "        ''' \n",
      "\n",
      "        Checks if the word is a proper word by our definition \n",
      "\n",
      "        Arguments: \n",
      "\n",
      "            token     -- The token as a string \n",
      "\n",
      "        Return: \n",
      "\n",
      "            is_proper -- True / False\n",
      "\n",
      "        '''\n",
      "\n",
      "        match = re.search(r'\\b(\\w{2,})\\b', token)\n",
      "\n",
      "        return match and token == match[0]\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Question: What is the purpose of the regular expression `r'\\b(\\w{2,})\\b'` in the is_proper_word function?\n",
      "{'reasoning': [\"The given code snippet is from a StopWordKeywordExtractor class that appears to be used for keyword extraction in Natural Language Processing (NLP). The regular expression `r'\\\\b(\\\\w{2,})\\\\b'` in the `is_proper_word` function is used to check if the input token is a proper word according to the definition implemented in this class.\\n\\nThe regular expression consists of two parts:\\n\\n1. `\\\\b`: This is a word boundary, which matches the position between a word character (letters, digits, or underscores) and a non-word character (anything else). In other words, it ensures that the match starts and ends at a word boundary.\\n\\n2. `(\\\\w{2,})`: This part of the regular expression matches one or more consecutive word characters (letters, digits, or underscores) with a minimum length of 2. This means that the matched string should consist of two or more alphanumeric characters to be considered a proper word.\\n\\nBy combining these two parts, the regular expression `r'\\\\b(\\\\w{2,})\\\\b'` matches words consisting of at least two consecutive alphanumeric characters. The `is_proper_word` function then checks if the input token is equal to the matched string (i.e., the extracted word), and returns True if it is a proper word according to this definition.\\n\"], 'relevance': ['10'], 'difficulty': ['5'], 'answer': [\"The purpose of the regular expression `r'\\\\b(\\\\w{2,})\\\\b'` in the `is_proper_word` function is to match words consisting of at least two consecutive alphanumeric characters. This regular expression helps ensure that only proper words (according to this definition) are considered valid inputs for the StopWordKeywordExtractor class.\\n\"]}\n",
      "-----\n",
      "In the context of BPE tokenization, what is the significance of merging less frequent pairs before more frequent ones during the training phase?\n",
      "{}\n",
      "-----\n",
      "What role do morphemes play in subword tokenization methods like BPE and why are they often represented as part of a token in text processed by such algorithms? \n",
      "{}\n",
      "-----\n",
      "In the context of NLP, how might sentence segmentation impact the effectiveness of a byte pair encoding token learner and what precautions can be taken to avoid any negative impact of mis-segmented sentences on the learning process?\n",
      "{}\n",
      "-----\n",
      "What is the role of the Byte Pair Encoding (BPE) token learner algorithm in text preprocessing for NLP?\n",
      "\n",
      "A) It separates punctuation from words.\n",
      "\n",
      "B) It normalizes case folding, making everything lowercase or uppercase as needed.\n",
      "\n",
      "C) It learns subword tokens that can represent frequent word subparts and are often morphemes\n",
      "\n",
      "D) It performs full stemming of the words in a corpus.\n",
      "{}\n",
      "-----\n",
      "Based on the function `get_splits`, what does the dictionary created by this function represent? Can you provide an example of a key and its associated value in the output of this function for the input `pretokenize([[\"apple\", \"banana\", \"apple\"], [\"apple\"]])`?\n",
      "\n",
      "\n",
      "\n",
      "Code Snippet:\n",
      "\n",
      "```python\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "from typing import Dict, Tuple\n",
      "\n",
      "\n",
      "\n",
      "def get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\n",
      "\n",
      "    \"\"\"Get subword splits of tokens in a corpus.\n",
      "\n",
      "    \n",
      "\n",
      "    Args:\n",
      "\n",
      "        corpus (List[List[str]]): A list of sentences where each sentence is represented\n",
      "\n",
      "            as a list of tokens.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\n",
      "\n",
      "            and values are the counts of occurrences of those splits in the corpus.\n",
      "\n",
      "    \n",
      "\n",
      "    Example: \n",
      "\n",
      "        >>> corpus = [[\"apple\", \"banana\", \"apple\"], [\"apple\"]]\n",
      "\n",
      "        >>> get_splits(corpus)\n",
      "\n",
      "        {('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
      "\n",
      "```\n",
      "{}\n",
      "-----\n",
      "In the context of large language models, what is the purpose of fine-tuning with human feedback and how does it improve a model's performance?\n",
      "{'relevance': ['10'], 'difficulty': ['5']}\n",
      "-----\n",
      "What are the potential challenges associated with creating high-quality datasets for training transformer decoders, as depicted in the process by OpenAI in 2021?\n",
      "{'relevance': ['8'], 'difficulty': ['7']}\n",
      "-----\n",
      "How can multi-task learning be harnessed when training large language models to generate more coherent and human-like text?\n",
      "{'relevance': ['9'], 'difficulty': ['7']}\n",
      "-----\n",
      "What was the foundational paper in the field of transformers that sparked research into their applications within natural language processing?\n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': ['Attention is All You Need by Vaswani et al (2017)']}\n",
      "-----\n",
      "Which of the following statements is true regarding the process discussed in \"Learning to summarize from human feedback\" by Stiennon et al (OpenAI), 2022?\n",
      "\n",
      "A) The researchers only used non-expert judges for evaluation.\n",
      "\n",
      "B) The summaries were not posted on a public platform for evaluation but assessed solely in-house.\n",
      "\n",
      "C) Both expert and non-expert judges evaluated the summaries, giving them rewards based on quality as perceived by their respective levels of expertise. \n",
      "\n",
      "D) The model was only fine-tuned once after collecting the initial batch of human feedback.\n",
      "{}\n",
      "-----\n",
      "What is the potential issue with a low-rank assumption in LoRa, and how might it impact model performance?\n",
      "{}\n",
      "-----\n",
      "In few-shot learning, what's the difference between zero-shot and few-shot prompting, and why might one be more effective than the other for certain tasks? \n",
      "{}\n",
      "-----\n",
      "How does prefix tuning differ from parameter-efficient fine-tuning methods like LoRa, in terms of their approach to updating parameters while maintaining model efficiency?\n",
      "{}\n",
      "-----\n",
      "In an NLP context, what are some potential advantages and disadvantages of fine-tuning a large pre-trained LLM on a specific task rather than starting with a model trained directly for the target task from scratch?\n",
      "{}\n",
      "-----\n",
      "What is the low-rank assumption in LoRa (Low-Rank Adaptation of Large Language Models) and how does it help with fine-tuning these models?\n",
      "\n",
      "A) It assumes that only a few new parameters are needed to adapt the model, reducing the computational resources required during training.\n",
      "\n",
      "B: It assumes the update weight matrix contains just as much information as the original weights, requiring no reduction in dimensionality or computation.\n",
      "\n",
      "C: It assumes the update weight matrix is of higher rank than the original weights, actually increasing the complexity and parameters of the fine-tuned model. \n",
      "\n",
      "D: It makes no assumptions about the weight matrices' ranks, making the approach computationally intensive like a full parameter update.\n",
      "{'answer': ['A) It assumes that only a few new parameters are needed to adapt the model, reducing the computational resources required during training.']}\n",
      "-----\n",
      "What are the key differences between a retrieval-based approach and a keyword search in an NLP context, especially in relation to the RAG system? \n",
      "{'reasoning': ['The lecture discusses the concept of Retrieval Augmented Generators (RAG) systems for reducing the context size in language models. In this context, retrieval-based approaches involve embedding text sequences into a vector space and comparing vectors to retrieve semantically similar documents. On the other hand, keyword search is an algorithm that finds documents fitting a given text query without the need for VectorDB.\\n\\nThe key differences between these two methods in an NLP context, especially in relation to the RAG system, are as follows:\\n1. Retrieval-based approaches require embedding algorithms and similarity measures, while keyword search only requires matching keywords or phrases from a query with documents in a database.\\n2. Retrieval-based approaches can be more complex due to parameters like chunking, metadata inclusion, image representation, and summarization, whereas keyword search is simpler and faster.\\n3. In the RAG system, retrieval-based approaches aim to retrieve semantically similar documents by comparing vectors, while keyword search directly finds documents fitting a given text query.\\n4. Retrieval-based approaches can be combined with other methods (hybrid search), whereas keyword search is typically used alone.\\n5. Dynamic RAG systems use intermediate queries and LLMs to improve the retrieval process, which is not a feature of keyword search.\\n6. In a corporate environment, data privacy concerns, network structures, user authentication, and internal certification are factors that may affect the implementation of RAG systems but not keyword search.\\n'], 'relevance': ['9'], 'difficulty': ['5'], 'answer': ['The key differences between a retrieval-based approach and a keyword search in an NLP context, especially in relation to the RAG system, are:\\n1. Retrieval-based approaches require embedding algorithms and similarity measures, while keyword search only requires matching keywords or phrases from a query with documents in a database.\\n2. Retrieval-based approaches can be more complex due to parameters like chunking, metadata inclusion, image representation, and summarization, whereas keyword search is simpler and faster.\\n3. In the RAG system, retrieval-based approaches aim to retrieve semantically similar documents by comparing vectors, while keyword search directly finds documents fitting a given text query.\\n4. Retrieval-based approaches can be combined with other methods (hybrid search), whereas keyword search is typically used alone.\\n5. Dynamic RAG systems use intermediate queries and LLMs to improve the retrieval process, which is not a feature of keyword search.\\n6. In a corporate environment, data privacy concerns, network structures, user authentication, and internal certification are factors that may affect the implementation of RAG systems but not keyword search.']}\n",
      "-----\n",
      "How can multi-armed bandit theory be leveraged in hybrid document retrieval for a RAG system, given user feedback on relevancy of retrieved documents?\n",
      "{}\n",
      "-----\n",
      "In a corporate environment, what are some potential challenges or special considerations when implementing an NLP RAG system and how might these require adaptations to the standard approach? \n",
      "{'relevance': ['9'], 'difficulty': ['6'], 'answer': ['Potential challenges or special considerations when implementing an NLP RAG system in a corporate environment include data privacy concerns, cost implications, competition between different groups building similar solutions, internal networks and user authentication, abuse of the service, and corporate internal certification. These factors require adaptations to the standard approach, such as implementing secure data handling methods, considering cost-effective solutions, addressing competition through collaboration or intellectual property protection, ensuring network security, implementing robust user authentication mechanisms, preventing misuse, and complying with corporate standards and certifications. ']}\n",
      "-----\n",
      "How can intermediate queries improve the effectiveness of the interaction between users, document retrieval, and LLMs in a RAG system?\n",
      "{}\n",
      "-----\n",
      "What is the primary motivation behind using a Retrieval-Augmented Generator (RAG) in natural language processing, and what key components does it consist of?\n",
      "\n",
      "A) To reduce computation time and resources; vector search and document retrieval\n",
      "\n",
      "B) To increase context length and handle larger datasets; LLMs and multi-head attention\n",
      "\n",
      "C) To improve the understanding of visual content and tables in documents; computer vision models and table parsing algorithms\n",
      "\n",
      "D) To personalize text generation for different users; user modeling and reinforcement learning\n",
      "{'reasoning': ['The primary motivation behind using a Retrieval-Augmented Generator (RAG) in natural language processing is to reduce the context size of the prompt by not using all documents but rather a selection. This is known as a Retrieval Augmented Generator. The key components it consists of are vector search, which embeds text sequences into a vector space, and retrieval of semantically similar documents by comparing the vectors (see slide 20-22).'], 'relevance': ['10'], 'difficulty': ['5'], 'answer': ['A) To reduce computation time and resources; vector search and document retrieval']}\n",
      "-----\n",
      "What is the intuition behind the smoothing techniques in statistical language modeling, and how do they help with the sparsity issue of n-gram models?\n",
      "{}\n",
      "-----\n",
      "How does the use of caching models impact speech recognition tasks as opposed to text-based applications? \n",
      "{}\n",
      "-----\n",
      "How can N-gram probabilities be combined through interpolation, and why might simple linear interpolation not always provide the best results in practice?\n",
      "{}\n",
      "-----\n",
      "What is the primary purpose of smoothing techniques in statistical language models? \n",
      "\n",
      "A) To improve the probability assigned to unseen n-grams during evaluation on test sets\n",
      "\n",
      "B) To reduce the complexity of the model for more efficient processing\n",
      "\n",
      "C) To increase the likelihood that all words are part of the model's vocabulary\n",
      "\n",
      "D) To specifically weight more recently used words as more likely to appear next in a text[IDE]\n",
      "{}\n",
      "-----\n",
      "Question:In the lecture content, a bigram model is built for text from the movie *Spider-Man Homecoming*. The `bigram_probability` method in the BigramModel class calculates the probability of one word given another. Can you explain, step-by-step, how the probabilities are calculated and what data structures are used to enable quick access to these probabilities?\n",
      "{}\n",
      "-----\n",
      "What is the main purpose of converting words into vector representations in natural language processing, and why can't machine learning models work directly with text data? \n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': ['The main purpose of converting words into vector representations in Natural Language Processing is to enable machines to understand the context, relationships, and meaning between words more effectively than if they were working directly with raw text data. Machine learning models cannot work directly with text data because it is unstructured, computationally expensive to analyze without dimensionality reduction, and may contain ambiguities that require proper representation for accurate processing. [IDE]']}\n",
      "-----\n",
      "Why might one-hot encoding not always be an ideal representation for word embeddings, and what are some alternative methods to capture semantic meaning in vectors?\n",
      "{'reasoning': ['In the given lecture, the topic of discussion is \"Static_embedding_OneHot_encoding,\" which refers to the process of representing words as binary vectors using one-hot encoding. While one-hot encoding can be useful for machine learning algorithms, it has some limitations when dealing with word embeddings. One major issue is that one-hot encoding does not capture semantic relationships between words effectively, as each word is represented as a separate vector with no overlap or similarity between them. This makes it difficult to understand the context and meaning of sentences.\\n\\nTo address this limitation, alternative methods for creating word embeddings have been developed. These methods aim to create dense vector representations that capture the semantic meaning of words in a more meaningful way. Some examples include Word2Vec, GloVe, and FastText. These techniques use various algorithms to analyze large amounts of text data and learn relationships between words based on their contexts within sentences. This allows them to generate embeddings that better capture the nuances and subtleties of language, making it easier for machine learning models to understand and process natural language.\\n\\nIn summary, one-hot encoding may not always be an ideal representation for word embeddings because it does not effectively capture semantic relationships between words. Alternative methods like Word2Vec, GloVe, and FastText have been developed to overcome this limitation by generating dense vector representations that better capture the meaning of words in a more meaningful way.\\n'], 'relevance': ['9'], 'difficulty': ['5'], 'answer': ['Alternative methods like Word2Vec, GloVe, and FastText have been developed to overcome the limitations of one-hot encoding in capturing semantic relationships between words. These techniques generate dense vector representations that better capture the meaning of words in a more meaningful way.\\n']}\n",
      "-----\n",
      "In the context of NLP, why is it important for a model to understand the similarity between words, and how does representing words as vectors help with this task?\n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': ['It is important for a model to understand word similarities in NLP because it helps improve the overall performance of various tasks, such as text classification, sentiment analysis, and language modeling. Representing words as vectors enables models to capture semantic meaning by placing similar words closer together in the vector space, making it easier to perform tasks like predicting missing words or understanding the context of a sentence. [IDE]']}\n",
      "-----\n",
      "What is the primary purpose of one-hot encoding in NLP?\n",
      "\n",
      "A) To reduce dimensionality of word embeddings\n",
      "\n",
      "B) To convert categorical data into a usable format for machine learning algorithms\n",
      "\n",
      "C) To quantify the semantic similarity between words in vector space\n",
      "\n",
      "D) To create a sparse representation of text documents for improved computational efficiency\n",
      "{'reasoning': ['The primary purpose of one-hot encoding in NLP is to convert categorical data into a usable format for machine learning algorithms. In this lecture, the topic \"Static_embedding_OneHot_encoding\" refers to the process of creating static embeddings using one-hot encoding, which helps in creating a dense vector representation of words that captures their semantic meaning. OneHot encoding is essential for NLP tasks such as text classification, sentiment analysis, and language modeling, as it enables machine learning algorithms to understand and process language more effectively. The correct answer is B.'], 'relevance': ['10'], 'difficulty': ['2'], 'answer': ['B) To convert categorical data into a usable format for machine learning algorithms']}\n",
      "-----\n",
      "**Question:**\n",
      "\n",
      "In the `TfIdfModel` class, the method `build_index` is implemented to create a dictionary that maps each word type in the documents to an index. The same functionality was already implemented in the `OneHotModel` class and can be reused there. Should you copy that implementation over or write your own? What are the trade-offs between these two decisions, considering the need for maintaining the code DRY while also avoiding unnecessary complexity due to shared code with a different model?\n",
      "{'reasoning': [\"In this question, the student is asked to evaluate whether they should reuse the implementation of the `build_index` method from the `TfIdfModel` class in the `OneHotModel` class or write their own. This question covers the topic of code reusability and the trade-offs between maintaining the DRY (Don't Repeat Yourself) principle and avoiding unnecessary complexity due to shared code with a different model.\\n\\nThe `TfIdfModel` and `OneHotModel` are both classes used for text processing, but they serve different purposes: `TfIdfModel` is an algorithm for calculating the weight of each word in a document based on its frequency within the entire corpus, while `OneHotModel` converts categorical data into a numerical format suitable for machine learning algorithms.\\n\\nReusing the implementation from the `TfIdfModel` class in the `OneHotModel` class would help maintain code DRY and reduce duplicated effort. However, it may introduce unnecessary complexity due to shared code with a different model, making the code harder to understand and potentially leading to errors or inconsistencies.\\n\\nOn the other hand, writing their own implementation for the `build_index` method in the `OneHotModel` class would ensure that the code is tailored specifically to the needs of the one-hot encoding process, resulting in cleaner and more efficient code. However, this approach requires additional effort and time compared to reusing the existing implementation.\\n\\nIn terms of relevance, this question is highly relevant as it deals with an important aspect of software development: code reusability and maintaining DRY principles while avoiding unnecessary complexity. The difficulty level depends on the student's familiarity with the `TfIdfModel` and `OneHotModel` classes and their understanding of the trade-offs between code reuse and avoiding shared code with different models.\\n\"], 'relevance': ['9'], 'difficulty': ['7'], 'answer': [\"It is generally a good practice to avoid unnecessary duplication of code, so it would be more efficient to reuse the implementation of the `build_index` method from the `TfIdfModel` class in the `OneHotModel` class. However, it's important to consider the specific requirements and potential complications that may arise when sharing code between different models. In this case, since both models deal with mapping words to indices, reusing the implementation should be feasible as long as any necessary adaptations are made to accommodate the differences in the one-hot encoding process.\"]}\n",
      "-----\n",
      "What is the purpose of normalizing term frequencies in the TF-IDF method?\n",
      "{'reasoning': ['The question asks about the purpose of normalizing term frequencies in the Term Frequency - Inverse Document Frequency (TF-IDF) method. The lecture explains that the goal of TF-IDF is to create embeddings that make it easy to distinguish between documents, which are used in information retrieval (finding relevant documents to a search query). Normalizing term frequencies helps achieve this by making terms that appear often in a document less important and terms that only appear in a few documents more important. This is done through two parts: term frequency (tf) and inverse document frequency (idf). The idf part of the method is normalized using logarithm (log10), which helps to prevent terms that appear in all documents from being given too much weight.\\n'], 'relevance': ['9'], 'difficulty': ['3'], 'answer': ['. The purpose of normalizing term frequencies in the TF-IDF method is to make it easier to distinguish between documents, which is useful for information retrieval. This is achieved by making terms that appear often in a document less important and terms that only appear in a few documents more important through two parts: term frequency (tf) and inverse document frequency (idf). The idf part of the method is normalized using logarithm (log10), which helps to prevent terms that appear in all documents from being given too much weight.\\n']}\n",
      "-----\n",
      "In the given example, what are the calculated TF-IDF values for \"a\" and \"linux\", respectively? \n",
      "{'reasoning': ['The lecture covers the calculation of Term Frequency (TF) and Inverse Document Frequency (IDF), and their combination to calculate the TF-IDF value. The given example shows the calculation of TF and IDF for each term in a corpus, but does not explicitly show the final TF-IDF values for \"a\" and \"linux\". However, we can calculate these values based on the information provided.\\n\\nFirst, let\\'s find the TF values for \"a\" and \"linux\":\\n\\n1. For \"a\", it appears 6 times in total (in documents 1, 2, 3, 4, 5, and 7). In document 1, it appears 3 times; in document 2, it appears once; in document 3, it appears once; in document 4, it appears once; in document 5, it appears twice; and in document 7, it appears once. So the TF value for \"a\" is:\\n\\n   TF(a) = (3 + 1 + 1 + 1 + 2 + 1) / 7 = 0.857\\n\\n2. For \"linux\", it appears 6 times in total (in documents 4, 5, 6, and 7). In document 4, it appears once; in document 5, it appears twice; in document 6, it appears once; and in document 7, it appears twice. So the TF value for \"linux\" is:\\n\\n   TF(linux) = (1 + 2 + 1 + 2) / 4 = 1.5\\n\\nNext, let\\'s find the IDF values for \"a\" and \"linux\":\\n\\n1. For \"a\", it appears in all 7 documents. So the number of documents where \"a\" does not appear (N - df(a)) is:\\n\\n   N - df(a) = 7 - 7 = 0\\n\\n   Since \"a\" appears in every document, its IDF value will be 0.\\n\\n2. For \"linux\", it appears in documents 4, 5, 6, and 7. So the number of documents where \"linux\" does not appear (N - df(linux)) is:\\n\\n   N - df(linux) = 7 - 4 = 3\\n\\n   The IDF value for \"linux\" can be calculated as follows:\\n\\n   IDF(linux) = log10((N - df(linux)) + 1) = log10(3 + 1) = log10(4) ≈ 0.602\\n\\nFinally, let\\'s find the TF-IDF values for \"a\" and \"linux\":\\n\\n1. For \"a\", since its IDF value is 0, the TF-IDF value will be:\\n\\n   TF-IDF(a) = TF(a) * IDF(a) = 0.857 * 0 = 0\\n\\n2. For \"linux\", the TF-IDF value can be calculated as follows:\\n\\n   TF-IDF(linux) = TF(linux) * IDF(linux) = 1.5 * 0.602 ≈ 0.903\\n'], 'relevance': ['8'], 'difficulty': ['4'], 'answer': ['The calculated TF-IDF value for \"a\" is 0, and the calculated TF-IDF value for \"linux\" is approximately 0.903.']}\n",
      "-----\n",
      "Why would a term with an inverse document frequency of 0 not be useful for distinguishing between documents in this context?\n",
      "{'reasoning': ['The question asks about the reason why a term with an inverse document frequency (idf) of 0 is not useful for distinguishing between documents in the context of Term Frequency - Inverse Document Frequency (TF-IDF).\\n\\nIn TF-IDF, idf is used to reflect how important a word is to a document in the collection. A high idf value means that the word is very important and appears in few documents, while a low or zero idf value means that the word is not important because it appears in all of the documents in the collection.\\n\\nIn this context, if a term has an idf of 0, it means that the term appears in all of the documents in the collection. Since all documents contain the term, it does not help to distinguish between them and therefore is not useful for distinguishing between documents.\\n\\nThe section of the lecture that covers this topic is \"idf of 0 → can not be used to distinguish documents. Appears in all of them!\"'], 'relevance': ['10'], 'difficulty': ['2'], 'answer': ['A term with an inverse document frequency (idf) of 0 is not useful for distinguishing between documents because it appears in all of the documents in the collection and does not help to differentiate between them.']}\n",
      "-----\n",
      "What is the effect of a high term frequency and low document frequency on a word's TF-IDF value in a text corpus?\n",
      "\n",
      "A) The value will be low, as high tf and low df could indicate the word is common within a single document but not very important across the entire corpus.\n",
      "\n",
      "B) The value will be moderate, as the high tf somewhat offsets the low df.\n",
      "\n",
      "C) The value will be high, as a high tf and a low df may suggest the word is particularly significant to one specific document in the corpus.\n",
      "\n",
      "D) It's impossible to say without more information on the inverse document frequency values.\n",
      "{'reasoning': [\"The question asks about the effect of high term frequency (tf) and low document frequency (df) on a word's TF-IDF value. The lecture explains that TF-IDF is calculated by multiplying tf with idf, where idf is the inverse of df. It is intuitive that a high tf and low df would result in a higher idf as the term appears often in a document but not in many documents. Therefore, the product of tf and idf (TF-IDF) will be relatively high for words with this characteristic.\\n\\nThe correct answer is C) The value will be high, as a high tf and low df may suggest the word is particularly significant to one specific document in the corpus.\\n\"], 'relevance': ['10'], 'difficulty': ['5'], 'answer': ['C) The value will be high, as a high tf and low df may suggest the word is particularly significant to one specific document in the corpus.']}\n",
      "-----\n",
      "Based on the given excerpt, a potential question could be: \n",
      "\n",
      "\n",
      "\n",
      "In the 'TfIdfModel' class, what is the purpose of the 'df_vector' and how is it computed from the 'term_doc_matrix'? \n",
      "\n",
      "Code snippet for reference:\n",
      "\n",
      "```python\n",
      "\n",
      "df_vector = np.zeros(len(self.index))\n",
      "\n",
      "# Fill the matrix and compute the tfidf matrix\n",
      "\n",
      "raise NotImplementedError()\n",
      "\n",
      "```\n",
      "{'relevance': ['9'], 'difficulty': ['5'], 'answer': [\"The 'df_vector' is used to store the document frequency for each document in the corpus. It is computed by counting how many non-zero entries (i.e., terms) there are in each row of the 'term_doc_matrix'. This can be done using the numpy function np.count_nonzero().\"]}\n",
      "-----\n",
      "What is the main difference between the traditional approach to word embeddings and the Negative Sampling Skip Gram Model in terms of computational time? \n",
      "{'relevance': ['10'], 'difficulty': ['2'], 'answer': ['The main difference between the traditional approach and the Negative Sampling Skip Gram Model in terms of computational time is that the latter is significantly faster, taking approximately 5000 times less time than the former for a vocabulary size of 1,000,000 words. [IDE]']}\n",
      "-----\n",
      "In the context of Word2Vec, why is it crucial for the model to incorporate negative examples along with positive examples during training?\n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': ['Incorporating negative examples is crucial for the Word2Vec model during training because it helps the model learn more effectively by allowing it to distinguish between words that are semantically similar but not necessarily related in a given context. Without negative examples, the classifier could predict 1 (or true) for all positive examples and achieve a 100% accuracy, which would not be useful in real-world applications where the goal is to accurately predict whether two words co-occur or not. Negative examples help the model learn the boundaries of word relationships by providing counterexamples that demonstrate when two words do not belong together in a given context. This information is essential for improving the performance of the Word2Vec model and making it more robust in various NLP tasks such as language translation, sentiment analysis, and text classification. [IDE]']}\n",
      "-----\n",
      "How does the frequency-based negative sampling method help improve the Word2Vec model's understanding of word relationships and context within a text corpus?\n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': [\"The frequency-based negative sampling method helps improve the Word2Vec model's understanding of word relationships and context within a text corpus by providing more diverse and informative training data, leading to better performance in NLP tasks such as language translation, sentiment analysis, and text classification. [IDE]\"]}\n",
      "-----\n",
      "Question: In the context of Word2Vec and negative sampling, what is one primary reason for creating synthetic negative examples during training?\n",
      "\n",
      "A) To outweigh the number of positive examples in a dataset\n",
      "\n",
      "B) To ensure that the model isn't \"tricked\" by only learning from positive examples \n",
      "\n",
      "C) To make the most of sparse data where there are limited negative examples available\n",
      "\n",
      "D) To artificially boost the performance of the classifier on test data[IDE]\n",
      "{'reasoning': ['The given lecture discusses the Word2Vec model and its application in creating word embeddings, specifically focusing on the Negative Sampling Skip Gram Model. This model is used to learn about the context of words and their relationships by predicting the surrounding words in a given context. The question asks for the primary reason for creating synthetic negative examples during training. In the lecture, it is mentioned that without negative examples, the classifier could always predict 1 (assuming a binary classification problem) and achieve a 100% accuracy, which would not be an accurate representation of the data. Therefore, the creation of negative examples helps ensure that the model isn\\'t \"tricked\" by only learning from positive examples. The correct answer is B.'], 'relevance': ['9'], 'difficulty': ['5'], 'answer': ['B) To ensure that the model isn\\'t \"tricked\" by only learning from positive examples']}\n",
      "-----\n",
      "### Question:\n",
      "\n",
      "In the given lecture content, there's a function called `bagOfWords` that is used to create document embeddings. It takes in a model and a document as input and returns the document embedding by summing up word vectors. Can you explain why taking the mean of the word vectors would not work here?\n",
      "{'relevance': ['10'], 'difficulty': ['5'], 'answer': [\"Taking the mean of word vectors would not work here because Word2Vec is based on the distributional hypothesis, which states that words that appear in similar contexts have similar meanings. By summing the word vectors, we are effectively capturing the overall distribution of a document's context, rather than just its average meaning. This approach helps to preserve more information about the document's content and better capture the relationships between the words used within it. [IDE]\"]}\n",
      "-----\n",
      "How does the parallelogram method work in solving analogies, and why is it not effective for all types of analogical relations?\n",
      "{}\n",
      "-----\n",
      "What role can historical diachronic embeddings play as a tool to study semantic shifts and cultural biases over time? Can you provide an example from your text above? \n",
      "{}\n",
      "-----\n",
      "How are gender and ethnic bias quantified in word embeddings, and what was the trend observed by Garg et al for competence adjectives during 1960-1990?\n",
      "{}\n",
      "-----\n",
      "What analogy-solving method is related to the parallelogram model of analogical reasoning and was shown by Mikolov et al. (2013b) to be viable with word2vec embeddings? \n",
      "\n",
      "A) Vector addition only in a single dimension \n",
      "\n",
      "B) Vector subtraction only in a single dimension \n",
      "\n",
      "C) Parallelogram method\n",
      "\n",
      "D) The cosine between the vectors of the two pairs is maximized for the second pair relative to the first.\n",
      "{}\n",
      "-----\n",
      "Based on the given code snippet and content, a good question could be: How does the function `bagOfWords` create document embeddings, and how do these differ from the input token-level embeddings in the dictionaries?\n",
      "{}\n",
      "-----\n",
      "What is the main reason why a simple feedforward neural language model often performs almost as well as more complex models in predicting the next word in a sequence? \n",
      "\n",
      "A) The use of embeddings for words allows the network to find semantic similarities.\n",
      "\n",
      "B) It only considers a fixed number of prior words through sliding windows, limiting complexity. \n",
      "\n",
      "C) It is easier and faster to train compared to more complex architectures. \n",
      "\n",
      "D) It does not suffer from the vanishing gradient problem that can affect recurrent neural networks.\n",
      "{'reasoning': [\"The question covers the topic of Neural Language Models (LMs), specifically focusing on simple feedforward LMs and their performance in predicting the next word in a sequence. The reasoning behind this question is to evaluate the student's understanding of how these models work, their advantages over n-gram language models, and the key features that contribute to their effectiveness.\\n\\nThe main reason why a simple feedforward neural language model often performs almost as well as more complex models in predicting the next word in a sequence is because it uses embeddings for words, allowing the network to find semantic similarities (Option A). This is an essential feature that enables the model to generalize and make accurate predictions based on the context of the input sequence.\\n\\nWhile options B, C, and D are partially correct, they do not fully explain why simple feedforward LMs can perform well in predicting the next word. Option B mentions sliding windows but does not emphasize their role in capturing semantic similarities between words. Option C is incorrect because training a neural network is not necessarily easier or faster than training more complex architectures, and option D is only partially correct as simple feedforward LMs do not necessarily avoid the vanishing gradient problem that can affect recurrent neural networks.\"], 'relevance': ['9'], 'difficulty': ['6'], 'answer': ['A) The use of embeddings for words allows the network to find semantic similarities.']}\n",
      "-----\n",
      "Based on the implementation of the `OneHotEncoder` class and its `encode` method in the lecture content, what would be a proper way to call the `encode` function for the token with index 5 (assuming our tokenizer's vocabulary size is 20), and what should be the shape of the output tensor?```python\n",
      "\n",
      "# Assuming that we've created an instance of OneHotEncoder:\n",
      "\n",
      "encoder = OneHotEncoder(vocab_size=20)\n",
      "\n",
      "# To get a one-hot encoded vector for the token at index 5, we would call our method as follows:\n",
      "\n",
      "encoding = encoder.encode(5)\n",
      "\n",
      "# And the shape of this output tensor should be (20,) which indicates that there are 20 elements in the vector, matching the size of our assumed vocabulary.\n",
      "\n",
      "```\n",
      "{'reasoning': ['The question asks for the proper way to call the `encode` function from the `OneHotEncoder` class and the expected shape of the output tensor when given a specific token index (5) assuming the vocabulary size is 20. To achieve this, we first create an instance of the `OneHotEncoder` class with the appropriate vocabulary size. Then, to call the `encode` function for the desired token index, we simply pass the integer value as an argument. In this case, it would be `encoder.encode(5)`. The output tensor should have a shape of (20,), which corresponds to the number of elements in the one-hot encoded vector, matching the size of our assumed vocabulary.'], 'relevance': ['10'], 'difficulty': ['3'], 'answer': ['encoder.encode(5).shape == (20,)']}\n",
      "-----\n",
      "What is the main advantage of a Bidirectional RNN as compared to a unidirectional one for NLP tasks?\n",
      "\n",
      "A) It can only process data in one direction, which may limit its understanding of context.\n",
      "\n",
      "B) It's less computationally expensive than a bidirectional model.\n",
      "\n",
      "C) It captures context from both past and future time points, improving the performance on many NLP tasks\n",
      "\n",
      "D) It is easier to train as it has fewer connections to learn. [IDE]\n",
      "{}\n",
      "-----\n",
      "What is the role of self-attention in the Transformer Encoder, and how does it enable the model to understand context within a sequence of tokens? \n",
      "{}\n",
      "-----\n",
      "In BERT's masked language modeling task, what do we feed into the simple classifier for predicting the masked token, given that we have the final context embeddings from the encoder?\n",
      "{}\n",
      "-----\n",
      "How are [SEP] and [CLS] tokens utilized in BERT to tackle the next sentence prediction task and how does this improve its understanding of the order of sentences? \n",
      "{}\n",
      "-----\n",
      "What is the purpose of positional encoding in Transformer models, and why can't we simply use a flat word embedding for each token without any information about their positions within the sequence?\n",
      "{}\n",
      "-----\n",
      "What is the main purpose of the self-attention mechanism in a Transformer Encoder?\n",
      "\n",
      "\n",
      "\n",
      "A) To generate individual token embeddings from positional information only\n",
      "\n",
      "B) To weigh the importance of different input tokens when generating output representations\n",
      "\n",
      "C) To learn a fixed, one-size-fits-all representation for all inputs\n",
      "\n",
      "D) To rearrange the order of the input sequence based on attention weights [IDE]B) To weigh the importance of different input tokens when generating output representations\n",
      "{}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for lecture_index in questions:\n",
    "    for index, question in enumerate(questions[lecture_index].questions):\n",
    "        print(questions[lecture_index].questions[index])\n",
    "        print(questions[lecture_index].evaluations[index])\n",
    "        print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
